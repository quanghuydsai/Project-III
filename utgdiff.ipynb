{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13786455,"sourceType":"datasetVersion","datasetId":8776350}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch torch_geometric tqdm rdkit transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-29T14:43:08.667971Z","iopub.execute_input":"2025-11-29T14:43:08.668560Z","iopub.status.idle":"2025-11-29T14:44:22.022221Z","shell.execute_reply.started":"2025-11-29T14:43:08.668540Z","shell.execute_reply":"2025-11-29T14:44:22.020576Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nCollecting torch_geometric\n  Downloading torch_geometric-2.7.0-py3-none-any.whl.metadata (63 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nCollecting rdkit\n  Downloading rdkit-2025.9.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.1 kB)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.20.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.10.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.13.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (7.1.3)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.0.9)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.5)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.6.0)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit) (11.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.36.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.11.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torch_geometric) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torch_geometric) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torch_geometric) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torch_geometric) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torch_geometric) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torch_geometric) (2.4.1)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.22.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.3)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.10.5)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch_geometric) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch_geometric) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch_geometric) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torch_geometric) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torch_geometric) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torch_geometric) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torch_geometric-2.7.0-py3-none-any.whl (1.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading rdkit-2025.9.1-cp311-cp311-manylinux_2_28_x86_64.whl (36.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.2/36.2 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch_geometric, rdkit\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 rdkit-2025.9.1 torch_geometric-2.7.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import RobertaModel, RobertaPreTrainedModel, RobertaConfig, RobertaTokenizer\nfrom transformers.models.roberta.modeling_roberta import RobertaSelfAttention, RobertaAttention, RobertaEncoder, RobertaLayer\nimport numpy as np\nimport os\nimport pickle\nfrom typing import List, Tuple","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T14:44:22.023343Z","iopub.execute_input":"2025-11-29T14:44:22.023695Z","iopub.status.idle":"2025-11-29T14:44:54.127489Z","shell.execute_reply.started":"2025-11-29T14:44:22.023653Z","shell.execute_reply":"2025-11-29T14:44:54.126615Z"}},"outputs":[{"name":"stderr","text":"2025-11-29 14:44:35.651245: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764427475.840414      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764427475.894419      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nTEXT_VOCAB_SIZE = 50265 \nATOM_TYPES = ['C', 'N', 'O', 'F', 'P', 'S', 'Cl', 'Br', 'I', 'B', 'Si', 'Unknown']\nBOND_TYPES = [0, 1, 2, 3] # 0: No Bond, 1: Single, 2: Double, 3: Triple\nEDGE_VOCAB_SIZE = 4\nNODE_VOCAB_SIZE = len(ATOM_TYPES) + 1 \nFULL_VOCAB_SIZE = TEXT_VOCAB_SIZE + NODE_VOCAB_SIZE\nNODE_VOCAB_START_ID = TEXT_VOCAB_SIZE\nMASK_NODE_ID = NODE_VOCAB_START_ID + len(ATOM_TYPES)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T14:50:46.006233Z","iopub.execute_input":"2025-11-29T14:50:46.006894Z","iopub.status.idle":"2025-11-29T14:50:46.012265Z","shell.execute_reply.started":"2025-11-29T14:50:46.006868Z","shell.execute_reply":"2025-11-29T14:50:46.011386Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"MAX_SEQ_LEN_TEXT = 128\nMAX_SEQ_LEN_GRAPH = 50 \nDIFFUSION_STEPS = 1000","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T14:50:48.794715Z","iopub.execute_input":"2025-11-29T14:50:48.795320Z","iopub.status.idle":"2025-11-29T14:50:48.799146Z","shell.execute_reply.started":"2025-11-29T14:50:48.795281Z","shell.execute_reply":"2025-11-29T14:50:48.798365Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"class PubChemDataset(InMemoryDataset):\n    \"\"\"\n    Custom Dataset class to correctly load the PyG data tuple (data, slices)\n    from the .pt file using torch.load.\n    \"\"\"\n    def __init__(self, path):\n        super(PubChemDataset, self).__init__()\n        self.data, self.slices = torch.load(path, weights_only=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T15:13:45.742543Z","iopub.execute_input":"2025-11-29T15:13:45.742823Z","iopub.status.idle":"2025-11-29T15:13:45.747143Z","shell.execute_reply.started":"2025-11-29T15:13:45.742803Z","shell.execute_reply":"2025-11-29T15:13:45.746376Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"def graph_to_V_E(data_point: Data, max_nodes: int = MAX_SEQ_LEN_GRAPH):\n    \"\"\" \n    Converts a PyG Data object into padded Node Tokens (V) and Edge Matrix (E). \n    \"\"\"\n    x = data_point.x \n    edge_index = data_point.edge_index \n    edge_attr = data_point.edge_attr \n    \n    N = x.size(0)\n    N_padded = min(N, max_nodes)\n    \n    # 1. Node Token IDs (V)\n    atom_type_ids = torch.argmax(x[:N_padded].float(), dim=-1)\n    V_tokens = atom_type_ids + NODE_VOCAB_START_ID \n    \n    V_padded = torch.ones(max_nodes, dtype=torch.long) * MASK_NODE_ID\n    V_padded[:N_padded] = V_tokens\n    \n    # 2. Edge Matrix (E)\n    E_padded = torch.zeros(max_nodes, max_nodes, dtype=torch.long) # 0: No Bond\n    \n    for i in range(edge_index.size(1)):\n        u, v = edge_index[:, i]\n        if u < N_padded and v < N_padded:\n            bond_type = torch.argmax(edge_attr[i].float()) + 1 \n            E_padded[u, v] = bond_type\n            E_padded[v, u] = bond_type\n\n    return V_padded, E_padded, N_padded","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T14:53:00.738053Z","iopub.execute_input":"2025-11-29T14:53:00.738813Z","iopub.status.idle":"2025-11-29T14:53:00.744847Z","shell.execute_reply.started":"2025-11-29T14:53:00.738787Z","shell.execute_reply":"2025-11-29T14:53:00.744007Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"def utgdiff_collate_fn(batch):\n    \"\"\" Custom collate function for S+V sequence formation. \"\"\"\n    \n    V_padded_list, E_padded_list = [], []\n    text_input_ids, text_attention_masks = [], []\n    num_nodes_list = []\n    \n    for data_point in batch:\n        # Process Graph\n        V_padded, E_padded, num_nodes = graph_to_V_E(data_point)\n        V_padded_list.append(V_padded)\n        E_padded_list.append(E_padded)\n        num_nodes_list.append(num_nodes)\n        \n        # Process Text (extract attribute 'text' from Data object)\n        text_content = data_point.text\n        # Ensure text is a string; some datasets might have lists\n        if isinstance(text_content, list): text_content = text_content[0]\n            \n        text_encoded = tokenizer.encode_plus(\n            text_content,\n            max_length=MAX_SEQ_LEN_TEXT,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        text_input_ids.append(text_encoded['input_ids'].squeeze(0))\n        text_attention_masks.append(text_encoded['attention_mask'].squeeze(0))\n\n    V_true_0 = torch.stack(V_padded_list) \n    E_true_0 = torch.stack(E_padded_list) \n    S_input_ids = torch.stack(text_input_ids) \n    S_attention_mask = torch.stack(text_attention_masks) \n    \n    L_s_max = S_input_ids.size(1)\n    N_max = V_true_0.size(1)\n    B = len(batch)\n    L_unified = L_s_max + N_max \n    \n    input_ids_SV = torch.zeros(B, L_unified, dtype=torch.long)\n    attention_mask_SV = torch.zeros(B, L_unified, dtype=torch.long)\n    \n    text_indices = []\n    node_indices = []\n    \n    for i in range(B):\n        S_i = S_input_ids[i]\n        L_s_i = torch.sum(S_attention_mask[i]).item() \n        N_i = num_nodes_list[i]\n        V_i = V_true_0[i, :N_i] \n        \n        full_sequence = torch.cat([S_i[:L_s_i], V_i])\n        L_total = full_sequence.size(0)\n        \n        input_ids_SV[i, :L_total] = full_sequence\n        attention_mask_SV[i, :L_total] = 1\n        \n        text_indices.append(list(range(L_s_i)))\n        node_indices.append(list(range(L_s_i, L_s_i + N_i)))\n        \n    return {\n        'input_ids_SV': input_ids_SV.to(DEVICE),\n        'attention_mask_SV': attention_mask_SV.to(DEVICE),\n        'V_true_0': V_true_0.to(DEVICE),\n        'E_true_0': E_true_0.to(DEVICE),\n        'S_true_0': S_input_ids.to(DEVICE),\n        'S_attention_mask': S_attention_mask.to(DEVICE),\n        'text_indices': text_indices,\n        'node_indices': node_indices,\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T15:15:14.076616Z","iopub.execute_input":"2025-11-29T15:15:14.077310Z","iopub.status.idle":"2025-11-29T15:15:14.085307Z","shell.execute_reply.started":"2025-11-29T15:15:14.077283Z","shell.execute_reply":"2025-11-29T15:15:14.084609Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"class UTGDiffSelfAttention(RobertaSelfAttention):\n    def forward(self, hidden_states: torch.Tensor, attention_mask: torch.FloatTensor = None, edge_bias: torch.Tensor = None, **kwargs) -> Tuple[torch.Tensor]:\n        \n        mixed_query_layer = self.query(hidden_states)\n        key_layer = self.transpose_for_scores(self.key(hidden_states))\n        value_layer = self.transpose_for_scores(self.value(hidden_states))\n        query_layer = self.transpose_for_scores(mixed_query_layer)\n\n        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n        attention_scores = attention_scores / np.sqrt(self.attention_head_size)\n\n        # Apply Edge Bias B\n        if edge_bias is not None:\n            attention_scores = attention_scores + edge_bias\n\n        if attention_mask is not None:\n            attention_scores = attention_scores + attention_mask\n\n        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n        \n        context_layer = torch.matmul(attention_probs, value_layer)\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n        context_layer = context_layer.view(new_context_layer_shape)\n        \n        output_layer = self.output.dense(context_layer)\n        output_layer = self.output.dropout(output_layer)\n        output = self.output.LayerNorm(output_layer + mixed_query_layer)\n        \n        return (output, attention_probs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T15:15:34.143347Z","iopub.execute_input":"2025-11-29T15:15:34.143633Z","iopub.status.idle":"2025-11-29T15:15:34.150488Z","shell.execute_reply.started":"2025-11-29T15:15:34.143613Z","shell.execute_reply":"2025-11-29T15:15:34.149665Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"class UTGDiffAttention(RobertaAttention):\n    \"\"\" FIX: Custom Attention layer that replaces RobertaAttention to accept 'edge_bias'. \"\"\"\n    def __init__(self, config):\n        super().__init__(config)\n        # Ensure the internal self-attention uses our custom implementation\n        self.self = UTGDiffSelfAttention(config)\n\n    def forward(self, hidden_states, attention_mask=None, edge_bias=None, **kwargs):\n        # The crucial step: pass the edge_bias into the custom self-attention module\n        self_outputs = self.self(\n            hidden_states,\n            attention_mask,\n            edge_bias=edge_bias,\n            **kwargs\n        )\n        # Pass through the standard output layer (which is part of RobertaAttention)\n        attention_output = self.output(self_outputs[0], hidden_states)\n        return (attention_output,) + self_outputs[1:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T15:30:02.192611Z","iopub.execute_input":"2025-11-29T15:30:02.192923Z","iopub.status.idle":"2025-11-29T15:30:02.198307Z","shell.execute_reply.started":"2025-11-29T15:30:02.192903Z","shell.execute_reply":"2025-11-29T15:30:02.197465Z"}},"outputs":[],"execution_count":90},{"cell_type":"code","source":"class UTGDiffLayer(RobertaLayer):\n    def __init__(self, config):\n        super().__init__(config)\n        # FIX: Use the custom attention wrapper here\n        self.attention = UTGDiffAttention(config)\n\n    def forward(self, hidden_states, attention_mask=None, **kwargs):\n        edge_bias = kwargs.pop('edge_bias', None)\n        \n        # Now self.attention is UTGDiffAttention and correctly accepts edge_bias\n        self_attention_outputs = self.attention(\n            hidden_states,\n            attention_mask=attention_mask,\n            edge_bias=edge_bias\n        )\n        attention_output = self_attention_outputs[0]\n        layer_output = self.feed_forward_chunk(attention_output)\n        return (layer_output,) + self_attention_outputs[1:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T15:30:11.202661Z","iopub.execute_input":"2025-11-29T15:30:11.202944Z","iopub.status.idle":"2025-11-29T15:30:11.207979Z","shell.execute_reply.started":"2025-11-29T15:30:11.202925Z","shell.execute_reply":"2025-11-29T15:30:11.207272Z"}},"outputs":[],"execution_count":91},{"cell_type":"code","source":"class UTGDiffEncoder(RobertaEncoder):\n    def __init__(self, config):\n        super().__init__(config)\n        # Use custom layers\n        self.layer = nn.ModuleList([UTGDiffLayer(config) for _ in range(config.num_hidden_layers)])\n        \n    def forward(self, hidden_states, **kwargs):\n        edge_bias = kwargs.pop('edge_bias', None)\n        \n        for layer_module in self.layer:\n            layer_outputs = layer_module(\n                hidden_states, \n                edge_bias=edge_bias,\n                **kwargs\n            )\n            hidden_states = layer_outputs[0]\n        \n        return (hidden_states,)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T15:30:13.920156Z","iopub.execute_input":"2025-11-29T15:30:13.920973Z","iopub.status.idle":"2025-11-29T15:30:13.925799Z","shell.execute_reply.started":"2025-11-29T15:30:13.920947Z","shell.execute_reply":"2025-11-29T15:30:13.925021Z"}},"outputs":[],"execution_count":92},{"cell_type":"code","source":"class UTGDiffModel(RobertaPreTrainedModel):\n    def __init__(self, config, roberta_base_model=None):\n        super().__init__(config)\n        \n        if roberta_base_model is not None:\n            self.roberta = roberta_base_model\n            \n            # Transfer weights and inject custom encoder\n            old_encoder = self.roberta.encoder\n            new_encoder = UTGDiffEncoder(config)\n            \n            # The layers are structurally the same, so state_dict loading should work\n            new_encoder.load_state_dict(old_encoder.state_dict(), strict=False)\n            self.roberta.encoder = new_encoder\n        else:\n            self.roberta = RobertaModel(config, add_pooling_layer=False)\n            self.roberta.encoder = UTGDiffEncoder(config)\n            self.roberta.resize_token_embeddings(FULL_VOCAB_SIZE) \n\n        self.node_prediction_head = nn.Linear(config.hidden_size, NODE_VOCAB_SIZE)\n        self.edge_prediction_head = nn.Linear(config.hidden_size, EDGE_VOCAB_SIZE)\n        self.text_prediction_head = nn.Linear(config.hidden_size, TEXT_VOCAB_SIZE)\n        \n        self.bond_embedding = nn.Embedding(EDGE_VOCAB_SIZE, config.hidden_size // config.num_attention_heads)\n        self.bond_projection = nn.Linear(config.hidden_size // config.num_attention_heads, config.num_attention_heads)\n\n    def generate_edge_bias(self, edge_tensor, seq_len):\n        batch_size, N_v = edge_tensor.shape[:2]\n        L_s = seq_len - N_v       \n        \n        edge_features = self.bond_embedding(edge_tensor)\n        B_graph = self.bond_projection(edge_features).permute(0, 3, 1, 2)\n        \n        B_full = torch.zeros(batch_size, self.config.num_attention_heads, seq_len, seq_len, device=edge_tensor.device)\n        B_full[:, :, L_s:, L_s:] = B_graph\n        return B_full\n\n    def forward(self, input_ids, attention_mask, edge_tensor, text_token_indices, node_token_indices):\n        seq_len = input_ids.size(1)\n        edge_bias = self.generate_edge_bias(edge_tensor, seq_len)\n        \n        # Manually call embeddings and encoder to pass edge_bias\n        embedding_output = self.roberta.embeddings(input_ids=input_ids)\n        extended_attention_mask = self.get_extended_attention_mask(attention_mask, input_ids.shape)\n        \n        encoder_outputs = self.roberta.encoder(\n            embedding_output,\n            attention_mask=extended_attention_mask,\n            edge_bias=edge_bias \n        )\n        \n        sequence_output = encoder_outputs[0]\n        \n        batch_size, N_max = edge_tensor.shape[:2]\n        L_s_max = input_ids.size(1) - N_max\n        hidden_size = sequence_output.size(-1)\n\n        node_logits = torch.zeros(batch_size, N_max, NODE_VOCAB_SIZE, device=DEVICE)\n        text_logits = torch.zeros(batch_size, L_s_max, TEXT_VOCAB_SIZE, device=DEVICE)\n        edge_logits = torch.zeros(batch_size, N_max, N_max, EDGE_VOCAB_SIZE, device=DEVICE)\n        \n        for i in range(batch_size):\n            v_idx = node_token_indices[i]\n            N_i = len(v_idx)\n            node_h = sequence_output[i, v_idx, :]\n            node_logits[i, :N_i] = self.node_prediction_head(node_h) \n\n            node_A = node_h.unsqueeze(1).expand(-1, N_i, hidden_size) \n            node_B = node_h.unsqueeze(0).expand(N_i, -1, hidden_size) \n            edge_logits[i, :N_i, :N_i] = self.edge_prediction_head(node_A * node_B)\n            \n            s_idx = text_token_indices[i]\n            text_logits[i, :len(s_idx)] = self.text_prediction_head(sequence_output[i, s_idx, :])\n        \n        return node_logits, edge_logits, text_logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T15:30:55.191206Z","iopub.execute_input":"2025-11-29T15:30:55.191817Z","iopub.status.idle":"2025-11-29T15:30:55.202857Z","shell.execute_reply.started":"2025-11-29T15:30:55.191791Z","shell.execute_reply":"2025-11-29T15:30:55.201872Z"}},"outputs":[],"execution_count":93},{"cell_type":"code","source":"def compute_utgdiff_loss(node_logits, edge_logits, text_logits, V_true, E_true, S_true, text_mask):\n    L_nodes = F.cross_entropy(node_logits.permute(0, 2, 1), V_true, ignore_index=MASK_NODE_ID)\n    L_edges = F.cross_entropy(edge_logits.permute(0, 3, 1, 2), E_true) \n    \n    active_loss = text_mask.view(-1) == 1\n    L_text = F.cross_entropy(\n        text_logits.transpose(1, 2).reshape(-1, TEXT_VOCAB_SIZE)[active_loss], \n        S_true.view(-1)[active_loss]\n    )\n    return L_nodes + L_edges + L_text, L_nodes, L_edges, L_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T15:31:04.557171Z","iopub.execute_input":"2025-11-29T15:31:04.557911Z","iopub.status.idle":"2025-11-29T15:31:04.563603Z","shell.execute_reply.started":"2025-11-29T15:31:04.557880Z","shell.execute_reply":"2025-11-29T15:31:04.562833Z"}},"outputs":[],"execution_count":94},{"cell_type":"code","source":"class DiffusionForwardModel(nn.Module):\n    def __init__(self, mask_node_id, mask_edge_id, steps):\n        super().__init__()\n        self.mask_node_id = mask_node_id\n        \n    def forward(self, V_true, E_true, t):\n        mask_V = torch.rand_like(V_true.float()) < 0.3\n        V_t = torch.where(mask_V, torch.full_like(V_true, self.mask_node_id), V_true)\n        \n        mask_E = torch.rand_like(E_true.float()) < 0.3\n        E_t = torch.where(mask_E, torch.zeros_like(E_true), E_true)\n        return V_t, E_t","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T15:31:11.437023Z","iopub.execute_input":"2025-11-29T15:31:11.437304Z","iopub.status.idle":"2025-11-29T15:31:11.442627Z","shell.execute_reply.started":"2025-11-29T15:31:11.437284Z","shell.execute_reply":"2025-11-29T15:31:11.441740Z"}},"outputs":[],"execution_count":95},{"cell_type":"code","source":"LEARNING_RATE = 1e-5\n\ndef train_model(data_path, batch_size, num_epochs):\n    print(f\"Initializing Model on {DEVICE}...\")\n    \n    # 1. Load RoBERTa\n    base_roberta = RobertaModel.from_pretrained('roberta-base', add_pooling_layer=False)\n    if base_roberta.config.vocab_size != FULL_VOCAB_SIZE:\n        print(f\"Resizing embeddings to {FULL_VOCAB_SIZE}...\")\n        base_roberta.resize_token_embeddings(FULL_VOCAB_SIZE)\n        \n    # 2. Instantiate UTGDiffModel\n    config = base_roberta.config\n    config.vocab_size = FULL_VOCAB_SIZE\n    model = UTGDiffModel(config, roberta_base_model=base_roberta).to(DEVICE)\n    \n    # 3. Setup\n    diffusion_forward = DiffusionForwardModel(MASK_NODE_ID, 0, DIFFUSION_STEPS)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n    \n    # 4. Load Data\n    train_dataset = PubChemDataset(data_path)\n    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=utgdiff_collate_fn)\n\n    print(f\"Starting training: {num_epochs} epochs, {len(train_dataset)} samples.\")\n    \n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0\n        \n        for step, batch in enumerate(train_dataloader):\n            optimizer.zero_grad()\n            \n            # Forward Diffusion\n            t = torch.randint(1, DIFFUSION_STEPS + 1, (batch['input_ids_SV'].size(0),), device=DEVICE)\n            V_t, E_t = diffusion_forward(batch['V_true_0'], batch['E_true_0'], t)\n\n            # Reconstruct Unified Input\n            input_ids_SV_t = batch['input_ids_SV'].clone() \n            for i in range(input_ids_SV_t.size(0)):\n                v_indices = batch['node_indices'][i]\n                input_ids_SV_t[i, v_indices] = V_t[i, :len(v_indices)]\n                \n            # Denoising Network Forward Pass\n            node_logits, edge_logits, text_logits = model(\n                input_ids_SV_t, batch['attention_mask_SV'], E_t, \n                batch['text_indices'], batch['node_indices']\n            )\n            \n            # Compute Loss\n            loss_total, L_v, L_e, L_s = compute_utgdiff_loss(\n                node_logits, edge_logits, text_logits, \n                batch['V_true_0'], batch['E_true_0'], batch['S_true_0'], batch['S_attention_mask'] \n            )\n            \n            loss_total.backward()\n            optimizer.step()\n            total_loss += loss_total.item()\n\n            if (step + 1) % 10 == 0:\n                print(f\"  Step {step+1}: Loss: {loss_total.item():.4f} (V: {L_v:.3f}, E: {L_e:.3f}, S: {L_s:.3f})\")\n        \n        print(f\"Epoch {epoch+1} finished. Avg Loss: {total_loss / len(train_dataloader):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T15:31:45.980024Z","iopub.execute_input":"2025-11-29T15:31:45.980310Z","iopub.status.idle":"2025-11-29T15:31:45.989297Z","shell.execute_reply.started":"2025-11-29T15:31:45.980287Z","shell.execute_reply":"2025-11-29T15:31:45.988495Z"}},"outputs":[],"execution_count":96},{"cell_type":"code","source":"KAGGLE_DATA_PATH = \"/kaggle/input/pubchem324k-dataset/train.pt\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T15:31:49.019147Z","iopub.execute_input":"2025-11-29T15:31:49.019502Z","iopub.status.idle":"2025-11-29T15:31:49.023696Z","shell.execute_reply.started":"2025-11-29T15:31:49.019467Z","shell.execute_reply":"2025-11-29T15:31:49.022954Z"}},"outputs":[],"execution_count":97},{"cell_type":"code","source":"tokenizer = RobertaTokenizer.from_pretrained('roberta-base')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T15:31:51.002546Z","iopub.execute_input":"2025-11-29T15:31:51.002823Z","iopub.status.idle":"2025-11-29T15:31:51.424754Z","shell.execute_reply.started":"2025-11-29T15:31:51.002803Z","shell.execute_reply":"2025-11-29T15:31:51.423982Z"}},"outputs":[],"execution_count":98},{"cell_type":"code","source":"train_model(KAGGLE_DATA_PATH, batch_size=64, num_epochs=20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T15:31:54.649707Z","iopub.execute_input":"2025-11-29T15:31:54.650255Z","iopub.status.idle":"2025-11-29T15:31:58.330998Z","shell.execute_reply.started":"2025-11-29T15:31:54.650232Z","shell.execute_reply":"2025-11-29T15:31:58.329671Z"}},"outputs":[{"name":"stdout","text":"Initializing Model on cuda...\nResizing embeddings to 50278...\nStarting training: 20 epochs, 12000 samples.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/3705900058.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKAGGLE_DATA_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_47/4294736948.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(data_path, batch_size, num_epochs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;31m# Denoising Network Forward Pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             node_logits, edge_logits, text_logits = model(\n\u001b[0m\u001b[1;32m     46\u001b[0m                 \u001b[0minput_ids_SV_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask_SV'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE_t\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_indices'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'node_indices'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/1697820658.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, edge_tensor, text_token_indices, node_token_indices)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mextended_attention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_extended_attention_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         encoder_outputs = self.roberta.encoder(\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/824744492.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, **kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer_module\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m     12\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0medge_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0medge_bias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/3844607344.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# Now self.attention is UTGDiffAttention and correctly accepts edge_bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/658799211.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, edge_bias, **kwargs)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# The crucial step: pass the edge_bias into the custom self-attention module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/147171590.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, edge_bias, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_context_layer_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0moutput_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0moutput_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_layer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmixed_query_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1926\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1927\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1928\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m   1929\u001b[0m             \u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1930\u001b[0m         )\n","\u001b[0;31mAttributeError\u001b[0m: 'UTGDiffSelfAttention' object has no attribute 'output'"],"ename":"AttributeError","evalue":"'UTGDiffSelfAttention' object has no attribute 'output'","output_type":"error"}],"execution_count":99}]}