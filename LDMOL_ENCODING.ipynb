{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP54SRMeNzyI3tu+hLho0Bg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quanghuydsai/Project-III/blob/main/LDMOL_ENCODING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IBAPwhR1U99",
        "outputId": "7dce0bb9-8b37-4159-df3e-187d76a93b38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.6)\n",
            "Requirement already satisfied: pathlib in /usr/local/lib/python3.12/dist-packages (1.0.1)\n",
            "Requirement already satisfied: rdkit in /usr/local/lib/python3.12/dist-packages (2025.9.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from rdkit) (11.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2026.1.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch transformers pathlib rdkit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XSGkYaCCKBd",
        "outputId": "0170544c-e07a-4706-a672-132d68b9daee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import re\n",
        "class regexTokenizer():\n",
        "    def __init__(self,vocab_path='/content/drive/MyDrive/LDMOL/vocab_bpe_300_sc.txt',max_len=127):\n",
        "        with open(vocab_path,'r') as f:\n",
        "            x = f.readlines()\n",
        "            x = [xx.replace('##', '') for xx in x]\n",
        "            x2 = x.copy()\n",
        "        x2.sort(key=len, reverse=True)\n",
        "        pattern = \"(\"+\"|\".join(re.escape(token).strip()[:-1] for token in x2)+\")\"\n",
        "        self.rg = re.compile(pattern)\n",
        "\n",
        "        self.idtotok  = { cnt:i.strip() for cnt,i in enumerate(x)}\n",
        "        self.vocab_size = len(self.idtotok) #SOS, EOS, pad\n",
        "        self.toktoid = { v:k for k,v in self.idtotok.items()}\n",
        "        self.max_len = max_len\n",
        "        self.cls_token_id = self.toktoid['[CLS]']\n",
        "        self.sep_token_id = self.toktoid['[SEP]']\n",
        "        self.pad_token_id = self.toktoid['[PAD]']\n",
        "\n",
        "    def decode_one(self, iter):\n",
        "        if self.sep_token_id in iter:   iter = iter[:(iter == self.sep_token_id).nonzero(as_tuple=True)[0][0].item()]\n",
        "        # return \"\".join([self.ind2Letter(i) for i in iter]).replace('[SOS]','').replace('[EOS]','').replace('[PAD]','')\n",
        "        return \"\".join([self.idtotok[i.item()] for i in iter[1:]])\n",
        "\n",
        "    def decode(self,ids:torch.tensor):\n",
        "        if len(ids.shape)==1:\n",
        "            return [self.decode_one(ids)]\n",
        "        else:\n",
        "            smiles  = []\n",
        "            for i in ids:\n",
        "                smiles.append(self.decode_one(i))\n",
        "            return smiles\n",
        "    def __len__(self):\n",
        "        return self.vocab_size\n",
        "\n",
        "    def __call__(self,smis:list, truncation='max_len'):\n",
        "        tensors = []\n",
        "        lengths = []\n",
        "        if type(smis) is str:\n",
        "            smis = [smis]\n",
        "        for i in smis:\n",
        "            length, tensor = self.encode_one(i)\n",
        "            tensors.append(tensor)\n",
        "            lengths.append(length)\n",
        "        output = torch.concat(tensors,dim=0)\n",
        "        if truncation == 'max_len':\n",
        "            return output\n",
        "        elif truncation == 'longest':\n",
        "            return output[:, :max(lengths)]\n",
        "        else:\n",
        "            raise ValueError('truncation should be either max_len or longest')\n",
        "\n",
        "    def encode_one(self, smi):\n",
        "        smi = '[CLS]' + smi + '[SEP]'\n",
        "        res = [self.toktoid[i] for i in self.rg.findall(smi)]\n",
        "        token_length = len(res)\n",
        "        if token_length < self.max_len:\n",
        "            res += [self.pad_token_id]*(self.max_len-len(res))\n",
        "        else:\n",
        "            res = res[:self.max_len]\n",
        "            # res[-1] = self.sep_token_id\n",
        "        return token_length, torch.LongTensor([res])"
      ],
      "metadata": {
        "id": "I-q8EE3n4s1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rdkit import RDLogger\n",
        "RDLogger.DisableLog('rdApp.*')"
      ],
      "metadata": {
        "id": "vFRfmGZJ49De"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import abstractmethod\n",
        "from random import shuffle\n",
        "from typing import Any\n",
        "from typing import Iterable\n",
        "from typing import List\n",
        "from typing import Union\n",
        "\n",
        "import numpy as np\n",
        "from rdkit import Chem"
      ],
      "metadata": {
        "id": "YLs430W-4_yl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Mol = Chem.Mol\n",
        "class Augmenter:\n",
        "    \"\"\"An abstract base class for molecular augmenters.\n",
        "\n",
        "    The class has one method, `augment`, which is overriden by child classes.\n",
        "    It is possible to call the class with either a list of molecules or a single\n",
        "    molecules. This input will then be passed to `augment` and the augmented\n",
        "    molecule(s) will be returned.\n",
        "    The Boolean \".active\" property can be set to toggle augmentation.\n",
        "\n",
        "    :param active: Whether the augmentation should be active or not, defaults to True.\n",
        "    :param augment_prob: if lower than 1, it is used to randomly turn-off augmentation on an individual basis\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, active: bool = True, augment_prob: float = 1.0) -> None:\n",
        "        self.active = active\n",
        "        self.augment_prob = augment_prob\n",
        "\n",
        "    def __call__(self, data: Union[Iterable[Any], Any]) -> List[Any]:\n",
        "        \"\"\"Augments either a list of Anys or a single molecule by making sure\n",
        "        the input is put into a `List` and then passed to the `augment` function.\n",
        "\n",
        "        :param data: Either a list of molecules or a single molecules to be augmented.\n",
        "\n",
        "        :return: A list of augmented molecules.\n",
        "        \"\"\"\n",
        "        # Str is Iterable but must be encapsulated (e.g. single SMILES string)\n",
        "        if not isinstance(data, Iterable) or isinstance(data, str):\n",
        "            data = [data]\n",
        "\n",
        "        return self.augment(data)\n",
        "\n",
        "    @abstractmethod\n",
        "    def _augment(self, data: Iterable[Any]) -> List[Any]:\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def augment(self, data: Iterable[Any]) -> List[Any]:\n",
        "        \"\"\"\n",
        "        Augment a given list\n",
        "\n",
        "        :param data: a list of molecules to be augmented.\n",
        "        :return: A list of augmented molecules.\n",
        "        \"\"\"\n",
        "        if self.active:\n",
        "            return self._augment(data)\n",
        "        return list(data)\n",
        "class MolAugmenter(Augmenter):\n",
        "    \"\"\"\n",
        "    Augmenter that works on RDKit Mol objects\n",
        "    \"\"\"\n",
        "\n",
        "    def randomize_mols_restricted(self, mols: Iterable[Mol]) -> List[Mol]:\n",
        "        \"\"\"Randomizes the atom ordering of a list of RDKit molecules (`rdkit.Chem.Mol`:s).\n",
        "\n",
        "        :param mols: List of RDKit molecules to be augmented.\n",
        "        :return:  List of augmented RDKit molecules.\n",
        "        \"\"\"\n",
        "        return list(map(self.randomize_mol_restricted, mols))\n",
        "\n",
        "    def randomize_mol_restricted(self, mol: Mol) -> Mol:\n",
        "        \"\"\"Randomize the atom ordering of a RDKit molecule (`rdkit.Chem.Mol`).\n",
        "\n",
        "        :param mol:  RDKit molecule to get a randomized atom order.\n",
        "        :return: RDKit molecule object with a randomized atom-order.\n",
        "        \"\"\"\n",
        "        # Standard shuffle surprisingly leads to 35% slower code.\n",
        "        if self.augment_prob < np.random.rand():\n",
        "            return mol\n",
        "        atom_order: List[int] = list(range(mol.GetNumAtoms()))\n",
        "        np.random.shuffle(atom_order)\n",
        "        return Chem.RenumberAtoms(mol, atom_order)\n",
        "\n",
        "    def _augment(self, data: Iterable[Mol]) -> List[Mol]:\n",
        "        \"\"\"Randomizes `RDKit molecules by shuffling the atom order.\n",
        "\n",
        "        :param data: List of RDKit molecules to be randomized.\n",
        "        :return:  A list of randomized molecules.\n",
        "        \"\"\"\n",
        "        return self.randomize_mols_restricted(data)"
      ],
      "metadata": {
        "id": "Tcr-QgDz5kmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import random\n",
        "from rdkit import Chem, RDLogger\n",
        "from rdkit.Chem.EnumerateStereoisomers import EnumerateStereoisomers"
      ],
      "metadata": {
        "id": "1FRd7f7m51Ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SMILESDataset_pretrain(Dataset):\n",
        "    def __init__(self, data_path, data_length=None, shuffle=False, is_train=True):\n",
        "        if data_length is not None:\n",
        "            with open(data_path, 'r') as f:\n",
        "                for _ in range(data_length[0]):\n",
        "                    f.readline()\n",
        "                lines = []\n",
        "                for _ in range(data_length[1] - data_length[0]):\n",
        "                    lines.append(f.readline())\n",
        "        else:\n",
        "            with open(data_path, 'r') as f:\n",
        "                lines = f.readlines()\n",
        "\n",
        "        self.data = [l.strip() for l in lines]\n",
        "\n",
        "        if shuffle:\n",
        "            random.shuffle(self.data)\n",
        "\n",
        "        self.train = is_train\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        smiles = self.data[index].split('\\t')[0]\n",
        "        smiles2 = smiles\n",
        "        if random.random() > 0.:\n",
        "            try:\n",
        "                mol = Chem.MolFromSmiles(smiles)\n",
        "                sc_list = list(EnumerateStereoisomers(mol))\n",
        "                if self.train and len(sc_list) > 1:\n",
        "                    mol, mol2 = random.sample(sc_list, k=2)\n",
        "                    smiles = Chem.MolToSmiles(mol, canonical=True, isomericSmiles=True)\n",
        "                    smiles2 = Chem.MolToSmiles(mol2, canonical=True, isomericSmiles=True)\n",
        "                else:\n",
        "                    mol = random.choice(sc_list)\n",
        "                    smiles = Chem.MolToSmiles(mol, canonical=True, isomericSmiles=True)\n",
        "            except:\n",
        "                pass\n",
        "        if self.train and smiles2 != smiles:\n",
        "            return '[CLS]'+smiles+'Q[CLS]'+smiles2\n",
        "        return '[CLS]' + smiles"
      ],
      "metadata": {
        "id": "93f4LZvv6KyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.models.bert.configuration_bert import BertConfig"
      ],
      "metadata": {
        "id": "06ynLSif6wV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm torch_geometric"
      ],
      "metadata": {
        "id": "_MG39zUK63_F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e26a1dcb-8628-4ebf-cec1-923617c70440"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.7.0-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.13.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.32.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch_geometric) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2026.1.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch_geometric) (4.15.0)\n",
            "Downloading torch_geometric-2.7.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.data import InMemoryDataset\n",
        "class PubChemDataset(InMemoryDataset):\n",
        "    def __init__(self, path):\n",
        "        super(PubChemDataset, self).__init__()\n",
        "        self.data, self.slices = torch.load(path, weights_only=False)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.get(idx)"
      ],
      "metadata": {
        "id": "UZGkWG1v9yks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_dataset = PubChemDataset('/content/drive/MyDrive/LDMOL/pretrain.pt')"
      ],
      "metadata": {
        "id": "f2QkuGSc-XKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ldmol_dataset = []\n",
        "for data in raw_dataset:\n",
        "    ldmol_dataset.append({\n",
        "            'smiles': data.smiles,\n",
        "            'text': data.text\n",
        "        })"
      ],
      "metadata": {
        "id": "eTpQ5r08_KI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ldmol_dataset[0:2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dPSJSzs_UId",
        "outputId": "e3ae43d8-b475-46da-a68f-0d239e02f663"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'smiles': 'CC(=O)OC(CC(=O)[O-])C[N+](C)(C)C', 'text': 'The molecule is an O-acylcarnitine having acetyl as the acyl substituent. It has a role as a human metabolite. It is functionally related to an acetic acid. It is a conjugate base of an O-acetylcarnitinium.\\nThe molecule is a natural product found in Pseudo-nitzschia multistriata, Euglena gracilis, and other organisms with data available.\\nThe molecule is a metabolite found in or produced by Saccharomyces cerevisiae.\\nAn acetic acid ester of CARNITINE that facilitates movement of ACETYL COA into the matrices of mammalian MITOCHONDRIA during the oxidation of FATTY ACIDS.'}, {'smiles': 'C(CCl)C(F)(F)F', 'text': '3-chloro-1,1,1-trifluoropropane appears as a colorless odorless nonflammable liquid. Poisonous by inhalation. Emits toxic fumes of chlorine and fluorine when heated to decomposition.'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from transformers import BertModel\n",
        "from rdkit import Chem\n",
        "import tqdm"
      ],
      "metadata": {
        "id": "NGto7A7E_2qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BERT_CONFIG_DICT = {\n",
        "    \"attention_probs_dropout_prob\": 0.1,\n",
        "    \"hidden_act\": \"gelu\",\n",
        "    \"hidden_dropout_prob\": 0.1,\n",
        "    \"hidden_size\": 1024,\n",
        "    \"initializer_range\": 0.02,\n",
        "    \"intermediate_size\": 3072,\n",
        "    \"layer_norm_eps\": 1e-12,\n",
        "    \"max_position_embeddings\": 512,\n",
        "    \"model_type\": \"bert\",\n",
        "    \"num_attention_heads\": 16,\n",
        "    \"num_hidden_layers\": 12,\n",
        "    \"pad_token_id\": 0,\n",
        "    \"type_vocab_size\": 2,\n",
        "    \"vocab_size\": 300\n",
        "}"
      ],
      "metadata": {
        "id": "vkdMpun8_92E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ListDataset(Dataset):\n",
        "    def __init__(self, data_list):\n",
        "        self.data = data_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]"
      ],
      "metadata": {
        "id": "QxHYz2QlAAsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_CONFIG = {\n",
        "    'embed_dim': 256,\n",
        "    'batch_size': 32, # Giảm xuống nếu bị tràn bộ nhớ GPU trên Colab\n",
        "    'temp': 0.07,\n",
        "    'queue_size': 16384,\n",
        "    'momentum': 0.995,\n",
        "    'alpha': 0.4,\n",
        "    'lr': 1e-4,\n",
        "    'weight_decay': 0.02,\n",
        "    'epochs': 5,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "}"
      ],
      "metadata": {
        "id": "tHYLcV_pAD_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LDMolEncoder(nn.Module):\n",
        "    def __init__(self, config, bert_config_dict):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # Khởi tạo BertConfig từ dict\n",
        "        bert_config = BertConfig(**bert_config_dict)\n",
        "        self.text_encoder = BertModel(config=bert_config)\n",
        "\n",
        "        text_width = self.text_encoder.config.hidden_size\n",
        "        self.text_proj = nn.Linear(text_width, config['embed_dim'])\n",
        "        self.aug = MolAugmenter()\n",
        "\n",
        "        # Momentum Models\n",
        "        self.text_encoder_m = BertModel(config=bert_config)\n",
        "        self.text_proj_m = nn.Linear(text_width, config['embed_dim'])\n",
        "\n",
        "        # Đóng băng gradient cho momentum models\n",
        "        for p in self.text_encoder_m.parameters(): p.requires_grad = False\n",
        "        for p in self.text_proj_m.parameters(): p.requires_grad = False\n",
        "\n",
        "        self.copy_params()\n",
        "\n",
        "        # Contrastive Queue\n",
        "        self.temp = nn.Parameter(torch.ones([]) * config['temp'])\n",
        "        self.register_buffer(\"text1_queue\", torch.randn(config['embed_dim'], config['queue_size']))\n",
        "        self.register_buffer(\"text2_queue\", torch.randn(config['embed_dim'], config['queue_size']))\n",
        "        self.register_buffer(\"queue_ptr\", torch.zeros(1, dtype=torch.long))\n",
        "\n",
        "        self.text1_queue = F.normalize(self.text1_queue, dim=0)\n",
        "        self.text2_queue = F.normalize(self.text2_queue, dim=0)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def copy_params(self):\n",
        "        for p, p_m in zip(self.text_encoder.parameters(), self.text_encoder_m.parameters()):\n",
        "            p_m.data.copy_(p.data)\n",
        "        for p, p_m in zip(self.text_proj.parameters(), self.text_proj_m.parameters()):\n",
        "            p_m.data.copy_(p.data)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _momentum_update(self):\n",
        "        m = self.config['momentum']\n",
        "        for p, p_m in zip(self.text_encoder.parameters(), self.text_encoder_m.parameters()):\n",
        "            p_m.data = p_m.data * m + p.data * (1. - m)\n",
        "        for p, p_m in zip(self.text_proj.parameters(), self.text_proj_m.parameters()):\n",
        "            p_m.data = p_m.data * m + p.data * (1. - m)\n",
        "\n",
        "    def forward(self, ids1, mask1, ids2, mask2, alpha=0):\n",
        "        # Cập nhật momentum model\n",
        "        self._momentum_update()\n",
        "\n",
        "        # Trích xuất đặc trưng hiện tại\n",
        "        out1 = self.text_encoder(ids1, attention_mask=mask1).last_hidden_state[:, 0, :]\n",
        "        feat1 = F.normalize(self.text_proj(out1), dim=-1)\n",
        "\n",
        "        out2 = self.text_encoder(ids2, attention_mask=mask2).last_hidden_state[:, 0, :]\n",
        "        feat2 = F.normalize(self.text_proj(out2), dim=-1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            self.temp.clamp_(0.07, 0.5)\n",
        "\n",
        "            # Momentum features\n",
        "            m_out1 = self.text_encoder_m(ids1, attention_mask=mask1).last_hidden_state[:, 0, :]\n",
        "            feat1_m = F.normalize(self.text_proj_m(m_out1), dim=-1)\n",
        "            feat1_all = torch.cat([feat1_m.t(), self.text1_queue.clone().detach()], dim=1)\n",
        "\n",
        "            m_out2 = self.text_encoder_m(ids2, attention_mask=mask2).last_hidden_state[:, 0, :]\n",
        "            feat2_m = F.normalize(self.text_proj_m(m_out2), dim=-1)\n",
        "            feat2_all = torch.cat([feat2_m.t(), self.text2_queue.clone().detach()], dim=1)\n",
        "\n",
        "            # Targets\n",
        "            batch_size = feat1.size(0)\n",
        "            # Tạo ma trận mục tiêu: [batch_size, batch_size + queue_size]\n",
        "            sim_targets = torch.zeros(batch_size, feat1_all.size(1)).to(feat1.device)\n",
        "            # Fill diagonal phần batch hiện tại với 1\n",
        "            sim_targets.fill_diagonal_(1)\n",
        "\n",
        "            sim_21_m = feat2_m @ feat1_all / self.temp\n",
        "            sim_21_targets = alpha * F.softmax(sim_21_m, dim=1) + (1 - alpha) * sim_targets\n",
        "\n",
        "            sim_12_m = feat1_m @ feat2_all / self.temp\n",
        "            sim_12_targets = alpha * F.softmax(sim_12_m, dim=1) + (1 - alpha) * sim_targets\n",
        "\n",
        "        # Tính Loss\n",
        "        sim_21 = feat2 @ feat1_all / self.temp\n",
        "        sim_12 = feat1 @ feat2_all / self.temp\n",
        "\n",
        "        loss_21 = -torch.sum(F.log_softmax(sim_21, dim=1) * sim_21_targets, dim=1).mean()\n",
        "        loss_12 = -torch.sum(F.log_softmax(sim_12, dim=1) * sim_12_targets, dim=1).mean()\n",
        "\n",
        "        loss_ita = (loss_21 + loss_12) / 2\n",
        "\n",
        "        # Cập nhật Queue\n",
        "        self._dequeue_and_enqueue(feat1_m, feat2_m)\n",
        "\n",
        "        return loss_ita\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _dequeue_and_enqueue(self, feat1, feat2):\n",
        "        batch_size = feat1.shape[0]\n",
        "        ptr = int(self.queue_ptr)\n",
        "\n",
        "        # Nếu batch cuối cùng nhỏ hơn queue_size còn lại, chỉ lấy phần vừa đủ\n",
        "        space = self.config['queue_size'] - ptr\n",
        "        actual_batch = min(batch_size, space)\n",
        "\n",
        "        self.text1_queue[:, ptr:ptr + actual_batch] = feat1[:actual_batch].T\n",
        "        self.text2_queue[:, ptr:ptr + actual_batch] = feat2[:actual_batch].T\n",
        "\n",
        "        ptr = (ptr + actual_batch) % self.config['queue_size']\n",
        "        self.queue_ptr[0] = ptr"
      ],
      "metadata": {
        "id": "d-FPpGWDANEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR\n",
        "\n",
        "def plot_metrics(steps, losses, lrs, epoch, save_path=None):\n",
        "    \"\"\"Vẽ và lưu biểu đồ sau mỗi epoch\"\"\"\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # Biểu đồ Loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(steps, losses, color='#1f77b4', label='Train Loss', alpha=0.8)\n",
        "    # Thêm đường trung bình trượt để dễ quan sát xu hướng\n",
        "    if len(losses) > 50:\n",
        "        window = 50\n",
        "        means = [sum(losses[i:i+window])/window for i in range(len(losses)-window)]\n",
        "        plt.plot(steps[window:], means, color='orange', label='Trend (Moving Avg)')\n",
        "\n",
        "    plt.title(f'Loss Progression (Epoch {epoch})')\n",
        "    plt.xlabel('Global Step')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.legend()\n",
        "\n",
        "    # Biểu đồ Learning Rate\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(steps, lrs, color='#d62728', label='Learning Rate')\n",
        "    plt.title(f'LR Schedule (Epoch {epoch})')\n",
        "    plt.xlabel('Global Step')\n",
        "    plt.ylabel('LR')\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "1JQqu77VAVC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_with_best_checkpoint(ldmol_dataset, tokenizer_path='/content/drive/MyDrive/LDMOL/vocab_bpe_300_sc.txt'):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    save_path = '/content/drive/MyDrive/LDMOL/best_encoder.pt'\n",
        "    tokenizer = regexTokenizer(vocab_path=tokenizer_path, max_len=127)\n",
        "    train_ds = ListDataset(ldmol_dataset)\n",
        "    loader = DataLoader(train_ds, batch_size=TRAIN_CONFIG['batch_size'], shuffle=True, drop_last=True)\n",
        "\n",
        "    model = LDMolEncoder(TRAIN_CONFIG, BERT_CONFIG_DICT).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=TRAIN_CONFIG['lr'], weight_decay=TRAIN_CONFIG['weight_decay'])\n",
        "\n",
        "    # Scheduler: Warmup (1 ep) + Cosine (4 ep)\n",
        "    warmup_epochs = 1\n",
        "    main_epochs = 4\n",
        "    scheduler_warmup = LinearLR(optimizer, start_factor=5e-5/1e-4, end_factor=1.0, total_iters=warmup_epochs)\n",
        "    scheduler_cosine = CosineAnnealingLR(optimizer, T_max=TRAIN_CONFIG['epochs'], eta_min=1e-5)\n",
        "    scheduler = SequentialLR(optimizer, schedulers=[scheduler_warmup, scheduler_cosine], milestones=[warmup_epochs])\n",
        "\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    # Tracking variables\n",
        "    history_steps, history_losses, history_lrs = [], [], []\n",
        "    global_step = 0\n",
        "    best_loss = float('inf')  # Khởi tạo loss tốt nhất là vô cùng lớn\n",
        "\n",
        "    model.train()\n",
        "    total_epochs = warmup_epochs + main_epochs\n",
        "\n",
        "    for epoch in range(total_epochs):\n",
        "        epoch_total_loss = 0\n",
        "        num_batches = 0\n",
        "        pbar = tqdm.tqdm(loader, desc=f\"Epoch {epoch+1}/{total_epochs}\")\n",
        "\n",
        "        for batch_idx, batch in enumerate(pbar):\n",
        "            smiles_list = batch['smiles']\n",
        "            text1_input, text2_input = [], []\n",
        "            for sm in smiles_list:\n",
        "                try:\n",
        "                    mol = Chem.MolFromSmiles(sm)\n",
        "                    if mol:\n",
        "                        text1_input.append('[CLS]' + Chem.MolToSmiles(mol, canonical=True))\n",
        "                        text2_input.append('[CLS]' + Chem.MolToSmiles(model.aug([mol])[0], canonical=False))\n",
        "                except: continue\n",
        "            if not text1_input: continue\n",
        "\n",
        "            # Training step\n",
        "            ids1 = tokenizer(text1_input, truncation='longest').to(device)\n",
        "            mask1 = torch.where(ids1 == 0, 0, 1)\n",
        "            ids2 = tokenizer(text2_input, truncation='longest').to(device)\n",
        "            mask2 = torch.where(ids2 == 0, 0, 1)\n",
        "\n",
        "            alpha = TRAIN_CONFIG['alpha'] * min(1.0, batch_idx / len(loader)) if epoch == 0 else TRAIN_CONFIG['alpha']\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            with autocast():\n",
        "                loss = model(ids1, mask1, ids2, mask2, alpha=alpha)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            # Logging\n",
        "            global_step += 1\n",
        "            current_lr = optimizer.param_groups[0]['lr']\n",
        "            history_steps.append(global_step)\n",
        "            history_losses.append(loss.item())\n",
        "            history_lrs.append(current_lr)\n",
        "\n",
        "            epoch_total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "            if global_step % 100 == 0:\n",
        "                print(f\" > Step {global_step} | Loss: {loss.item():.4f} | LR: {current_lr:.2e}\")\n",
        "\n",
        "            pbar.set_postfix({'loss': f\"{loss.item():.4f}\", 'lr': f\"{current_lr:.2e}\"})\n",
        "\n",
        "        # Kết thúc epoch\n",
        "        avg_epoch_loss = epoch_total_loss / num_batches\n",
        "        scheduler.step()\n",
        "\n",
        "        # Kiểm tra và lưu model tốt nhất\n",
        "        if avg_epoch_loss < best_loss:\n",
        "            best_loss = avg_epoch_loss\n",
        "            torch.save({\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': best_loss,\n",
        "            }, save_path)\n",
        "            print(f\"--- [MỚI] Đã lưu model tốt nhất tại Epoch {epoch+1} với Avg Loss: {best_loss:.4f} ---\")\n",
        "\n",
        "        # Vẽ biểu đồ\n",
        "        plot_metrics(history_steps, history_losses, history_lrs, epoch + 1, save_path=f\"metrics_epoch_{epoch+1}.png\")\n",
        "\n",
        "    print(f\"Huấn luyện hoàn tất! Loss tốt nhất đạt được: {best_loss:.4f}\")"
      ],
      "metadata": {
        "id": "bvzCwJftBXTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_with_best_checkpoint(ldmol_dataset, tokenizer_path='/content/drive/MyDrive/LDMOL/vocab_bpe_300_sc.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "VSG4e2sOBx12",
        "outputId": "9819c4b7-a801-473b-9b1f-cac36fe155bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1827738000.py:16: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "Epoch 1/25:   0%|          | 0/9315 [00:00<?, ?it/s]/tmp/ipython-input-1827738000.py:52: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Epoch 1/25:   1%|          | 95/9315 [01:00<1:37:47,  1.57it/s, loss=5.3512, lr=5.00e-05]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3698301616.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_with_best_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mldmol_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/MyDrive/LDMOL/vocab_bpe_300_sc.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1827738000.py\u001b[0m in \u001b[0;36mtrain_with_best_checkpoint\u001b[0;34m(ldmol_dataset, tokenizer_path)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3624173743.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, ids1, mask1, ids2, mask2, alpha)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;31m# Momentum features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mm_out1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_encoder_m\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0mfeat1_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_proj_m\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_out1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mfeat1_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeat1_m\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext1_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    998\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1000\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1001\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    648\u001b[0m             \u001b[0mlayer_head_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhead_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m             layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    651\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mcache_position\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m     ) -> tuple[torch.Tensor]:\n\u001b[0;32m--> 558\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    559\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, past_key_values, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0mcache_position\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m     ) -> tuple[torch.Tensor]:\n\u001b[0;32m--> 488\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, past_key_values, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0mis_causal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_decoder\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_cross_attention\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtgt_len\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m         attn_output = torch.nn.functional.scaled_dot_product_attention(\n\u001b[0m\u001b[1;32m    414\u001b[0m             \u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[0mkey_layer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_testset = PubChemDataset('/content/drive/MyDrive/LDMOL/test.pt')"
      ],
      "metadata": {
        "id": "IUBnQnunpP65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ldmol_testset = []\n",
        "for data in raw_testset:\n",
        "    ldmol_testset.append({\n",
        "            'smiles': data.smiles,\n",
        "            'text': data.text\n",
        "        })"
      ],
      "metadata": {
        "id": "JE8NbOz4pYtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import tqdm\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "u-mIKHW_pjUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def evaluate_encoding(ldmol_testset, model_path=\"'/content/drive/MyDrive/LDMOL/best_encoder.pt'\", tokenizer_path='vocab_bpe_300_sc.txt'):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # 1. Tải Tokenizer\n",
        "    tokenizer = regexTokenizer(vocab_path=tokenizer_path, max_len=127)\n",
        "\n",
        "    # 2. Khởi tạo và tải mô hình\n",
        "    # Đảm bảo TRAIN_CONFIG và BERT_CONFIG_DICT đã được định nghĩa như các bước trước\n",
        "    model = LDMolEncoder(TRAIN_CONFIG, BERT_CONFIG_DICT).to(device)\n",
        "\n",
        "    try:\n",
        "        checkpoint = torch.load(model_path, map_location=device)\n",
        "        # Hỗ trợ cả việc load dict lưu ở bước trước hoặc chỉ state_dict thuần\n",
        "        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
        "            model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        else:\n",
        "            model.load_state_dict(checkpoint)\n",
        "        print(f\"--- Đã tải mô hình thành công từ {model_path} ---\")\n",
        "    except Exception as e:\n",
        "        print(f\"--- Lỗi khi tải mô hình: {e} ---\")\n",
        "        return\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    test_ds = ListDataset(ldmol_testset)\n",
        "    # Batch size lớn hơn một chút để đánh giá nhanh hơn\n",
        "    loader = DataLoader(test_ds, batch_size=32, shuffle=False)\n",
        "\n",
        "    similarities = []\n",
        "\n",
        "    print(\"Bắt đầu đánh giá trên bộ test...\")\n",
        "    for batch in tqdm.tqdm(loader):\n",
        "        smiles_list = batch['smiles']\n",
        "\n",
        "        text1_input, text2_input = [], []\n",
        "        for sm in smiles_list:\n",
        "            try:\n",
        "                mol = Chem.MolFromSmiles(sm)\n",
        "                if mol:\n",
        "                    # Tạo cặp tương đồng\n",
        "                    text1_input.append('[CLS]' + Chem.MolToSmiles(mol, canonical=True))\n",
        "                    text2_input.append('[CLS]' + Chem.MolToSmiles(model.aug([mol])[0], canonical=False))\n",
        "            except: continue\n",
        "\n",
        "        if not text1_input: continue\n",
        "\n",
        "        # Tokenize\n",
        "        ids1 = tokenizer(text1_input, truncation='longest').to(device)\n",
        "        mask1 = torch.where(ids1 == 0, 0, 1)\n",
        "        ids2 = tokenizer(text2_input, truncation='longest').to(device)\n",
        "        mask2 = torch.where(ids2 == 0, 0, 1)\n",
        "\n",
        "        # Trích xuất đặc trưng (Features) qua Encoder hiện tại (không dùng momentum model)\n",
        "        # Lấy vector [CLS] (index 0) và chiếu qua Projection Head\n",
        "        feat1_out = model.text_encoder(ids1, attention_mask=mask1).last_hidden_state[:, 0, :]\n",
        "        feat1 = F.normalize(model.text_proj(feat1_out), dim=-1)\n",
        "\n",
        "        feat2_out = model.text_encoder(ids2, attention_mask=mask2).last_hidden_state[:, 0, :]\n",
        "        feat2 = F.normalize(model.text_proj(feat2_out), dim=-1)\n",
        "\n",
        "        # Tính Cosine Similarity cho từng cặp trong batch (phần tử tương ứng)\n",
        "        # Vì feat đã normalize nên dot product chính là cosine similarity\n",
        "        cosine_sim = torch.sum(feat1 * feat2, dim=-1)\n",
        "        similarities.extend(cosine_sim.cpu().numpy())\n",
        "\n",
        "    # 3. Tổng kết kết quả\n",
        "    avg_sim = np.mean(similarities)\n",
        "    std_sim = np.std(similarities)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*30)\n",
        "    print(f\"KẾT QUẢ ĐÁNH GIÁ (ENCODING EVAL):\")\n",
        "    print(f\"Số lượng mẫu kiểm tra: {len(similarities)}\")\n",
        "    print(f\"Độ tương đồng Cosine trung bình: {avg_sim:.4f}\")\n",
        "    print(f\"Độ lệch chuẩn: {std_sim:.4f}\")\n",
        "    print(\"=\"*30)\n",
        "\n",
        "    return similarities"
      ],
      "metadata": {
        "id": "6qrYvve6pr8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = evaluate_encoding(ldmol_testset)"
      ],
      "metadata": {
        "id": "IvI29VYvpwHW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}