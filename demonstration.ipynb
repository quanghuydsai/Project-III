{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ce6fcde",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-02-09T06:31:21.131249Z",
     "iopub.status.busy": "2026-02-09T06:31:21.130547Z",
     "iopub.status.idle": "2026-02-09T06:31:29.036065Z",
     "shell.execute_reply": "2026-02-09T06:31:29.034822Z"
    },
    "papermill": {
     "duration": 7.916659,
     "end_time": "2026-02-09T06:31:29.038310",
     "exception": false,
     "start_time": "2026-02-09T06:31:21.121651",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\r\n",
      "Collecting rdkit\r\n",
      "  Downloading rdkit-2025.9.4-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.2 kB)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\r\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (0.8.1)\r\n",
      "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.20)\r\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.3)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.10.0)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\r\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\r\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rdkit) (2.0.2)\r\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from rdkit) (11.3.0)\r\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm) (0.23.0+cu126)\r\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm) (6.0.3)\r\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm) (0.36.0)\r\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm) (0.6.2)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (26.0rc2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.5)\r\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (1.2.1rc0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.6.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2026.1.4)\r\n",
      "Downloading rdkit-2025.9.4-cp312-cp312-manylinux_2_28_x86_64.whl (36.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.6/36.6 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: rdkit\r\n",
      "Successfully installed rdkit-2025.9.4\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torch rdkit tqdm einops timm transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07628375",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T06:31:29.056351Z",
     "iopub.status.busy": "2026-02-09T06:31:29.055597Z",
     "iopub.status.idle": "2026-02-09T06:31:48.707538Z",
     "shell.execute_reply": "2026-02-09T06:31:48.706208Z"
    },
    "papermill": {
     "duration": 19.664019,
     "end_time": "2026-02-09T06:31:48.709934",
     "exception": false,
     "start_time": "2026-02-09T06:31:29.045915",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.jit import Final\n",
    "import numpy as np\n",
    "import math\n",
    "from timm.models.vision_transformer import PatchEmbed, Attention, Mlp\n",
    "from timm.layers import use_fused_attn\n",
    "import einops\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52ef8bdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T06:31:48.727021Z",
     "iopub.status.busy": "2026-02-09T06:31:48.726063Z",
     "iopub.status.idle": "2026-02-09T06:31:48.730877Z",
     "shell.execute_reply": "2026-02-09T06:31:48.729904Z"
    },
    "papermill": {
     "duration": 0.01543,
     "end_time": "2026-02-09T06:31:48.732685",
     "exception": false,
     "start_time": "2026-02-09T06:31:48.717255",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd42369f",
   "metadata": {
    "papermill": {
     "duration": 0.007046,
     "end_time": "2026-02-09T06:31:48.747988",
     "exception": false,
     "start_time": "2026-02-09T06:31:48.740942",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Diffusion  Transformer Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f77b1e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T06:31:48.764507Z",
     "iopub.status.busy": "2026-02-09T06:31:48.764169Z",
     "iopub.status.idle": "2026-02-09T06:31:48.796885Z",
     "shell.execute_reply": "2026-02-09T06:31:48.796098Z"
    },
    "papermill": {
     "duration": 0.043999,
     "end_time": "2026-02-09T06:31:48.799069",
     "exception": false,
     "start_time": "2026-02-09T06:31:48.755070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def modulate(x, shift, scale):\n",
    "    return x * (1 + scale.unsqueeze(1)) + shift.unsqueeze(1)\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    fused_attn: Final[bool]\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim: int, dim_y: int,\n",
    "            num_heads: int = 8,\n",
    "            qkv_bias: bool = False,\n",
    "            qk_norm: bool = False,\n",
    "            attn_drop: float = 0.,\n",
    "            proj_drop: float = 0.,\n",
    "            norm_layer: nn.Module = nn.LayerNorm,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.fused_attn = use_fused_attn()\n",
    "\n",
    "        # self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.q = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.kv = nn.Linear(dim_y, dim * 2, bias=qkv_bias)\n",
    "        self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n",
    "        self.k_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor, pad_mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        B, N, C = x.shape\n",
    "        N2 = y.size(1)\n",
    "        # qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        # q, k, v = qkv.unbind(0)\n",
    "        q = self.q(x).reshape(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        # print('aa', x.shape, y.shape, pad_mask.shape, self.kv(y).shape)\n",
    "        kv = self.kv(y).reshape(B, N2, 2, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        k, v = kv.unbind(0)\n",
    "        q, k = self.q_norm(q), self.k_norm(k)\n",
    "\n",
    "        if self.fused_attn and False:\n",
    "            raise NotImplementedError\n",
    "            x = F.scaled_dot_product_attention(\n",
    "                q, k, v,\n",
    "                dropout_p=self.attn_drop.p if self.training else 0.,\n",
    "            )\n",
    "        else:\n",
    "            q = q * self.scale\n",
    "            attn = q @ k.transpose(-2, -1)  # (B, head, len_q, len_k)\n",
    "            if pad_mask is not None:\n",
    "                pad_mask = einops.repeat(pad_mask, 'B L -> B H Q L', H=attn.size(1), Q=attn.size(2)).bool()\n",
    "                attn.masked_fill_(pad_mask.logical_not(), float('-inf'))\n",
    "\n",
    "            attn = attn.softmax(dim=-1)\n",
    "            attn = self.attn_drop(attn)\n",
    "            x = attn @ v\n",
    "\n",
    "        x = x.transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        # print('cross', x.shape, y.shape, pad_mask.shape, pad_mask[0])\n",
    "        return x\n",
    "\n",
    "class TimestepEmbedder(nn.Module):\n",
    "    \"\"\"\n",
    "    Embeds scalar timesteps into vector representations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size, frequency_embedding_size=256):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(frequency_embedding_size, hidden_size, bias=True),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_size, hidden_size, bias=True),\n",
    "        )\n",
    "        self.frequency_embedding_size = frequency_embedding_size\n",
    "\n",
    "    @staticmethod\n",
    "    def timestep_embedding(t, dim, max_period=10000):\n",
    "        \"\"\"\n",
    "        Create sinusoidal timestep embeddings.\n",
    "        :param t: a 1-D Tensor of N indices, one per batch element.\n",
    "                          These may be fractional.\n",
    "        :param dim: the dimension of the output.\n",
    "        :param max_period: controls the minimum frequency of the embeddings.\n",
    "        :return: an (N, D) Tensor of positional embeddings.\n",
    "        \"\"\"\n",
    "        # https://github.com/openai/glide-text2im/blob/main/glide_text2im/nn.py\n",
    "        half = dim // 2\n",
    "        freqs = torch.exp(\n",
    "            -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half\n",
    "        ).to(device=t.device)\n",
    "        args = t[:, None].float() * freqs[None]\n",
    "        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
    "        if dim % 2:\n",
    "            embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
    "        return embedding\n",
    "\n",
    "    def forward(self, t):\n",
    "        t_freq = self.timestep_embedding(t, self.frequency_embedding_size)\n",
    "        t_emb = self.mlp(t_freq)\n",
    "        return t_emb\n",
    "\n",
    "class LabelEmbedder(nn.Module):\n",
    "    \"\"\"\n",
    "    Embeds class labels into vector representations. Also handles label dropout for classifier-free guidance.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes, hidden_size, dropout_prob):\n",
    "        super().__init__()\n",
    "        use_cfg_embedding = dropout_prob > 0\n",
    "        self.embedding_table = nn.Embedding(num_classes + use_cfg_embedding, hidden_size)\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "    def token_drop(self, labels, force_drop_ids=None):\n",
    "        \"\"\"\n",
    "        Drops labels to enable classifier-free guidance.\n",
    "        \"\"\"\n",
    "        if force_drop_ids is None:\n",
    "            drop_ids = torch.rand(labels.shape[0], device=labels.device) < self.dropout_prob\n",
    "        else:\n",
    "            drop_ids = force_drop_ids == 1\n",
    "        labels = torch.where(drop_ids, self.num_classes, labels)\n",
    "        return labels\n",
    "\n",
    "    def forward(self, labels, train, force_drop_ids=None):\n",
    "        use_dropout = self.dropout_prob > 0\n",
    "        if (train and use_dropout) or (force_drop_ids is not None):\n",
    "            labels = self.token_drop(labels, force_drop_ids)\n",
    "        embeddings = self.embedding_table(labels)\n",
    "        return embeddings\n",
    "\n",
    "class DiTBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A DiT block with adaptive layer norm zero (adaLN-Zero) conditioning.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size, num_heads, mlp_ratio=4.0, cross_attn=0, **block_kwargs):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
    "        self.attn = Attention(hidden_size, num_heads=num_heads, qkv_bias=True, **block_kwargs)\n",
    "        if cross_attn > 0:\n",
    "            self.norm3 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
    "            self.cross_attn = CrossAttention(hidden_size, cross_attn, num_heads=num_heads, qkv_bias=True, **block_kwargs)\n",
    "        self.norm2 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
    "        mlp_hidden_dim = int(hidden_size * mlp_ratio)\n",
    "        approx_gelu = lambda: nn.GELU(approximate=\"tanh\")\n",
    "        self.mlp = Mlp(in_features=hidden_size, hidden_features=mlp_hidden_dim, act_layer=approx_gelu, drop=0)\n",
    "        self.factor = 9 if cross_attn > 0 else 6\n",
    "        self.adaLN_modulation = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_size, hidden_size * self.factor, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, c, y=None, pad_mask=None):\n",
    "        if self.factor == 9:\n",
    "            shift_msa, scale_msa, gate_msa, shift_mca, scale_mca, gate_mca, shift_mlp, scale_mlp, gate_mlp = self.adaLN_modulation(c).chunk(self.factor, dim=1)\n",
    "        else:\n",
    "            shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.adaLN_modulation(c).chunk(self.factor, dim=1)\n",
    "        x = x + gate_msa.unsqueeze(1) * self.attn(modulate(self.norm1(x), shift_msa, scale_msa))\n",
    "        if self.factor == 9:\n",
    "            x = x + gate_mca.unsqueeze(1) * self.cross_attn(modulate(self.norm3(x), shift_mca, scale_mca), y, pad_mask)\n",
    "        x = x + gate_mlp.unsqueeze(1) * self.mlp(modulate(self.norm2(x), shift_mlp, scale_mlp))\n",
    "        return x\n",
    "\n",
    "class FinalLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    The final layer of DiT.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size, patch_size, out_channels):\n",
    "        super().__init__()\n",
    "        self.norm_final = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
    "        self.linear = nn.Linear(hidden_size, patch_size * patch_size * out_channels, bias=True)\n",
    "        self.adaLN_modulation = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_size, 2 * hidden_size, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        shift, scale = self.adaLN_modulation(c).chunk(2, dim=1)\n",
    "        x = modulate(self.norm_final(x), shift, scale)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
    "    \"\"\"\n",
    "    embed_dim: output dimension for each position\n",
    "    pos: a list of positions to be encoded: size (M,)\n",
    "    out: (M, D)\n",
    "    \"\"\"\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype=np.float64)\n",
    "    omega /= embed_dim / 2.\n",
    "    omega = 1. / 10000 ** omega  # (D/2,)\n",
    "\n",
    "    pos = pos.reshape(-1)  # (M,)\n",
    "    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
    "\n",
    "    emb_sin = np.sin(out)  # (M, D/2)\n",
    "    emb_cos = np.cos(out)  # (M, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "    return emb\n",
    "\n",
    "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False, extra_tokens=0):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid height and width\n",
    "    return:\n",
    "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid_h = np.arange(grid_size, dtype=np.float32)\n",
    "    grid_w = np.arange(grid_size, dtype=np.float32)\n",
    "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
    "    grid = np.stack(grid, axis=0)\n",
    "\n",
    "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
    "    # pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid_h)\n",
    "    if cls_token and extra_tokens > 0:\n",
    "        pos_embed = np.concatenate([np.zeros([extra_tokens, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7530e7e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T06:31:48.815387Z",
     "iopub.status.busy": "2026-02-09T06:31:48.815078Z",
     "iopub.status.idle": "2026-02-09T06:31:48.833798Z",
     "shell.execute_reply": "2026-02-09T06:31:48.832998Z"
    },
    "papermill": {
     "duration": 0.029248,
     "end_time": "2026-02-09T06:31:48.835608",
     "exception": false,
     "start_time": "2026-02-09T06:31:48.806360",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DiT(nn.Module):\n",
    "    \"\"\"\n",
    "    Diffusion model with a Transformer backbone.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size=32,\n",
    "            patch_size=1,\n",
    "            in_channels=64,\n",
    "            hidden_size=768,\n",
    "            depth=12,\n",
    "            num_heads=16,\n",
    "            mlp_ratio=4.0,\n",
    "            class_dropout_prob=0.1,\n",
    "            num_classes=1000,\n",
    "            learn_sigma=True, cross_attn=768, condition_dim=1024\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.learn_sigma = learn_sigma\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = in_channels * 2 if learn_sigma else in_channels\n",
    "        self.patch_size = patch_size\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.input_size = input_size\n",
    "        # self.x_embedder = PatchEmbed(input_size, patch_size, in_channels, hidden_size, bias=True)\n",
    "        self.x_embedder = nn.Linear(in_channels, hidden_size)\n",
    "        self.t_embedder = TimestepEmbedder(hidden_size)\n",
    "        self.y_embedder = LabelEmbedder(num_classes, hidden_size, class_dropout_prob)\n",
    "        self.y_linear = nn.Linear(condition_dim, cross_attn)\n",
    "        num_patches = input_size  # self.x_embedder.num_patches\n",
    "        # Will use fixed sin-cos embedding:\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, hidden_size), requires_grad=False)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            DiTBlock(hidden_size, num_heads, mlp_ratio=mlp_ratio, cross_attn=cross_attn) for _ in range(depth)\n",
    "        ])\n",
    "        self.final_layer = FinalLayer(hidden_size, patch_size, self.out_channels)\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        # Initialize transformer layers:\n",
    "        def _basic_init(module):\n",
    "            if isinstance(module, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "\n",
    "        self.apply(_basic_init)\n",
    "\n",
    "        # Initialize (and freeze) pos_embed by sin-cos embedding:\n",
    "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], self.input_size)  # int(self.x_embedder.num_patches ** 0.5)\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        # Initialize patch_embed like nn.Linear (instead of nn.Conv2d):\n",
    "        # w = self.x_embedder.proj.weight.data\n",
    "        # nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
    "        # nn.init.constant_(self.x_embedder.proj.bias, 0)\n",
    "\n",
    "        # Initialize label embedding table:\n",
    "        nn.init.normal_(self.y_embedder.embedding_table.weight, std=0.02)\n",
    "\n",
    "        # Initialize timestep embedding MLP:\n",
    "        nn.init.normal_(self.t_embedder.mlp[0].weight, std=0.02)\n",
    "        nn.init.normal_(self.t_embedder.mlp[2].weight, std=0.02)\n",
    "\n",
    "        # Zero-out adaLN modulation layers in DiT blocks:\n",
    "        for block in self.blocks:\n",
    "            nn.init.constant_(block.adaLN_modulation[-1].weight, 0)\n",
    "            nn.init.constant_(block.adaLN_modulation[-1].bias, 0)\n",
    "\n",
    "        # Zero-out output layers:\n",
    "        nn.init.constant_(self.final_layer.adaLN_modulation[-1].weight, 0)\n",
    "        nn.init.constant_(self.final_layer.adaLN_modulation[-1].bias, 0)\n",
    "        nn.init.constant_(self.final_layer.linear.weight, 0)\n",
    "        nn.init.constant_(self.final_layer.linear.bias, 0)\n",
    "\n",
    "    def unpatchify(self, x):\n",
    "        \"\"\"\n",
    "        x: (N, T, patch_size**2 * C)\n",
    "        imgs: (N, H, W, C)\n",
    "        \"\"\"\n",
    "        c = self.out_channels\n",
    "        p = self.x_embedder.patch_size[0]\n",
    "        h = w = int(x.shape[1] ** 0.5)\n",
    "        assert h * w == x.shape[1]\n",
    "\n",
    "        x = x.reshape(shape=(x.shape[0], h, w, p, p, c))\n",
    "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
    "        imgs = x.reshape(shape=(x.shape[0], c, h * p, h * p))\n",
    "        return imgs\n",
    "\n",
    "    def forward(self, x, t, y=None, pad_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of DiT.\n",
    "        x: (N, C, H, W) tensor of spatial inputs (images or latent representations of images)\n",
    "        t: (N,) tensor of diffusion timesteps\n",
    "        y: (N,) tensor of class labels\n",
    "        \"\"\"\n",
    "        x = x.squeeze(-1).permute((0, 2, 1))\n",
    "        x = self.x_embedder(x) + self.pos_embed  # (N, T, D), where T = H * W / patch_size ** 2\n",
    "        t = self.t_embedder(t)  # (N, D)\n",
    "        # y = self.y_embedder(y, self.training)    # (N, D)\n",
    "        if y is not None:   y = self.y_linear(y)\n",
    "        # c = t + y                                # (N, D)\n",
    "        c = t  # (N, D)\n",
    "        for block in self.blocks:\n",
    "            x = block(x, c, y, pad_mask)  # (N, T, D)\n",
    "        x = self.final_layer(x, c).permute((0, 2, 1)).unsqueeze(-1)  # (N, T, patch_size ** 2 * out_channels)\n",
    "        # x = self.unpatchify(x)                   # (N, out_channels, H, W)\n",
    "        return x\n",
    "\n",
    "    def forward_with_cfg(self, x, t, y, pad_mask, cfg_scale):\n",
    "        \"\"\"\n",
    "        Forward pass of DiT, but also batches the unconditional forward pass for classifier-free guidance.\n",
    "        \"\"\"\n",
    "        # https://github.com/openai/glide-text2im/blob/main/notebooks/text2im.ipynb\n",
    "        half = x[: len(x) // 2]\n",
    "        combined = torch.cat([half, half], dim=0)\n",
    "        # print('bb', combined.shape, t.shape, y.shape, pad_mask.shape)\n",
    "        model_out = self.forward(combined, t, y, pad_mask)\n",
    "        # For exact reproducibility reasons, we apply classifier-free guidance on only\n",
    "        # three channels by default. The standard approach to cfg applies it to all channels.\n",
    "        # This can be done by uncommenting the following line and commenting-out the line following that.\n",
    "        eps, rest = model_out[:, :self.in_channels], model_out[:, self.in_channels:]\n",
    "        # eps, rest = model_out[:, :3], model_out[:, 3:]\n",
    "        cond_eps, uncond_eps = torch.split(eps, len(eps) // 2, dim=0)\n",
    "        # print(cond_eps[0, 0, :8])\n",
    "        # print(uncond_eps[0, 0, :8])\n",
    "        # raise NotImplementedError\n",
    "        half_eps = uncond_eps + cfg_scale * (cond_eps - uncond_eps)\n",
    "        eps = torch.cat([half_eps, half_eps], dim=0)\n",
    "        return torch.cat([eps, rest], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fd08c1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T06:31:48.851872Z",
     "iopub.status.busy": "2026-02-09T06:31:48.851492Z",
     "iopub.status.idle": "2026-02-09T06:31:48.856533Z",
     "shell.execute_reply": "2026-02-09T06:31:48.855889Z"
    },
    "papermill": {
     "duration": 0.015473,
     "end_time": "2026-02-09T06:31:48.858398",
     "exception": false,
     "start_time": "2026-02-09T06:31:48.842925",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def find_model(model_name):\n",
    "  assert os.path.isfile(model_name), f'Could not find DiT checkpoint at {model_name}'\n",
    "  checkpoint = torch.load(model_name, map_location=lambda storage, loc: storage, weights_only=False)\n",
    "  if \"ema\" in checkpoint:  # supports checkpoints from train.py\n",
    "    checkpoint = checkpoint[\"ema\"]\n",
    "    return checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64069443",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T06:31:48.875191Z",
     "iopub.status.busy": "2026-02-09T06:31:48.874187Z",
     "iopub.status.idle": "2026-02-09T06:32:08.169978Z",
     "shell.execute_reply": "2026-02-09T06:32:08.169071Z"
    },
    "papermill": {
     "duration": 19.306476,
     "end_time": "2026-02-09T06:32:08.172125",
     "exception": false,
     "start_time": "2026-02-09T06:31:48.865649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiT from  /kaggle/input/checkpoint/checkpoint_ldmol.pt <All keys matched successfully>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DiT(\n",
       "  (x_embedder): Linear(in_features=64, out_features=768, bias=True)\n",
       "  (t_embedder): TimestepEmbedder(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=768, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (y_embedder): LabelEmbedder(\n",
       "    (embedding_table): Embedding(1001, 768)\n",
       "  )\n",
       "  (y_linear): Linear(in_features=1024, out_features=768, bias=True)\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x DiTBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=False)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm3): LayerNorm((768,), eps=1e-06, elementwise_affine=False)\n",
       "      (cross_attn): CrossAttention(\n",
       "        (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (kv): Linear(in_features=768, out_features=1536, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=False)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='tanh')\n",
       "        (drop1): Dropout(p=0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (adaLN_modulation): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=768, out_features=6912, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_layer): FinalLayer(\n",
       "    (norm_final): LayerNorm((768,), eps=1e-06, elementwise_affine=False)\n",
       "    (linear): Linear(in_features=768, out_features=128, bias=True)\n",
       "    (adaLN_modulation): Sequential(\n",
       "      (0): SiLU()\n",
       "      (1): Linear(in_features=768, out_features=1536, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_size = 127\n",
    "in_channels = 64  # 64\n",
    "cross_attn = 768\n",
    "condition_dim = 1024\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DiT(input_size=latent_size,\n",
    "        in_channels=in_channels,\n",
    "        cross_attn=cross_attn,\n",
    "        condition_dim=condition_dim,\n",
    "    ).to(device)\n",
    "\n",
    "ckpt_path = '/kaggle/input/checkpoint/checkpoint_ldmol.pt'\n",
    "state_dict = find_model(ckpt_path)\n",
    "msg = model.load_state_dict(state_dict, strict=False)\n",
    "print('DiT from ', ckpt_path, msg)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abac92b",
   "metadata": {
    "papermill": {
     "duration": 0.007674,
     "end_time": "2026-02-09T06:32:08.187313",
     "exception": false,
     "start_time": "2026-02-09T06:32:08.179639",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**SMILES String Autoencoding with Linear Schedule**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41415728",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T06:32:08.205599Z",
     "iopub.status.busy": "2026-02-09T06:32:08.205069Z",
     "iopub.status.idle": "2026-02-09T06:32:08.262191Z",
     "shell.execute_reply": "2026-02-09T06:32:08.261216Z"
    },
    "papermill": {
     "duration": 0.068855,
     "end_time": "2026-02-09T06:32:08.264258",
     "exception": false,
     "start_time": "2026-02-09T06:32:08.195403",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import enum\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.distributed as dist\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "# ==========================================\n",
    "# 1. ENUMS AND UTILITIES (from gaussian_diffusion.py & diffusion_utils.py)\n",
    "# ==========================================\n",
    "\n",
    "class ModelMeanType(enum.Enum):\n",
    "    PREVIOUS_X = enum.auto()\n",
    "    START_X = enum.auto()\n",
    "    EPSILON = enum.auto()\n",
    "\n",
    "class ModelVarType(enum.Enum):\n",
    "    LEARNED = enum.auto()\n",
    "    FIXED_SMALL = enum.auto()\n",
    "    FIXED_LARGE = enum.auto()\n",
    "    LEARNED_RANGE = enum.auto()\n",
    "\n",
    "class LossType(enum.Enum):\n",
    "    MSE = enum.auto()\n",
    "    RESCALED_MSE = enum.auto()\n",
    "    KL = enum.auto()\n",
    "    RESCALED_KL = enum.auto()\n",
    "\n",
    "    def is_vb(self):\n",
    "        return self == LossType.KL or self == LossType.RESCALED_KL\n",
    "\n",
    "def mean_flat(tensor):\n",
    "    return tensor.mean(dim=list(range(1, len(tensor.shape))))\n",
    "\n",
    "def normal_kl(mean1, logvar1, mean2, logvar2):\n",
    "    tensor = None\n",
    "    for obj in (mean1, logvar1, mean2, logvar2):\n",
    "        if isinstance(obj, th.Tensor):\n",
    "            tensor = obj\n",
    "            break\n",
    "    logvar1, logvar2 = [\n",
    "        x if isinstance(x, th.Tensor) else th.tensor(x).to(tensor)\n",
    "        for x in (logvar1, logvar2)\n",
    "    ]\n",
    "    return 0.5 * (-1.0 + logvar2 - logvar1 + th.exp(logvar1 - logvar2) + ((mean1 - mean2) ** 2) * th.exp(-logvar2))\n",
    "\n",
    "def approx_standard_normal_cdf(x):\n",
    "    return 0.5 * (1.0 + th.tanh(np.sqrt(2.0 / np.pi) * (x + 0.044715 * th.pow(x, 3))))\n",
    "\n",
    "def discretized_gaussian_log_likelihood(x, *, means, log_scales):\n",
    "    assert x.shape == means.shape == log_scales.shape\n",
    "    centered_x = x - means\n",
    "    inv_stdv = th.exp(-log_scales)\n",
    "    plus_in = inv_stdv * (centered_x + 1.0 / 255.0)\n",
    "    cdf_plus = approx_standard_normal_cdf(plus_in)\n",
    "    min_in = inv_stdv * (centered_x - 1.0 / 255.0)\n",
    "    cdf_min = approx_standard_normal_cdf(min_in)\n",
    "    log_cdf_plus = th.log(cdf_plus.clamp(min=1e-12))\n",
    "    log_one_minus_cdf_min = th.log((1.0 - cdf_min).clamp(min=1e-12))\n",
    "    cdf_delta = cdf_plus - cdf_min\n",
    "    log_probs = th.where(\n",
    "        x < -0.999,\n",
    "        log_cdf_plus,\n",
    "        th.where(x > 0.999, log_one_minus_cdf_min, th.log(cdf_delta.clamp(min=1e-12))),\n",
    "    )\n",
    "    return log_probs\n",
    "\n",
    "def _extract_into_tensor(arr, timesteps, broadcast_shape):\n",
    "    res = th.from_numpy(arr).to(device=timesteps.device)[timesteps].float()\n",
    "    while len(res.shape) < len(broadcast_shape):\n",
    "        res = res[..., None]\n",
    "    return res + th.zeros(broadcast_shape, device=timesteps.device)\n",
    "\n",
    "# ==========================================\n",
    "# 2. BETA SCHEDULES (from gaussian_diffusion.py)\n",
    "# ==========================================\n",
    "\n",
    "def get_beta_schedule(beta_schedule, *, beta_start, beta_end, num_diffusion_timesteps):\n",
    "    if beta_schedule == \"quad\":\n",
    "        betas = np.linspace(beta_start ** 0.5, beta_end ** 0.5, num_diffusion_timesteps, dtype=np.float64) ** 2\n",
    "    elif beta_schedule == \"linear\":\n",
    "        betas = np.linspace(beta_start, beta_end, num_diffusion_timesteps, dtype=np.float64)\n",
    "    elif beta_schedule == \"const\":\n",
    "        betas = beta_end * np.ones(num_diffusion_timesteps, dtype=np.float64)\n",
    "    elif beta_schedule == \"jsd\":\n",
    "        betas = 1.0 / np.linspace(num_diffusion_timesteps, 1, num_diffusion_timesteps, dtype=np.float64)\n",
    "    else:\n",
    "        raise NotImplementedError(beta_schedule)\n",
    "    return betas\n",
    "\n",
    "def betas_for_alpha_bar(num_diffusion_timesteps, alpha_bar, max_beta=0.999):\n",
    "    betas = []\n",
    "    for i in range(num_diffusion_timesteps):\n",
    "        t1 = i / num_diffusion_timesteps\n",
    "        t2 = (i + 1) / num_diffusion_timesteps\n",
    "        betas.append(min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta))\n",
    "    return np.array(betas)\n",
    "\n",
    "def get_named_beta_schedule(schedule_name, num_diffusion_timesteps):\n",
    "    if schedule_name == \"linear\":\n",
    "        scale = 1000 / num_diffusion_timesteps\n",
    "        return get_beta_schedule(\"linear\", beta_start=scale * 0.0001, beta_end=scale * 0.02, num_diffusion_timesteps=num_diffusion_timesteps)\n",
    "    elif schedule_name == \"squaredcos_cap_v2\":\n",
    "        return betas_for_alpha_bar(num_diffusion_timesteps, lambda t: math.cos((t + 0.008) / 1.008 * math.pi / 2) ** 2)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"unknown beta schedule: {schedule_name}\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. CORE DIFFUSION CLASS (from gaussian_diffusion.py)\n",
    "# ==========================================\n",
    "\n",
    "class GaussianDiffusion:\n",
    "    def __init__(self, *, betas, model_mean_type, model_var_type, loss_type):\n",
    "        self.model_mean_type = model_mean_type\n",
    "        self.model_var_type = model_var_type\n",
    "        self.loss_type = loss_type\n",
    "\n",
    "        betas = np.array(betas, dtype=np.float64)\n",
    "        self.betas = betas\n",
    "        self.num_timesteps = int(betas.shape[0])\n",
    "\n",
    "        alphas = 1.0 - betas\n",
    "        self.alphas_cumprod = np.cumprod(alphas, axis=0)\n",
    "        self.alphas_cumprod_prev = np.append(1.0, self.alphas_cumprod[:-1])\n",
    "        self.alphas_cumprod_next = np.append(self.alphas_cumprod[1:], 0.0)\n",
    "\n",
    "        self.sqrt_alphas_cumprod = np.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = np.sqrt(1.0 - self.alphas_cumprod)\n",
    "        self.log_one_minus_alphas_cumprod = np.log(1.0 - self.alphas_cumprod)\n",
    "        self.sqrt_recip_alphas_cumprod = np.sqrt(1.0 / self.alphas_cumprod)\n",
    "        self.sqrt_recipm1_alphas_cumprod = np.sqrt(1.0 / self.alphas_cumprod - 1)\n",
    "\n",
    "        self.posterior_variance = betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n",
    "        self.posterior_log_variance_clipped = np.log(np.append(self.posterior_variance[1], self.posterior_variance[1:])) if len(self.posterior_variance) > 1 else np.array([])\n",
    "        self.posterior_mean_coef1 = betas * np.sqrt(self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n",
    "        self.posterior_mean_coef2 = (1.0 - self.alphas_cumprod_prev) * np.sqrt(alphas) / (1.0 - self.alphas_cumprod)\n",
    "\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        if noise is None: noise = th.randn_like(x_start)\n",
    "        return (_extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n",
    "                _extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise)\n",
    "\n",
    "    def q_posterior_mean_variance(self, x_start, x_t, t):\n",
    "        posterior_mean = (_extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n",
    "                          _extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t)\n",
    "        posterior_variance = _extract_into_tensor(self.posterior_variance, t, x_t.shape)\n",
    "        posterior_log_variance_clipped = _extract_into_tensor(self.posterior_log_variance_clipped, t, x_t.shape)\n",
    "        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n",
    "\n",
    "    def p_mean_variance(self, model, x, t, clip_denoised=True, denoised_fn=None, model_kwargs=None):\n",
    "        if model_kwargs is None: model_kwargs = {}\n",
    "        B, C = x.shape[:2]\n",
    "        model_output = model(x, t, **model_kwargs)\n",
    "        if isinstance(model_output, tuple): model_output, extra = model_output\n",
    "        else: extra = None\n",
    "\n",
    "        if self.model_var_type in [ModelVarType.LEARNED, ModelVarType.LEARNED_RANGE]:\n",
    "            model_output, model_var_values = th.split(model_output, C, dim=1)\n",
    "            min_log = _extract_into_tensor(self.posterior_log_variance_clipped, t, x.shape)\n",
    "            max_log = _extract_into_tensor(np.log(self.betas), t, x.shape)\n",
    "            frac = (model_var_values + 1) / 2\n",
    "            model_log_variance = frac * max_log + (1 - frac) * min_log\n",
    "            model_variance = th.exp(model_log_variance)\n",
    "        else:\n",
    "            model_variance, model_log_variance = {\n",
    "                ModelVarType.FIXED_LARGE: (np.append(self.posterior_variance[1], self.betas[1:]), np.log(np.append(self.posterior_variance[1], self.betas[1:]))),\n",
    "                ModelVarType.FIXED_SMALL: (self.posterior_variance, self.posterior_log_variance_clipped),\n",
    "            }[self.model_var_type]\n",
    "            model_variance = _extract_into_tensor(model_variance, t, x.shape)\n",
    "            model_log_variance = _extract_into_tensor(model_log_variance, t, x.shape)\n",
    "\n",
    "        def process_xstart(x):\n",
    "            if denoised_fn is not None: x = denoised_fn(x)\n",
    "            return x.clamp(-1, 1) if clip_denoised else x\n",
    "\n",
    "        if self.model_mean_type == ModelMeanType.START_X:\n",
    "            pred_xstart = process_xstart(model_output)\n",
    "        else:\n",
    "            pred_xstart = process_xstart(self._predict_xstart_from_eps(x_t=x, t=t, eps=model_output))\n",
    "\n",
    "        model_mean, _, _ = self.q_posterior_mean_variance(x_start=pred_xstart, x_t=x, t=t)\n",
    "        return {\"mean\": model_mean, \"variance\": model_variance, \"log_variance\": model_log_variance, \"pred_xstart\": pred_xstart, \"extra\": extra}\n",
    "\n",
    "    def _predict_xstart_from_eps(self, x_t, t, eps):\n",
    "        return (_extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n",
    "                _extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * eps)\n",
    "\n",
    "    def _predict_eps_from_xstart(self, x_t, t, pred_xstart):\n",
    "        return (_extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - pred_xstart) / _extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n",
    "\n",
    "    def p_sample(self, model, x, t, clip_denoised=True, denoised_fn=None, cond_fn=None, model_kwargs=None):\n",
    "        out = self.p_mean_variance(model, x, t, clip_denoised=clip_denoised, denoised_fn=denoised_fn, model_kwargs=model_kwargs)\n",
    "        noise = th.randn_like(x)\n",
    "        nonzero_mask = (t != 0).float().view(-1, *([1] * (len(x.shape) - 1)))\n",
    "        sample = out[\"mean\"] + nonzero_mask * th.exp(0.5 * out[\"log_variance\"]) * noise\n",
    "        return {\"sample\": sample, \"pred_xstart\": out[\"pred_xstart\"]}\n",
    "\n",
    "    def p_sample_loop(self, model, shape, **kwargs):\n",
    "        final = None\n",
    "        for sample in self.p_sample_loop_progressive(model, shape, **kwargs):\n",
    "            final = sample\n",
    "        return final[\"sample\"]\n",
    "\n",
    "    def p_sample_loop_progressive(self, model, shape, noise=None, device=None, progress=False, **kwargs):\n",
    "        if device is None: device = next(model.parameters()).device\n",
    "        img = noise if noise is not None else th.randn(*shape, device=device)\n",
    "        indices = list(range(self.num_timesteps))[::-1]\n",
    "        if progress:\n",
    "            from tqdm.auto import tqdm\n",
    "            indices = tqdm(indices)\n",
    "        for i in indices:\n",
    "            t = th.tensor([i] * shape[0], device=device)\n",
    "            with th.no_grad():\n",
    "                out = self.p_sample(model, img, t, **kwargs)\n",
    "                yield out\n",
    "                img = out[\"sample\"]\n",
    "\n",
    "    def training_losses(self, model, x_start, t, model_kwargs=None, noise=None):\n",
    "        if model_kwargs is None: model_kwargs = {}\n",
    "        if noise is None: noise = th.randn_like(x_start)\n",
    "        x_t = self.q_sample(x_start, t, noise=noise)\n",
    "        terms = {}\n",
    "\n",
    "        if self.loss_type.is_vb():\n",
    "            terms[\"loss\"] = self._vb_terms_bpd(model, x_start, x_t, t, model_kwargs=model_kwargs)[\"output\"]\n",
    "            if self.loss_type == LossType.RESCALED_KL: terms[\"loss\"] *= self.num_timesteps\n",
    "        else:\n",
    "            model_output = model(x_t, t, **model_kwargs)\n",
    "            target = {ModelMeanType.PREVIOUS_X: self.q_posterior_mean_variance(x_start, x_t, t)[0],\n",
    "                      ModelMeanType.START_X: x_start, ModelMeanType.EPSILON: noise}[self.model_mean_type]\n",
    "            terms[\"mse\"] = mean_flat((target - model_output) ** 2)\n",
    "            terms[\"loss\"] = terms[\"mse\"]\n",
    "        return terms\n",
    "\n",
    "    def _vb_terms_bpd(self, model, x_start, x_t, t, clip_denoised=True, model_kwargs=None):\n",
    "        true_mean, _, true_log_v = self.q_posterior_mean_variance(x_start, x_t, t)\n",
    "        out = self.p_mean_variance(model, x_t, t, clip_denoised=clip_denoised, model_kwargs=model_kwargs)\n",
    "        kl = mean_flat(normal_kl(true_mean, true_log_v, out[\"mean\"], out[\"log_variance\"])) / np.log(2.0)\n",
    "        decoder_nll = -discretized_gaussian_log_likelihood(x_start, means=out[\"mean\"], log_scales=0.5 * out[\"log_variance\"])\n",
    "        decoder_nll = mean_flat(decoder_nll) / np.log(2.0)\n",
    "        output = th.where((t == 0), decoder_nll, kl)\n",
    "        return {\"output\": output, \"pred_xstart\": out[\"pred_xstart\"]}\n",
    "\n",
    "# ==========================================\n",
    "# 4. RESPACING LOGIC (from respace.py)\n",
    "# ==========================================\n",
    "\n",
    "def space_timesteps(num_timesteps, section_counts):\n",
    "    if isinstance(section_counts, str):\n",
    "        if section_counts.startswith(\"ddim\"):\n",
    "            desired_count = int(section_counts[len(\"ddim\") :])\n",
    "            for i in range(1, num_timesteps):\n",
    "                if len(range(0, num_timesteps, i)) == desired_count:\n",
    "                    return set(range(0, num_timesteps, i))\n",
    "            raise ValueError(f\"cannot create exactly {num_timesteps} steps\")\n",
    "        section_counts = [int(x) for x in section_counts.split(\",\")]\n",
    "\n",
    "    size_per = num_timesteps // len(section_counts)\n",
    "    extra = num_timesteps % len(section_counts)\n",
    "    start_idx, all_steps = 0, []\n",
    "    for i, section_count in enumerate(section_counts):\n",
    "        size = size_per + (1 if i < extra else 0)\n",
    "        frac_stride = 1 if section_count <= 1 else (size - 1) / (section_count - 1)\n",
    "        cur_idx = 0.0\n",
    "        for _ in range(section_count):\n",
    "            all_steps.append(start_idx + round(cur_idx))\n",
    "            cur_idx += frac_stride\n",
    "        start_idx += size\n",
    "    return set(all_steps)\n",
    "\n",
    "class _WrappedModel:\n",
    "    def __init__(self, model, timestep_map, original_num_steps):\n",
    "        self.model = model\n",
    "        self.timestep_map = timestep_map\n",
    "        self.original_num_steps = original_num_steps\n",
    "\n",
    "    def __call__(self, x, ts, **kwargs):\n",
    "        map_tensor = th.tensor(self.timestep_map, device=ts.device, dtype=ts.dtype)\n",
    "        new_ts = map_tensor[ts]\n",
    "        return self.model(x, new_ts, **kwargs)\n",
    "\n",
    "class SpacedDiffusion(GaussianDiffusion):\n",
    "    def __init__(self, use_timesteps, **kwargs):\n",
    "        self.use_timesteps = set(use_timesteps)\n",
    "        self.timestep_map = []\n",
    "        self.original_num_steps = len(kwargs[\"betas\"])\n",
    "        base_diffusion = GaussianDiffusion(**kwargs)\n",
    "        last_alpha_cumprod, new_betas = 1.0, []\n",
    "        for i, alpha_cumprod in enumerate(base_diffusion.alphas_cumprod):\n",
    "            if i in self.use_timesteps:\n",
    "                new_betas.append(1 - alpha_cumprod / last_alpha_cumprod)\n",
    "                last_alpha_cumprod = alpha_cumprod\n",
    "                self.timestep_map.append(i)\n",
    "        kwargs[\"betas\"] = np.array(new_betas)\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def p_mean_variance(self, model, *args, **kwargs):\n",
    "        return super().p_mean_variance(self._wrap_model(model), *args, **kwargs)\n",
    "\n",
    "    def training_losses(self, model, *args, **kwargs):\n",
    "        return super().training_losses(self._wrap_model(model), *args, **kwargs)\n",
    "\n",
    "    def _wrap_model(self, model):\n",
    "        if isinstance(model, _WrappedModel): return model\n",
    "        return _WrappedModel(model, self.timestep_map, self.original_num_steps)\n",
    "\n",
    "# ==========================================\n",
    "# 5. SAMPLERS (from timestep_sampler.py)\n",
    "# ==========================================\n",
    "\n",
    "class ScheduleSampler(ABC):\n",
    "    @abstractmethod\n",
    "    def weights(self): pass\n",
    "\n",
    "    def sample(self, batch_size, device):\n",
    "        w = self.weights()\n",
    "        p = w / np.sum(w)\n",
    "        indices_np = np.random.choice(len(p), size=(batch_size,), p=p)\n",
    "        indices = th.from_numpy(indices_np).long().to(device)\n",
    "        weights = th.from_numpy(1 / (len(p) * p[indices_np])).float().to(device)\n",
    "        return indices, weights\n",
    "\n",
    "class UniformSampler(ScheduleSampler):\n",
    "    def __init__(self, diffusion):\n",
    "        self._weights = np.ones([diffusion.num_timesteps])\n",
    "    def weights(self): return self._weights\n",
    "\n",
    "def create_named_schedule_sampler(name, diffusion):\n",
    "    if name == \"uniform\": return UniformSampler(diffusion)\n",
    "    raise NotImplementedError(f\"unknown sampler: {name}\")\n",
    "\n",
    "# ==========================================\n",
    "# 6. FACTORY FUNCTION (from __init__.py)\n",
    "# ==========================================\n",
    "\n",
    "def create_diffusion(\n",
    "    timestep_respacing,\n",
    "    noise_schedule=\"linear\",\n",
    "    use_kl=False,\n",
    "    sigma_small=False,\n",
    "    predict_xstart=False,\n",
    "    learn_sigma=True,\n",
    "    rescale_learned_sigmas=False,\n",
    "    diffusion_steps=1000\n",
    "):\n",
    "    betas = get_named_beta_schedule(noise_schedule, diffusion_steps)\n",
    "    if use_kl:\n",
    "        loss_type = LossType.RESCALED_KL\n",
    "    elif rescale_learned_sigmas:\n",
    "        loss_type = LossType.RESCALED_MSE\n",
    "    else:\n",
    "        loss_type = LossType.MSE\n",
    "\n",
    "    if timestep_respacing is None or timestep_respacing == \"\":\n",
    "        timestep_respacing = [diffusion_steps]\n",
    "\n",
    "    return SpacedDiffusion(\n",
    "        use_timesteps=space_timesteps(diffusion_steps, timestep_respacing),\n",
    "        betas=betas,\n",
    "        model_mean_type=(\n",
    "            ModelMeanType.EPSILON if not predict_xstart else ModelMeanType.START_X\n",
    "        ),\n",
    "        model_var_type=(\n",
    "            (\n",
    "                ModelVarType.FIXED_LARGE\n",
    "                if not sigma_small\n",
    "                else ModelVarType.FIXED_SMALL\n",
    "            )\n",
    "            if not learn_sigma\n",
    "            else ModelVarType.LEARNED_RANGE\n",
    "        ),\n",
    "        loss_type=loss_type\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b506711b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T06:32:08.281139Z",
     "iopub.status.busy": "2026-02-09T06:32:08.280620Z",
     "iopub.status.idle": "2026-02-09T06:32:08.286874Z",
     "shell.execute_reply": "2026-02-09T06:32:08.285464Z"
    },
    "papermill": {
     "duration": 0.016976,
     "end_time": "2026-02-09T06:32:08.288694",
     "exception": false,
     "start_time": "2026-02-09T06:32:08.271718",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "diffusion = create_diffusion(diffusion_steps=100,timestep_respacing=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffaecfbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T06:32:08.306402Z",
     "iopub.status.busy": "2026-02-09T06:32:08.306078Z",
     "iopub.status.idle": "2026-02-09T06:32:19.040201Z",
     "shell.execute_reply": "2026-02-09T06:32:19.039112Z"
    },
    "papermill": {
     "duration": 10.748029,
     "end_time": "2026-02-09T06:32:19.045087",
     "exception": false,
     "start_time": "2026-02-09T06:32:08.297058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- INSPECTING CHECKPOINT: /kaggle/input/checkpoint/checkpoint_autoencoder.ckpt ---\n",
      "Detected 'state_dict' key in checkpoint.\n",
      "Parameter Name                                                         | Shape\n",
      "------------------------------------------------------------------------------------------\n",
      "text_encoder.bert.embeddings.position_ids                              | [1, 512]\n",
      "text_encoder.bert.embeddings.word_embeddings.weight                    | [300, 768]\n",
      "text_encoder.bert.embeddings.position_embeddings.weight                | [512, 768]\n",
      "text_encoder.bert.embeddings.token_type_embeddings.weight              | [2, 768]\n",
      "text_encoder.bert.embeddings.LayerNorm.weight                          | [768]\n",
      "text_encoder.bert.embeddings.LayerNorm.bias                            | [768]\n",
      "text_encoder.bert.encoder.layer.0.attention.self.query.weight          | [768, 768]\n",
      "text_encoder.bert.encoder.layer.0.attention.self.query.bias            | [768]\n",
      "text_encoder.bert.encoder.layer.0.attention.self.key.weight            | [768, 768]\n",
      "text_encoder.bert.encoder.layer.0.attention.self.key.bias              | [768]\n",
      "text_encoder.bert.encoder.layer.0.attention.self.value.weight          | [768, 768]\n",
      "text_encoder.bert.encoder.layer.0.attention.self.value.bias            | [768]\n",
      "text_encoder.bert.encoder.layer.0.attention.output.dense.weight        | [768, 768]\n",
      "text_encoder.bert.encoder.layer.0.attention.output.dense.bias          | [768]\n",
      "text_encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight    | [768]\n",
      "text_encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias      | [768]\n",
      "text_encoder.bert.encoder.layer.0.crossattention.self.query.weight     | [768, 768]\n",
      "text_encoder.bert.encoder.layer.0.crossattention.self.query.bias       | [768]\n",
      "text_encoder.bert.encoder.layer.0.crossattention.self.key.weight       | [768, 1024]\n",
      "text_encoder.bert.encoder.layer.0.crossattention.self.key.bias         | [768]\n",
      "text_encoder.bert.encoder.layer.0.crossattention.self.value.weight     | [768, 1024]\n",
      "text_encoder.bert.encoder.layer.0.crossattention.self.value.bias       | [768]\n",
      "text_encoder.bert.encoder.layer.0.crossattention.output.dense.weight   | [768, 768]\n",
      "text_encoder.bert.encoder.layer.0.crossattention.output.dense.bias     | [768]\n",
      "text_encoder.bert.encoder.layer.0.crossattention.output.LayerNorm.weight | [768]\n",
      "text_encoder.bert.encoder.layer.0.crossattention.output.LayerNorm.bias | [768]\n",
      "text_encoder.bert.encoder.layer.0.intermediate.dense.weight            | [3072, 768]\n",
      "text_encoder.bert.encoder.layer.0.intermediate.dense.bias              | [3072]\n",
      "text_encoder.bert.encoder.layer.0.output.dense.weight                  | [768, 3072]\n",
      "text_encoder.bert.encoder.layer.0.output.dense.bias                    | [768]\n",
      "text_encoder.bert.encoder.layer.0.output.LayerNorm.weight              | [768]\n",
      "text_encoder.bert.encoder.layer.0.output.LayerNorm.bias                | [768]\n",
      "text_encoder.bert.encoder.layer.1.attention.self.query.weight          | [768, 768]\n",
      "text_encoder.bert.encoder.layer.1.attention.self.query.bias            | [768]\n",
      "text_encoder.bert.encoder.layer.1.attention.self.key.weight            | [768, 768]\n",
      "text_encoder.bert.encoder.layer.1.attention.self.key.bias              | [768]\n",
      "text_encoder.bert.encoder.layer.1.attention.self.value.weight          | [768, 768]\n",
      "text_encoder.bert.encoder.layer.1.attention.self.value.bias            | [768]\n",
      "text_encoder.bert.encoder.layer.1.attention.output.dense.weight        | [768, 768]\n",
      "text_encoder.bert.encoder.layer.1.attention.output.dense.bias          | [768]\n",
      "text_encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight    | [768]\n",
      "text_encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias      | [768]\n",
      "text_encoder.bert.encoder.layer.1.crossattention.self.query.weight     | [768, 768]\n",
      "text_encoder.bert.encoder.layer.1.crossattention.self.query.bias       | [768]\n",
      "text_encoder.bert.encoder.layer.1.crossattention.self.key.weight       | [768, 1024]\n",
      "text_encoder.bert.encoder.layer.1.crossattention.self.key.bias         | [768]\n",
      "text_encoder.bert.encoder.layer.1.crossattention.self.value.weight     | [768, 1024]\n",
      "text_encoder.bert.encoder.layer.1.crossattention.self.value.bias       | [768]\n",
      "text_encoder.bert.encoder.layer.1.crossattention.output.dense.weight   | [768, 768]\n",
      "text_encoder.bert.encoder.layer.1.crossattention.output.dense.bias     | [768]\n",
      "text_encoder.bert.encoder.layer.1.crossattention.output.LayerNorm.weight | [768]\n",
      "text_encoder.bert.encoder.layer.1.crossattention.output.LayerNorm.bias | [768]\n",
      "text_encoder.bert.encoder.layer.1.intermediate.dense.weight            | [3072, 768]\n",
      "text_encoder.bert.encoder.layer.1.intermediate.dense.bias              | [3072]\n",
      "text_encoder.bert.encoder.layer.1.output.dense.weight                  | [768, 3072]\n",
      "text_encoder.bert.encoder.layer.1.output.dense.bias                    | [768]\n",
      "text_encoder.bert.encoder.layer.1.output.LayerNorm.weight              | [768]\n",
      "text_encoder.bert.encoder.layer.1.output.LayerNorm.bias                | [768]\n",
      "text_encoder.bert.encoder.layer.2.attention.self.query.weight          | [768, 768]\n",
      "text_encoder.bert.encoder.layer.2.attention.self.query.bias            | [768]\n",
      "text_encoder.bert.encoder.layer.2.attention.self.key.weight            | [768, 768]\n",
      "text_encoder.bert.encoder.layer.2.attention.self.key.bias              | [768]\n",
      "text_encoder.bert.encoder.layer.2.attention.self.value.weight          | [768, 768]\n",
      "text_encoder.bert.encoder.layer.2.attention.self.value.bias            | [768]\n",
      "text_encoder.bert.encoder.layer.2.attention.output.dense.weight        | [768, 768]\n",
      "text_encoder.bert.encoder.layer.2.attention.output.dense.bias          | [768]\n",
      "text_encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight    | [768]\n",
      "text_encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias      | [768]\n",
      "text_encoder.bert.encoder.layer.2.crossattention.self.query.weight     | [768, 768]\n",
      "text_encoder.bert.encoder.layer.2.crossattention.self.query.bias       | [768]\n",
      "text_encoder.bert.encoder.layer.2.crossattention.self.key.weight       | [768, 1024]\n",
      "text_encoder.bert.encoder.layer.2.crossattention.self.key.bias         | [768]\n",
      "text_encoder.bert.encoder.layer.2.crossattention.self.value.weight     | [768, 1024]\n",
      "text_encoder.bert.encoder.layer.2.crossattention.self.value.bias       | [768]\n",
      "text_encoder.bert.encoder.layer.2.crossattention.output.dense.weight   | [768, 768]\n",
      "text_encoder.bert.encoder.layer.2.crossattention.output.dense.bias     | [768]\n",
      "text_encoder.bert.encoder.layer.2.crossattention.output.LayerNorm.weight | [768]\n",
      "text_encoder.bert.encoder.layer.2.crossattention.output.LayerNorm.bias | [768]\n",
      "text_encoder.bert.encoder.layer.2.intermediate.dense.weight            | [3072, 768]\n",
      "text_encoder.bert.encoder.layer.2.intermediate.dense.bias              | [3072]\n",
      "text_encoder.bert.encoder.layer.2.output.dense.weight                  | [768, 3072]\n",
      "text_encoder.bert.encoder.layer.2.output.dense.bias                    | [768]\n",
      "text_encoder.bert.encoder.layer.2.output.LayerNorm.weight              | [768]\n",
      "text_encoder.bert.encoder.layer.2.output.LayerNorm.bias                | [768]\n",
      "text_encoder.bert.encoder.layer.3.attention.self.query.weight          | [768, 768]\n",
      "text_encoder.bert.encoder.layer.3.attention.self.query.bias            | [768]\n",
      "text_encoder.bert.encoder.layer.3.attention.self.key.weight            | [768, 768]\n",
      "text_encoder.bert.encoder.layer.3.attention.self.key.bias              | [768]\n",
      "text_encoder.bert.encoder.layer.3.attention.self.value.weight          | [768, 768]\n",
      "text_encoder.bert.encoder.layer.3.attention.self.value.bias            | [768]\n",
      "text_encoder.bert.encoder.layer.3.attention.output.dense.weight        | [768, 768]\n",
      "text_encoder.bert.encoder.layer.3.attention.output.dense.bias          | [768]\n",
      "text_encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight    | [768]\n",
      "text_encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias      | [768]\n",
      "text_encoder.bert.encoder.layer.3.crossattention.self.query.weight     | [768, 768]\n",
      "text_encoder.bert.encoder.layer.3.crossattention.self.query.bias       | [768]\n",
      "text_encoder.bert.encoder.layer.3.crossattention.self.key.weight       | [768, 1024]\n",
      "text_encoder.bert.encoder.layer.3.crossattention.self.key.bias         | [768]\n",
      "text_encoder.bert.encoder.layer.3.crossattention.self.value.weight     | [768, 1024]\n",
      "text_encoder.bert.encoder.layer.3.crossattention.self.value.bias       | [768]\n",
      "text_encoder.bert.encoder.layer.3.crossattention.output.dense.weight   | [768, 768]\n",
      "text_encoder.bert.encoder.layer.3.crossattention.output.dense.bias     | [768]\n",
      "text_encoder.bert.encoder.layer.3.crossattention.output.LayerNorm.weight | [768]\n",
      "text_encoder.bert.encoder.layer.3.crossattention.output.LayerNorm.bias | [768]\n",
      "text_encoder.bert.encoder.layer.3.intermediate.dense.weight            | [3072, 768]\n",
      "text_encoder.bert.encoder.layer.3.intermediate.dense.bias              | [3072]\n",
      "text_encoder.bert.encoder.layer.3.output.dense.weight                  | [768, 3072]\n",
      "text_encoder.bert.encoder.layer.3.output.dense.bias                    | [768]\n",
      "text_encoder.bert.encoder.layer.3.output.LayerNorm.weight              | [768]\n",
      "text_encoder.bert.encoder.layer.3.output.LayerNorm.bias                | [768]\n",
      "text_encoder.bert.encoder.layer.4.attention.self.query.weight          | [768, 768]\n",
      "text_encoder.bert.encoder.layer.4.attention.self.query.bias            | [768]\n",
      "text_encoder.bert.encoder.layer.4.attention.self.key.weight            | [768, 768]\n",
      "text_encoder.bert.encoder.layer.4.attention.self.key.bias              | [768]\n",
      "text_encoder.bert.encoder.layer.4.attention.self.value.weight          | [768, 768]\n",
      "text_encoder.bert.encoder.layer.4.attention.self.value.bias            | [768]\n",
      "text_encoder.bert.encoder.layer.4.attention.output.dense.weight        | [768, 768]\n",
      "text_encoder.bert.encoder.layer.4.attention.output.dense.bias          | [768]\n",
      "text_encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight    | [768]\n",
      "text_encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias      | [768]\n",
      "text_encoder.bert.encoder.layer.4.crossattention.self.query.weight     | [768, 768]\n",
      "text_encoder.bert.encoder.layer.4.crossattention.self.query.bias       | [768]\n",
      "text_encoder.bert.encoder.layer.4.crossattention.self.key.weight       | [768, 1024]\n",
      "text_encoder.bert.encoder.layer.4.crossattention.self.key.bias         | [768]\n",
      "text_encoder.bert.encoder.layer.4.crossattention.self.value.weight     | [768, 1024]\n",
      "text_encoder.bert.encoder.layer.4.crossattention.self.value.bias       | [768]\n",
      "text_encoder.bert.encoder.layer.4.crossattention.output.dense.weight   | [768, 768]\n",
      "text_encoder.bert.encoder.layer.4.crossattention.output.dense.bias     | [768]\n",
      "text_encoder.bert.encoder.layer.4.crossattention.output.LayerNorm.weight | [768]\n",
      "text_encoder.bert.encoder.layer.4.crossattention.output.LayerNorm.bias | [768]\n",
      "text_encoder.bert.encoder.layer.4.intermediate.dense.weight            | [3072, 768]\n",
      "text_encoder.bert.encoder.layer.4.intermediate.dense.bias              | [3072]\n",
      "text_encoder.bert.encoder.layer.4.output.dense.weight                  | [768, 3072]\n",
      "text_encoder.bert.encoder.layer.4.output.dense.bias                    | [768]\n",
      "text_encoder.bert.encoder.layer.4.output.LayerNorm.weight              | [768]\n",
      "text_encoder.bert.encoder.layer.4.output.LayerNorm.bias                | [768]\n",
      "text_encoder.bert.encoder.layer.5.attention.self.query.weight          | [768, 768]\n",
      "text_encoder.bert.encoder.layer.5.attention.self.query.bias            | [768]\n",
      "text_encoder.bert.encoder.layer.5.attention.self.key.weight            | [768, 768]\n",
      "text_encoder.bert.encoder.layer.5.attention.self.key.bias              | [768]\n",
      "text_encoder.bert.encoder.layer.5.attention.self.value.weight          | [768, 768]\n",
      "text_encoder.bert.encoder.layer.5.attention.self.value.bias            | [768]\n",
      "text_encoder.bert.encoder.layer.5.attention.output.dense.weight        | [768, 768]\n",
      "text_encoder.bert.encoder.layer.5.attention.output.dense.bias          | [768]\n",
      "text_encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight    | [768]\n",
      "text_encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias      | [768]\n",
      "text_encoder.bert.encoder.layer.5.crossattention.self.query.weight     | [768, 768]\n",
      "text_encoder.bert.encoder.layer.5.crossattention.self.query.bias       | [768]\n",
      "text_encoder.bert.encoder.layer.5.crossattention.self.key.weight       | [768, 1024]\n",
      "text_encoder.bert.encoder.layer.5.crossattention.self.key.bias         | [768]\n",
      "text_encoder.bert.encoder.layer.5.crossattention.self.value.weight     | [768, 1024]\n",
      "text_encoder.bert.encoder.layer.5.crossattention.self.value.bias       | [768]\n",
      "text_encoder.bert.encoder.layer.5.crossattention.output.dense.weight   | [768, 768]\n",
      "text_encoder.bert.encoder.layer.5.crossattention.output.dense.bias     | [768]\n",
      "text_encoder.bert.encoder.layer.5.crossattention.output.LayerNorm.weight | [768]\n",
      "text_encoder.bert.encoder.layer.5.crossattention.output.LayerNorm.bias | [768]\n",
      "text_encoder.bert.encoder.layer.5.intermediate.dense.weight            | [3072, 768]\n",
      "text_encoder.bert.encoder.layer.5.intermediate.dense.bias              | [3072]\n",
      "text_encoder.bert.encoder.layer.5.output.dense.weight                  | [768, 3072]\n",
      "text_encoder.bert.encoder.layer.5.output.dense.bias                    | [768]\n",
      "text_encoder.bert.encoder.layer.5.output.LayerNorm.weight              | [768]\n",
      "text_encoder.bert.encoder.layer.5.output.LayerNorm.bias                | [768]\n",
      "text_encoder.bert.encoder.layer.6.attention.self.query.weight          | [768, 768]\n",
      "text_encoder.bert.encoder.layer.6.attention.self.query.bias            | [768]\n",
      "text_encoder.bert.encoder.layer.6.attention.self.key.weight            | [768, 768]\n",
      "text_encoder.bert.encoder.layer.6.attention.self.key.bias              | [768]\n",
      "text_encoder.bert.encoder.layer.6.attention.self.value.weight          | [768, 768]\n",
      "text_encoder.bert.encoder.layer.6.attention.self.value.bias            | [768]\n",
      "text_encoder.bert.encoder.layer.6.attention.output.dense.weight        | [768, 768]\n",
      "text_encoder.bert.encoder.layer.6.attention.output.dense.bias          | [768]\n",
      "text_encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight    | [768]\n",
      "text_encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias      | [768]\n",
      "text_encoder.bert.encoder.layer.6.crossattention.self.query.weight     | [768, 768]\n",
      "text_encoder.bert.encoder.layer.6.crossattention.self.query.bias       | [768]\n",
      "text_encoder.bert.encoder.layer.6.crossattention.self.key.weight       | [768, 1024]\n",
      "text_encoder.bert.encoder.layer.6.crossattention.self.key.bias         | [768]\n",
      "text_encoder.bert.encoder.layer.6.crossattention.self.value.weight     | [768, 1024]\n",
      "text_encoder.bert.encoder.layer.6.crossattention.self.value.bias       | [768]\n",
      "text_encoder.bert.encoder.layer.6.crossattention.output.dense.weight   | [768, 768]\n",
      "text_encoder.bert.encoder.layer.6.crossattention.output.dense.bias     | [768]\n",
      "text_encoder.bert.encoder.layer.6.crossattention.output.LayerNorm.weight | [768]\n",
      "text_encoder.bert.encoder.layer.6.crossattention.output.LayerNorm.bias | [768]\n",
      "text_encoder.bert.encoder.layer.6.intermediate.dense.weight            | [3072, 768]\n",
      "text_encoder.bert.encoder.layer.6.intermediate.dense.bias              | [3072]\n",
      "text_encoder.bert.encoder.layer.6.output.dense.weight                  | [768, 3072]\n",
      "text_encoder.bert.encoder.layer.6.output.dense.bias                    | [768]\n",
      "text_encoder.bert.encoder.layer.6.output.LayerNorm.weight              | [768]\n",
      "text_encoder.bert.encoder.layer.6.output.LayerNorm.bias                | [768]\n",
      "text_encoder.bert.encoder.layer.7.attention.self.query.weight          | [768, 768]\n",
      "text_encoder.bert.encoder.layer.7.attention.self.query.bias            | [768]\n",
      "text_encoder.bert.encoder.layer.7.attention.self.key.weight            | [768, 768]\n",
      "text_encoder.bert.encoder.layer.7.attention.self.key.bias              | [768]\n",
      "text_encoder.bert.encoder.layer.7.attention.self.value.weight          | [768, 768]\n",
      "text_encoder.bert.encoder.layer.7.attention.self.value.bias            | [768]\n",
      "text_encoder.bert.encoder.layer.7.attention.output.dense.weight        | [768, 768]\n",
      "text_encoder.bert.encoder.layer.7.attention.output.dense.bias          | [768]\n",
      "text_encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight    | [768]\n",
      "text_encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias      | [768]\n",
      "text_encoder.bert.encoder.layer.7.crossattention.self.query.weight     | [768, 768]\n",
      "text_encoder.bert.encoder.layer.7.crossattention.self.query.bias       | [768]\n",
      "text_encoder.bert.encoder.layer.7.crossattention.self.key.weight       | [768, 1024]\n",
      "text_encoder.bert.encoder.layer.7.crossattention.self.key.bias         | [768]\n",
      "text_encoder.bert.encoder.layer.7.crossattention.self.value.weight     | [768, 1024]\n",
      "text_encoder.bert.encoder.layer.7.crossattention.self.value.bias       | [768]\n",
      "text_encoder.bert.encoder.layer.7.crossattention.output.dense.weight   | [768, 768]\n",
      "text_encoder.bert.encoder.layer.7.crossattention.output.dense.bias     | [768]\n",
      "text_encoder.bert.encoder.layer.7.crossattention.output.LayerNorm.weight | [768]\n",
      "text_encoder.bert.encoder.layer.7.crossattention.output.LayerNorm.bias | [768]\n",
      "text_encoder.bert.encoder.layer.7.intermediate.dense.weight            | [3072, 768]\n",
      "text_encoder.bert.encoder.layer.7.intermediate.dense.bias              | [3072]\n",
      "text_encoder.bert.encoder.layer.7.output.dense.weight                  | [768, 3072]\n",
      "text_encoder.bert.encoder.layer.7.output.dense.bias                    | [768]\n",
      "text_encoder.bert.encoder.layer.7.output.LayerNorm.weight              | [768]\n",
      "text_encoder.bert.encoder.layer.7.output.LayerNorm.bias                | [768]\n",
      "text_encoder.bert.encoder.layer.8.attention.self.query.weight          | [768, 768]\n",
      "text_encoder.bert.encoder.layer.8.attention.self.query.bias            | [768]\n",
      "text_encoder.bert.encoder.layer.8.attention.self.key.weight            | [768, 768]\n",
      "text_encoder.bert.encoder.layer.8.attention.self.key.bias              | [768]\n",
      "text_encoder.bert.encoder.layer.8.attention.self.value.weight          | [768, 768]\n",
      "text_encoder.bert.encoder.layer.8.attention.self.value.bias            | [768]\n",
      "text_encoder.bert.encoder.layer.8.attention.output.dense.weight        | [768, 768]\n",
      "text_encoder.bert.encoder.layer.8.attention.output.dense.bias          | [768]\n",
      "text_encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight    | [768]\n",
      "text_encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias      | [768]\n",
      "text_encoder.bert.encoder.layer.8.crossattention.self.query.weight     | [768, 768]\n",
      "text_encoder.bert.encoder.layer.8.crossattention.self.query.bias       | [768]\n",
      "text_encoder.bert.encoder.layer.8.crossattention.self.key.weight       | [768, 1024]\n",
      "text_encoder.bert.encoder.layer.8.crossattention.self.key.bias         | [768]\n",
      "text_encoder.bert.encoder.layer.8.crossattention.self.value.weight     | [768, 1024]\n",
      "text_encoder.bert.encoder.layer.8.crossattention.self.value.bias       | [768]\n",
      "text_encoder.bert.encoder.layer.8.crossattention.output.dense.weight   | [768, 768]\n",
      "text_encoder.bert.encoder.layer.8.crossattention.output.dense.bias     | [768]\n",
      "text_encoder.bert.encoder.layer.8.crossattention.output.LayerNorm.weight | [768]\n",
      "text_encoder.bert.encoder.layer.8.crossattention.output.LayerNorm.bias | [768]\n",
      "text_encoder.bert.encoder.layer.8.intermediate.dense.weight            | [3072, 768]\n",
      "text_encoder.bert.encoder.layer.8.intermediate.dense.bias              | [3072]\n",
      "text_encoder.bert.encoder.layer.8.output.dense.weight                  | [768, 3072]\n",
      "text_encoder.bert.encoder.layer.8.output.dense.bias                    | [768]\n",
      "text_encoder.bert.encoder.layer.8.output.LayerNorm.weight              | [768]\n",
      "text_encoder.bert.encoder.layer.8.output.LayerNorm.bias                | [768]\n",
      "text_encoder.bert.encoder.layer.9.attention.self.query.weight          | [768, 768]\n",
      "text_encoder.bert.encoder.layer.9.attention.self.query.bias            | [768]\n",
      "text_encoder.bert.encoder.layer.9.attention.self.key.weight            | [768, 768]\n",
      "text_encoder.bert.encoder.layer.9.attention.self.key.bias              | [768]\n",
      "text_encoder.bert.encoder.layer.9.attention.self.value.weight          | [768, 768]\n",
      "text_encoder.bert.encoder.layer.9.attention.self.value.bias            | [768]\n",
      "text_encoder.bert.encoder.layer.9.attention.output.dense.weight        | [768, 768]\n",
      "text_encoder.bert.encoder.layer.9.attention.output.dense.bias          | [768]\n",
      "text_encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight    | [768]\n",
      "text_encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias      | [768]\n",
      "text_encoder.bert.encoder.layer.9.crossattention.self.query.weight     | [768, 768]\n",
      "text_encoder.bert.encoder.layer.9.crossattention.self.query.bias       | [768]\n",
      "text_encoder.bert.encoder.layer.9.crossattention.self.key.weight       | [768, 1024]\n",
      "text_encoder.bert.encoder.layer.9.crossattention.self.key.bias         | [768]\n",
      "text_encoder.bert.encoder.layer.9.crossattention.self.value.weight     | [768, 1024]\n",
      "text_encoder.bert.encoder.layer.9.crossattention.self.value.bias       | [768]\n",
      "text_encoder.bert.encoder.layer.9.crossattention.output.dense.weight   | [768, 768]\n",
      "text_encoder.bert.encoder.layer.9.crossattention.output.dense.bias     | [768]\n",
      "text_encoder.bert.encoder.layer.9.crossattention.output.LayerNorm.weight | [768]\n",
      "text_encoder.bert.encoder.layer.9.crossattention.output.LayerNorm.bias | [768]\n",
      "text_encoder.bert.encoder.layer.9.intermediate.dense.weight            | [3072, 768]\n",
      "text_encoder.bert.encoder.layer.9.intermediate.dense.bias              | [3072]\n",
      "text_encoder.bert.encoder.layer.9.output.dense.weight                  | [768, 3072]\n",
      "text_encoder.bert.encoder.layer.9.output.dense.bias                    | [768]\n",
      "text_encoder.bert.encoder.layer.9.output.LayerNorm.weight              | [768]\n",
      "text_encoder.bert.encoder.layer.9.output.LayerNorm.bias                | [768]\n",
      "text_encoder.bert.encoder.layer.10.attention.self.query.weight         | [768, 768]\n",
      "text_encoder.bert.encoder.layer.10.attention.self.query.bias           | [768]\n",
      "text_encoder.bert.encoder.layer.10.attention.self.key.weight           | [768, 768]\n",
      "text_encoder.bert.encoder.layer.10.attention.self.key.bias             | [768]\n",
      "text_encoder.bert.encoder.layer.10.attention.self.value.weight         | [768, 768]\n",
      "text_encoder.bert.encoder.layer.10.attention.self.value.bias           | [768]\n",
      "text_encoder.bert.encoder.layer.10.attention.output.dense.weight       | [768, 768]\n",
      "text_encoder.bert.encoder.layer.10.attention.output.dense.bias         | [768]\n",
      "text_encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight   | [768]\n",
      "text_encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias     | [768]\n",
      "text_encoder.bert.encoder.layer.10.crossattention.self.query.weight    | [768, 768]\n",
      "text_encoder.bert.encoder.layer.10.crossattention.self.query.bias      | [768]\n",
      "text_encoder.bert.encoder.layer.10.crossattention.self.key.weight      | [768, 1024]\n",
      "text_encoder.bert.encoder.layer.10.crossattention.self.key.bias        | [768]\n",
      "text_encoder.bert.encoder.layer.10.crossattention.self.value.weight    | [768, 1024]\n",
      "text_encoder.bert.encoder.layer.10.crossattention.self.value.bias      | [768]\n",
      "text_encoder.bert.encoder.layer.10.crossattention.output.dense.weight  | [768, 768]\n",
      "text_encoder.bert.encoder.layer.10.crossattention.output.dense.bias    | [768]\n",
      "text_encoder.bert.encoder.layer.10.crossattention.output.LayerNorm.weight | [768]\n",
      "text_encoder.bert.encoder.layer.10.crossattention.output.LayerNorm.bias | [768]\n",
      "text_encoder.bert.encoder.layer.10.intermediate.dense.weight           | [3072, 768]\n",
      "text_encoder.bert.encoder.layer.10.intermediate.dense.bias             | [3072]\n",
      "text_encoder.bert.encoder.layer.10.output.dense.weight                 | [768, 3072]\n",
      "text_encoder.bert.encoder.layer.10.output.dense.bias                   | [768]\n",
      "text_encoder.bert.encoder.layer.10.output.LayerNorm.weight             | [768]\n",
      "text_encoder.bert.encoder.layer.10.output.LayerNorm.bias               | [768]\n",
      "text_encoder.bert.encoder.layer.11.attention.self.query.weight         | [768, 768]\n",
      "text_encoder.bert.encoder.layer.11.attention.self.query.bias           | [768]\n",
      "text_encoder.bert.encoder.layer.11.attention.self.key.weight           | [768, 768]\n",
      "text_encoder.bert.encoder.layer.11.attention.self.key.bias             | [768]\n",
      "text_encoder.bert.encoder.layer.11.attention.self.value.weight         | [768, 768]\n",
      "text_encoder.bert.encoder.layer.11.attention.self.value.bias           | [768]\n",
      "text_encoder.bert.encoder.layer.11.attention.output.dense.weight       | [768, 768]\n",
      "text_encoder.bert.encoder.layer.11.attention.output.dense.bias         | [768]\n",
      "text_encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight   | [768]\n",
      "text_encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias     | [768]\n",
      "text_encoder.bert.encoder.layer.11.crossattention.self.query.weight    | [768, 768]\n",
      "text_encoder.bert.encoder.layer.11.crossattention.self.query.bias      | [768]\n",
      "text_encoder.bert.encoder.layer.11.crossattention.self.key.weight      | [768, 1024]\n",
      "text_encoder.bert.encoder.layer.11.crossattention.self.key.bias        | [768]\n",
      "text_encoder.bert.encoder.layer.11.crossattention.self.value.weight    | [768, 1024]\n",
      "text_encoder.bert.encoder.layer.11.crossattention.self.value.bias      | [768]\n",
      "text_encoder.bert.encoder.layer.11.crossattention.output.dense.weight  | [768, 768]\n",
      "text_encoder.bert.encoder.layer.11.crossattention.output.dense.bias    | [768]\n",
      "text_encoder.bert.encoder.layer.11.crossattention.output.LayerNorm.weight | [768]\n",
      "text_encoder.bert.encoder.layer.11.crossattention.output.LayerNorm.bias | [768]\n",
      "text_encoder.bert.encoder.layer.11.intermediate.dense.weight           | [3072, 768]\n",
      "text_encoder.bert.encoder.layer.11.intermediate.dense.bias             | [3072]\n",
      "text_encoder.bert.encoder.layer.11.output.dense.weight                 | [768, 3072]\n",
      "text_encoder.bert.encoder.layer.11.output.dense.bias                   | [768]\n",
      "text_encoder.bert.encoder.layer.11.output.LayerNorm.weight             | [768]\n",
      "text_encoder.bert.encoder.layer.11.output.LayerNorm.bias               | [768]\n",
      "text_encoder.cls.predictions.bias                                      | [300]\n",
      "text_encoder.cls.predictions.transform.dense.weight                    | [768, 768]\n",
      "text_encoder.cls.predictions.transform.dense.bias                      | [768]\n",
      "text_encoder.cls.predictions.transform.LayerNorm.weight                | [768]\n",
      "text_encoder.cls.predictions.transform.LayerNorm.bias                  | [768]\n",
      "text_encoder.cls.predictions.decoder.weight                            | [300, 768]\n",
      "text_encoder.cls.predictions.decoder.bias                              | [300]\n",
      "text_encoder2.embeddings.position_ids                                  | [1, 512]\n",
      "text_encoder2.embeddings.word_embeddings.weight                        | [300, 1024]\n",
      "text_encoder2.embeddings.position_embeddings.weight                    | [512, 1024]\n",
      "text_encoder2.embeddings.token_type_embeddings.weight                  | [2, 1024]\n",
      "text_encoder2.embeddings.LayerNorm.weight                              | [1024]\n",
      "text_encoder2.embeddings.LayerNorm.bias                                | [1024]\n",
      "text_encoder2.encoder.layer.0.attention.self.query.weight              | [1024, 1024]\n",
      "text_encoder2.encoder.layer.0.attention.self.query.bias                | [1024]\n",
      "text_encoder2.encoder.layer.0.attention.self.key.weight                | [1024, 1024]\n",
      "text_encoder2.encoder.layer.0.attention.self.key.bias                  | [1024]\n",
      "text_encoder2.encoder.layer.0.attention.self.value.weight              | [1024, 1024]\n",
      "text_encoder2.encoder.layer.0.attention.self.value.bias                | [1024]\n",
      "text_encoder2.encoder.layer.0.attention.output.dense.weight            | [1024, 1024]\n",
      "text_encoder2.encoder.layer.0.attention.output.dense.bias              | [1024]\n",
      "text_encoder2.encoder.layer.0.attention.output.LayerNorm.weight        | [1024]\n",
      "text_encoder2.encoder.layer.0.attention.output.LayerNorm.bias          | [1024]\n",
      "text_encoder2.encoder.layer.0.intermediate.dense.weight                | [3072, 1024]\n",
      "text_encoder2.encoder.layer.0.intermediate.dense.bias                  | [3072]\n",
      "text_encoder2.encoder.layer.0.output.dense.weight                      | [1024, 3072]\n",
      "text_encoder2.encoder.layer.0.output.dense.bias                        | [1024]\n",
      "text_encoder2.encoder.layer.0.output.LayerNorm.weight                  | [1024]\n",
      "text_encoder2.encoder.layer.0.output.LayerNorm.bias                    | [1024]\n",
      "text_encoder2.encoder.layer.1.attention.self.query.weight              | [1024, 1024]\n",
      "text_encoder2.encoder.layer.1.attention.self.query.bias                | [1024]\n",
      "text_encoder2.encoder.layer.1.attention.self.key.weight                | [1024, 1024]\n",
      "text_encoder2.encoder.layer.1.attention.self.key.bias                  | [1024]\n",
      "text_encoder2.encoder.layer.1.attention.self.value.weight              | [1024, 1024]\n",
      "text_encoder2.encoder.layer.1.attention.self.value.bias                | [1024]\n",
      "text_encoder2.encoder.layer.1.attention.output.dense.weight            | [1024, 1024]\n",
      "text_encoder2.encoder.layer.1.attention.output.dense.bias              | [1024]\n",
      "text_encoder2.encoder.layer.1.attention.output.LayerNorm.weight        | [1024]\n",
      "text_encoder2.encoder.layer.1.attention.output.LayerNorm.bias          | [1024]\n",
      "text_encoder2.encoder.layer.1.intermediate.dense.weight                | [3072, 1024]\n",
      "text_encoder2.encoder.layer.1.intermediate.dense.bias                  | [3072]\n",
      "text_encoder2.encoder.layer.1.output.dense.weight                      | [1024, 3072]\n",
      "text_encoder2.encoder.layer.1.output.dense.bias                        | [1024]\n",
      "text_encoder2.encoder.layer.1.output.LayerNorm.weight                  | [1024]\n",
      "text_encoder2.encoder.layer.1.output.LayerNorm.bias                    | [1024]\n",
      "text_encoder2.encoder.layer.2.attention.self.query.weight              | [1024, 1024]\n",
      "text_encoder2.encoder.layer.2.attention.self.query.bias                | [1024]\n",
      "text_encoder2.encoder.layer.2.attention.self.key.weight                | [1024, 1024]\n",
      "text_encoder2.encoder.layer.2.attention.self.key.bias                  | [1024]\n",
      "text_encoder2.encoder.layer.2.attention.self.value.weight              | [1024, 1024]\n",
      "text_encoder2.encoder.layer.2.attention.self.value.bias                | [1024]\n",
      "text_encoder2.encoder.layer.2.attention.output.dense.weight            | [1024, 1024]\n",
      "text_encoder2.encoder.layer.2.attention.output.dense.bias              | [1024]\n",
      "text_encoder2.encoder.layer.2.attention.output.LayerNorm.weight        | [1024]\n",
      "text_encoder2.encoder.layer.2.attention.output.LayerNorm.bias          | [1024]\n",
      "text_encoder2.encoder.layer.2.intermediate.dense.weight                | [3072, 1024]\n",
      "text_encoder2.encoder.layer.2.intermediate.dense.bias                  | [3072]\n",
      "text_encoder2.encoder.layer.2.output.dense.weight                      | [1024, 3072]\n",
      "text_encoder2.encoder.layer.2.output.dense.bias                        | [1024]\n",
      "text_encoder2.encoder.layer.2.output.LayerNorm.weight                  | [1024]\n",
      "text_encoder2.encoder.layer.2.output.LayerNorm.bias                    | [1024]\n",
      "text_encoder2.encoder.layer.3.attention.self.query.weight              | [1024, 1024]\n",
      "text_encoder2.encoder.layer.3.attention.self.query.bias                | [1024]\n",
      "text_encoder2.encoder.layer.3.attention.self.key.weight                | [1024, 1024]\n",
      "text_encoder2.encoder.layer.3.attention.self.key.bias                  | [1024]\n",
      "text_encoder2.encoder.layer.3.attention.self.value.weight              | [1024, 1024]\n",
      "text_encoder2.encoder.layer.3.attention.self.value.bias                | [1024]\n",
      "text_encoder2.encoder.layer.3.attention.output.dense.weight            | [1024, 1024]\n",
      "text_encoder2.encoder.layer.3.attention.output.dense.bias              | [1024]\n",
      "text_encoder2.encoder.layer.3.attention.output.LayerNorm.weight        | [1024]\n",
      "text_encoder2.encoder.layer.3.attention.output.LayerNorm.bias          | [1024]\n",
      "text_encoder2.encoder.layer.3.intermediate.dense.weight                | [3072, 1024]\n",
      "text_encoder2.encoder.layer.3.intermediate.dense.bias                  | [3072]\n",
      "text_encoder2.encoder.layer.3.output.dense.weight                      | [1024, 3072]\n",
      "text_encoder2.encoder.layer.3.output.dense.bias                        | [1024]\n",
      "text_encoder2.encoder.layer.3.output.LayerNorm.weight                  | [1024]\n",
      "text_encoder2.encoder.layer.3.output.LayerNorm.bias                    | [1024]\n",
      "text_encoder2.encoder.layer.4.attention.self.query.weight              | [1024, 1024]\n",
      "text_encoder2.encoder.layer.4.attention.self.query.bias                | [1024]\n",
      "text_encoder2.encoder.layer.4.attention.self.key.weight                | [1024, 1024]\n",
      "text_encoder2.encoder.layer.4.attention.self.key.bias                  | [1024]\n",
      "text_encoder2.encoder.layer.4.attention.self.value.weight              | [1024, 1024]\n",
      "text_encoder2.encoder.layer.4.attention.self.value.bias                | [1024]\n",
      "text_encoder2.encoder.layer.4.attention.output.dense.weight            | [1024, 1024]\n",
      "text_encoder2.encoder.layer.4.attention.output.dense.bias              | [1024]\n",
      "text_encoder2.encoder.layer.4.attention.output.LayerNorm.weight        | [1024]\n",
      "text_encoder2.encoder.layer.4.attention.output.LayerNorm.bias          | [1024]\n",
      "text_encoder2.encoder.layer.4.intermediate.dense.weight                | [3072, 1024]\n",
      "text_encoder2.encoder.layer.4.intermediate.dense.bias                  | [3072]\n",
      "text_encoder2.encoder.layer.4.output.dense.weight                      | [1024, 3072]\n",
      "text_encoder2.encoder.layer.4.output.dense.bias                        | [1024]\n",
      "text_encoder2.encoder.layer.4.output.LayerNorm.weight                  | [1024]\n",
      "text_encoder2.encoder.layer.4.output.LayerNorm.bias                    | [1024]\n",
      "text_encoder2.encoder.layer.5.attention.self.query.weight              | [1024, 1024]\n",
      "text_encoder2.encoder.layer.5.attention.self.query.bias                | [1024]\n",
      "text_encoder2.encoder.layer.5.attention.self.key.weight                | [1024, 1024]\n",
      "text_encoder2.encoder.layer.5.attention.self.key.bias                  | [1024]\n",
      "text_encoder2.encoder.layer.5.attention.self.value.weight              | [1024, 1024]\n",
      "text_encoder2.encoder.layer.5.attention.self.value.bias                | [1024]\n",
      "text_encoder2.encoder.layer.5.attention.output.dense.weight            | [1024, 1024]\n",
      "text_encoder2.encoder.layer.5.attention.output.dense.bias              | [1024]\n",
      "text_encoder2.encoder.layer.5.attention.output.LayerNorm.weight        | [1024]\n",
      "text_encoder2.encoder.layer.5.attention.output.LayerNorm.bias          | [1024]\n",
      "text_encoder2.encoder.layer.5.intermediate.dense.weight                | [3072, 1024]\n",
      "text_encoder2.encoder.layer.5.intermediate.dense.bias                  | [3072]\n",
      "text_encoder2.encoder.layer.5.output.dense.weight                      | [1024, 3072]\n",
      "text_encoder2.encoder.layer.5.output.dense.bias                        | [1024]\n",
      "text_encoder2.encoder.layer.5.output.LayerNorm.weight                  | [1024]\n",
      "text_encoder2.encoder.layer.5.output.LayerNorm.bias                    | [1024]\n",
      "text_encoder2.encoder.layer.6.attention.self.query.weight              | [1024, 1024]\n",
      "text_encoder2.encoder.layer.6.attention.self.query.bias                | [1024]\n",
      "text_encoder2.encoder.layer.6.attention.self.key.weight                | [1024, 1024]\n",
      "text_encoder2.encoder.layer.6.attention.self.key.bias                  | [1024]\n",
      "text_encoder2.encoder.layer.6.attention.self.value.weight              | [1024, 1024]\n",
      "text_encoder2.encoder.layer.6.attention.self.value.bias                | [1024]\n",
      "text_encoder2.encoder.layer.6.attention.output.dense.weight            | [1024, 1024]\n",
      "text_encoder2.encoder.layer.6.attention.output.dense.bias              | [1024]\n",
      "text_encoder2.encoder.layer.6.attention.output.LayerNorm.weight        | [1024]\n",
      "text_encoder2.encoder.layer.6.attention.output.LayerNorm.bias          | [1024]\n",
      "text_encoder2.encoder.layer.6.intermediate.dense.weight                | [3072, 1024]\n",
      "text_encoder2.encoder.layer.6.intermediate.dense.bias                  | [3072]\n",
      "text_encoder2.encoder.layer.6.output.dense.weight                      | [1024, 3072]\n",
      "text_encoder2.encoder.layer.6.output.dense.bias                        | [1024]\n",
      "text_encoder2.encoder.layer.6.output.LayerNorm.weight                  | [1024]\n",
      "text_encoder2.encoder.layer.6.output.LayerNorm.bias                    | [1024]\n",
      "text_encoder2.encoder.layer.7.attention.self.query.weight              | [1024, 1024]\n",
      "text_encoder2.encoder.layer.7.attention.self.query.bias                | [1024]\n",
      "text_encoder2.encoder.layer.7.attention.self.key.weight                | [1024, 1024]\n",
      "text_encoder2.encoder.layer.7.attention.self.key.bias                  | [1024]\n",
      "text_encoder2.encoder.layer.7.attention.self.value.weight              | [1024, 1024]\n",
      "text_encoder2.encoder.layer.7.attention.self.value.bias                | [1024]\n",
      "text_encoder2.encoder.layer.7.attention.output.dense.weight            | [1024, 1024]\n",
      "text_encoder2.encoder.layer.7.attention.output.dense.bias              | [1024]\n",
      "text_encoder2.encoder.layer.7.attention.output.LayerNorm.weight        | [1024]\n",
      "text_encoder2.encoder.layer.7.attention.output.LayerNorm.bias          | [1024]\n",
      "text_encoder2.encoder.layer.7.intermediate.dense.weight                | [3072, 1024]\n",
      "text_encoder2.encoder.layer.7.intermediate.dense.bias                  | [3072]\n",
      "text_encoder2.encoder.layer.7.output.dense.weight                      | [1024, 3072]\n",
      "text_encoder2.encoder.layer.7.output.dense.bias                        | [1024]\n",
      "text_encoder2.encoder.layer.7.output.LayerNorm.weight                  | [1024]\n",
      "text_encoder2.encoder.layer.7.output.LayerNorm.bias                    | [1024]\n",
      "text_encoder2.encoder.layer.8.attention.self.query.weight              | [1024, 1024]\n",
      "text_encoder2.encoder.layer.8.attention.self.query.bias                | [1024]\n",
      "text_encoder2.encoder.layer.8.attention.self.key.weight                | [1024, 1024]\n",
      "text_encoder2.encoder.layer.8.attention.self.key.bias                  | [1024]\n",
      "text_encoder2.encoder.layer.8.attention.self.value.weight              | [1024, 1024]\n",
      "text_encoder2.encoder.layer.8.attention.self.value.bias                | [1024]\n",
      "text_encoder2.encoder.layer.8.attention.output.dense.weight            | [1024, 1024]\n",
      "text_encoder2.encoder.layer.8.attention.output.dense.bias              | [1024]\n",
      "text_encoder2.encoder.layer.8.attention.output.LayerNorm.weight        | [1024]\n",
      "text_encoder2.encoder.layer.8.attention.output.LayerNorm.bias          | [1024]\n",
      "text_encoder2.encoder.layer.8.intermediate.dense.weight                | [3072, 1024]\n",
      "text_encoder2.encoder.layer.8.intermediate.dense.bias                  | [3072]\n",
      "text_encoder2.encoder.layer.8.output.dense.weight                      | [1024, 3072]\n",
      "text_encoder2.encoder.layer.8.output.dense.bias                        | [1024]\n",
      "text_encoder2.encoder.layer.8.output.LayerNorm.weight                  | [1024]\n",
      "text_encoder2.encoder.layer.8.output.LayerNorm.bias                    | [1024]\n",
      "text_encoder2.encoder.layer.9.attention.self.query.weight              | [1024, 1024]\n",
      "text_encoder2.encoder.layer.9.attention.self.query.bias                | [1024]\n",
      "text_encoder2.encoder.layer.9.attention.self.key.weight                | [1024, 1024]\n",
      "text_encoder2.encoder.layer.9.attention.self.key.bias                  | [1024]\n",
      "text_encoder2.encoder.layer.9.attention.self.value.weight              | [1024, 1024]\n",
      "text_encoder2.encoder.layer.9.attention.self.value.bias                | [1024]\n",
      "text_encoder2.encoder.layer.9.attention.output.dense.weight            | [1024, 1024]\n",
      "text_encoder2.encoder.layer.9.attention.output.dense.bias              | [1024]\n",
      "text_encoder2.encoder.layer.9.attention.output.LayerNorm.weight        | [1024]\n",
      "text_encoder2.encoder.layer.9.attention.output.LayerNorm.bias          | [1024]\n",
      "text_encoder2.encoder.layer.9.intermediate.dense.weight                | [3072, 1024]\n",
      "text_encoder2.encoder.layer.9.intermediate.dense.bias                  | [3072]\n",
      "text_encoder2.encoder.layer.9.output.dense.weight                      | [1024, 3072]\n",
      "text_encoder2.encoder.layer.9.output.dense.bias                        | [1024]\n",
      "text_encoder2.encoder.layer.9.output.LayerNorm.weight                  | [1024]\n",
      "text_encoder2.encoder.layer.9.output.LayerNorm.bias                    | [1024]\n",
      "text_encoder2.encoder.layer.10.attention.self.query.weight             | [1024, 1024]\n",
      "text_encoder2.encoder.layer.10.attention.self.query.bias               | [1024]\n",
      "text_encoder2.encoder.layer.10.attention.self.key.weight               | [1024, 1024]\n",
      "text_encoder2.encoder.layer.10.attention.self.key.bias                 | [1024]\n",
      "text_encoder2.encoder.layer.10.attention.self.value.weight             | [1024, 1024]\n",
      "text_encoder2.encoder.layer.10.attention.self.value.bias               | [1024]\n",
      "text_encoder2.encoder.layer.10.attention.output.dense.weight           | [1024, 1024]\n",
      "text_encoder2.encoder.layer.10.attention.output.dense.bias             | [1024]\n",
      "text_encoder2.encoder.layer.10.attention.output.LayerNorm.weight       | [1024]\n",
      "text_encoder2.encoder.layer.10.attention.output.LayerNorm.bias         | [1024]\n",
      "text_encoder2.encoder.layer.10.intermediate.dense.weight               | [3072, 1024]\n",
      "text_encoder2.encoder.layer.10.intermediate.dense.bias                 | [3072]\n",
      "text_encoder2.encoder.layer.10.output.dense.weight                     | [1024, 3072]\n",
      "text_encoder2.encoder.layer.10.output.dense.bias                       | [1024]\n",
      "text_encoder2.encoder.layer.10.output.LayerNorm.weight                 | [1024]\n",
      "text_encoder2.encoder.layer.10.output.LayerNorm.bias                   | [1024]\n",
      "text_encoder2.encoder.layer.11.attention.self.query.weight             | [1024, 1024]\n",
      "text_encoder2.encoder.layer.11.attention.self.query.bias               | [1024]\n",
      "text_encoder2.encoder.layer.11.attention.self.key.weight               | [1024, 1024]\n",
      "text_encoder2.encoder.layer.11.attention.self.key.bias                 | [1024]\n",
      "text_encoder2.encoder.layer.11.attention.self.value.weight             | [1024, 1024]\n",
      "text_encoder2.encoder.layer.11.attention.self.value.bias               | [1024]\n",
      "text_encoder2.encoder.layer.11.attention.output.dense.weight           | [1024, 1024]\n",
      "text_encoder2.encoder.layer.11.attention.output.dense.bias             | [1024]\n",
      "text_encoder2.encoder.layer.11.attention.output.LayerNorm.weight       | [1024]\n",
      "text_encoder2.encoder.layer.11.attention.output.LayerNorm.bias         | [1024]\n",
      "text_encoder2.encoder.layer.11.intermediate.dense.weight               | [3072, 1024]\n",
      "text_encoder2.encoder.layer.11.intermediate.dense.bias                 | [3072]\n",
      "text_encoder2.encoder.layer.11.output.dense.weight                     | [1024, 3072]\n",
      "text_encoder2.encoder.layer.11.output.dense.bias                       | [1024]\n",
      "text_encoder2.encoder.layer.11.output.LayerNorm.weight                 | [1024]\n",
      "text_encoder2.encoder.layer.11.output.LayerNorm.bias                   | [1024]\n",
      "text_encoder2.pooler.dense.weight                                      | [1024, 1024]\n",
      "text_encoder2.pooler.dense.bias                                        | [1024]\n",
      "encode_prefix.weight                                                   | [64, 1024]\n",
      "encode_prefix.bias                                                     | [64]\n",
      "decode_prefix.weight                                                   | [1024, 64]\n",
      "decode_prefix.bias                                                     | [1024]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def inspect_checkpoint(ckpt_path):\n",
    "    print(f\"--- INSPECTING CHECKPOINT: {ckpt_path} ---\")\n",
    "    \n",
    "    # Load the checkpoint (map to CPU to avoid VRAM issues)\n",
    "    checkpoint = torch.load(ckpt_path, map_location='cpu', weights_only=False)\n",
    "    \n",
    "    # Check if the state_dict is nested under 'model' or 'state_dict' (typical for PyTorch Lightning)\n",
    "    if 'model' in checkpoint:\n",
    "        state_dict = checkpoint['model']\n",
    "        print(\"Detected 'model' key in checkpoint.\")\n",
    "    elif 'state_dict' in checkpoint:\n",
    "        state_dict = checkpoint['state_dict']\n",
    "        print(\"Detected 'state_dict' key in checkpoint.\")\n",
    "    else:\n",
    "        state_dict = checkpoint\n",
    "        print(\"No nested keys found; using top-level as state_dict.\")\n",
    "\n",
    "    # Iterate and print key names and tensor shapes\n",
    "    print(f\"{'Parameter Name':<70} | {'Shape'}\")\n",
    "    print(\"-\" * 90)\n",
    "    \n",
    "    # We can filter for specific layers if the list is too long\n",
    "    # e.g., only looking at the bottleneck and cross-attention\n",
    "    relevant_keys = ['encode_prefix', 'decode_prefix', 'crossattention']\n",
    "    \n",
    "    for key, value in state_dict.items():\n",
    "        # Check if it's a tensor (parameters) or something else\n",
    "        if torch.is_tensor(value):\n",
    "            # Optional: Uncomment to filter for only relevant layers\n",
    "            # if any(r in key for r in relevant_keys):\n",
    "            print(f\"{key:<70} | {list(value.shape)}\")\n",
    "        else:\n",
    "            print(f\"{key:<70} | [Non-Tensor: {type(value)}]\")\n",
    "\n",
    "# Run the inspection\n",
    "ckpt_path = '/kaggle/input/checkpoint/checkpoint_autoencoder.ckpt'\n",
    "inspect_checkpoint(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d786b73c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T06:32:19.065629Z",
     "iopub.status.busy": "2026-02-09T06:32:19.064822Z",
     "iopub.status.idle": "2026-02-09T06:32:19.069911Z",
     "shell.execute_reply": "2026-02-09T06:32:19.068781Z"
    },
    "papermill": {
     "duration": 0.017515,
     "end_time": "2026-02-09T06:32:19.071970",
     "exception": false,
     "start_time": "2026-02-09T06:32:19.054455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    " ae_config = {\n",
    "        'bert_config_decoder': '/kaggle/input/params/config_decoder.json',\n",
    "        'bert_config_encoder': '/kaggle/input/params/config_encoder.json',\n",
    "        'embed_dim': 256,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fe89c4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T06:32:19.090843Z",
     "iopub.status.busy": "2026-02-09T06:32:19.090227Z",
     "iopub.status.idle": "2026-02-09T06:32:19.103836Z",
     "shell.execute_reply": "2026-02-09T06:32:19.102986Z"
    },
    "papermill": {
     "duration": 0.025551,
     "end_time": "2026-02-09T06:32:19.105778",
     "exception": false,
     "start_time": "2026-02-09T06:32:19.080227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "class regexTokenizer():\n",
    "    def __init__(self,vocab_path='/kaggle/input/vocab-bpe-300/vocab_bpe_300_sc.txt',max_len=127):\n",
    "        with open(vocab_path,'r') as f:\n",
    "            x = f.readlines()\n",
    "            x = [xx.replace('##', '') for xx in x]\n",
    "            x2 = x.copy()\n",
    "        x2.sort(key=len, reverse=True)\n",
    "        pattern = \"(\"+\"|\".join(re.escape(token).strip()[:-1] for token in x2)+\")\"\n",
    "        self.rg = re.compile(pattern)\n",
    "\n",
    "        self.idtotok  = { cnt:i.strip() for cnt,i in enumerate(x)}\n",
    "        self.vocab_size = len(self.idtotok) #SOS, EOS, pad\n",
    "        self.toktoid = { v:k for k,v in self.idtotok.items()}\n",
    "        self.max_len = max_len\n",
    "        self.cls_token_id = self.toktoid['[CLS]']\n",
    "        self.sep_token_id = self.toktoid['[SEP]']\n",
    "        self.pad_token_id = self.toktoid['[PAD]']\n",
    "\n",
    "    def decode_one(self, iter):\n",
    "        if self.sep_token_id in iter:   iter = iter[:(iter == self.sep_token_id).nonzero(as_tuple=True)[0][0].item()]\n",
    "        # return \"\".join([self.ind2Letter(i) for i in iter]).replace('[SOS]','').replace('[EOS]','').replace('[PAD]','')\n",
    "        return \"\".join([self.idtotok[i.item()] for i in iter[1:]])\n",
    "\n",
    "    def decode(self,ids:torch.tensor):\n",
    "        if len(ids.shape)==1:\n",
    "            return [self.decode_one(ids)]\n",
    "        else:\n",
    "            smiles  = []\n",
    "            for i in ids:\n",
    "                smiles.append(self.decode_one(i))\n",
    "            return smiles\n",
    "    def __len__(self):\n",
    "        return self.vocab_size\n",
    "\n",
    "    def __call__(self,smis:list, truncation='max_len'):\n",
    "        tensors = []\n",
    "        lengths = []\n",
    "        if type(smis) is str:\n",
    "            smis = [smis]\n",
    "        for i in smis:\n",
    "            length, tensor = self.encode_one(i)\n",
    "            tensors.append(tensor)\n",
    "            lengths.append(length)\n",
    "        output = torch.concat(tensors,dim=0)\n",
    "        if truncation == 'max_len':\n",
    "            return output\n",
    "        elif truncation == 'longest':\n",
    "            return output[:, :max(lengths)]\n",
    "        else:\n",
    "            raise ValueError('truncation should be either max_len or longest')\n",
    "\n",
    "    def encode_one(self, smi):\n",
    "        smi = '[CLS]' + smi + '[SEP]'\n",
    "        res = [self.toktoid[i] for i in self.rg.findall(smi)]\n",
    "        token_length = len(res)\n",
    "        if token_length < self.max_len:\n",
    "            res += [self.pad_token_id]*(self.max_len-len(res))\n",
    "        else:\n",
    "            res = res[:self.max_len]\n",
    "            # res[-1] = self.sep_token_id\n",
    "        return token_length, torch.LongTensor([res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "386106c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T06:32:19.124300Z",
     "iopub.status.busy": "2026-02-09T06:32:19.123826Z",
     "iopub.status.idle": "2026-02-09T06:32:19.136528Z",
     "shell.execute_reply": "2026-02-09T06:32:19.135484Z"
    },
    "papermill": {
     "duration": 0.024498,
     "end_time": "2026-02-09T06:32:19.138762",
     "exception": false,
     "start_time": "2026-02-09T06:32:19.114264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = regexTokenizer(vocab_path='/kaggle/input/vocab-bpe-300/vocab_bpe_300_sc.txt', max_len=127)#newtkn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11ff400d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T06:32:19.157544Z",
     "iopub.status.busy": "2026-02-09T06:32:19.157225Z",
     "iopub.status.idle": "2026-02-09T06:32:23.400083Z",
     "shell.execute_reply": "2026-02-09T06:32:23.398899Z"
    },
    "papermill": {
     "duration": 4.254988,
     "end_time": "2026-02-09T06:32:23.402307",
     "exception": false,
     "start_time": "2026-02-09T06:32:19.147319",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rdkit in /usr/local/lib/python3.12/dist-packages (2025.9.4)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rdkit) (2.0.2)\r\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from rdkit) (11.3.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install rdkit\n",
    "from abc import abstractmethod\n",
    "from rdkit import Chem\n",
    "from random import shuffle\n",
    "from typing import Any\n",
    "from typing import Iterable\n",
    "from typing import List\n",
    "from typing import Union\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "Mol = Chem.Mol\n",
    "\n",
    "\n",
    "class Augmenter:\n",
    "    \"\"\"An abstract base class for molecular augmenters.\n",
    "\n",
    "    The class has one method, `augment`, which is overriden by child classes.\n",
    "    It is possible to call the class with either a list of molecules or a single\n",
    "    molecules. This input will then be passed to `augment` and the augmented\n",
    "    molecule(s) will be returned.\n",
    "    The Boolean \".active\" property can be set to toggle augmentation.\n",
    "\n",
    "    :param active: Whether the augmentation should be active or not, defaults to True.\n",
    "    :param augment_prob: if lower than 1, it is used to randomly turn-off augmentation on an individual basis\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, active: bool = True, augment_prob: float = 1.0) -> None:\n",
    "        self.active = active\n",
    "        self.augment_prob = augment_prob\n",
    "\n",
    "    def __call__(self, data: Union[Iterable[Any], Any]) -> List[Any]:\n",
    "        \"\"\"Augments either a list of Anys or a single molecule by making sure\n",
    "        the input is put into a `List` and then passed to the `augment` function.\n",
    "\n",
    "        :param data: Either a list of molecules or a single molecules to be augmented.\n",
    "\n",
    "        :return: A list of augmented molecules.\n",
    "        \"\"\"\n",
    "        # Str is Iterable but must be encapsulated (e.g. single SMILES string)\n",
    "        if not isinstance(data, Iterable) or isinstance(data, str):\n",
    "            data = [data]\n",
    "\n",
    "        return self.augment(data)\n",
    "\n",
    "    @abstractmethod\n",
    "    def _augment(self, data: Iterable[Any]) -> List[Any]:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def augment(self, data: Iterable[Any]) -> List[Any]:\n",
    "        \"\"\"\n",
    "        Augment a given list\n",
    "\n",
    "        :param data: a list of molecules to be augmented.\n",
    "        :return: A list of augmented molecules.\n",
    "        \"\"\"\n",
    "        if self.active:\n",
    "            return self._augment(data)\n",
    "        return list(data)\n",
    "\n",
    "\n",
    "class MolAugmenter(Augmenter):\n",
    "    \"\"\"\n",
    "    Augmenter that works on RDKit Mol objects\n",
    "    \"\"\"\n",
    "\n",
    "    def randomize_mols_restricted(self, mols: Iterable[Mol]) -> List[Mol]:\n",
    "        \"\"\"Randomizes the atom ordering of a list of RDKit molecules (`rdkit.Chem.Mol`:s).\n",
    "\n",
    "        :param mols: List of RDKit molecules to be augmented.\n",
    "        :return:  List of augmented RDKit molecules.\n",
    "        \"\"\"\n",
    "        return list(map(self.randomize_mol_restricted, mols))\n",
    "\n",
    "    def randomize_mol_restricted(self, mol: Mol) -> Mol:\n",
    "        \"\"\"Randomize the atom ordering of a RDKit molecule (`rdkit.Chem.Mol`).\n",
    "\n",
    "        :param mol:  RDKit molecule to get a randomized atom order.\n",
    "        :return: RDKit molecule object with a randomized atom-order.\n",
    "        \"\"\"\n",
    "        # Standard shuffle surprisingly leads to 35% slower code.\n",
    "        if self.augment_prob < np.random.rand():\n",
    "            return mol\n",
    "        atom_order: List[int] = list(range(mol.GetNumAtoms()))\n",
    "        np.random.shuffle(atom_order)\n",
    "        return Chem.RenumberAtoms(mol, atom_order)\n",
    "\n",
    "    def _augment(self, data: Iterable[Mol]) -> List[Mol]:\n",
    "        \"\"\"Randomizes `RDKit molecules by shuffling the atom order.\n",
    "\n",
    "        :param data: List of RDKit molecules to be randomized.\n",
    "        :return:  A list of randomized molecules.\n",
    "        \"\"\"\n",
    "        return self.randomize_mols_restricted(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "617314f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T06:32:23.421564Z",
     "iopub.status.busy": "2026-02-09T06:32:23.421228Z",
     "iopub.status.idle": "2026-02-09T06:32:47.945852Z",
     "shell.execute_reply": "2026-02-09T06:32:47.944827Z"
    },
    "papermill": {
     "duration": 24.537119,
     "end_time": "2026-02-09T06:32:47.948039",
     "exception": false,
     "start_time": "2026-02-09T06:32:23.410920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-09 06:32:29.950791: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1770618750.198415      17 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1770618750.269201      17 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1770618750.859219      17 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770618750.859335      17 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770618750.859339      17 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770618750.859342      17 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertConfig, BertModel, BertForMaskedLM\n",
    "\n",
    "class LDMolAutoencoder(nn.Module):\n",
    "    def __init__(self, checkpoint_path=None, use_linear=True,tokenizer=None, device=None):\n",
    "        super().__init__()\n",
    "        self.use_linear = use_linear\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        # 1. Encoder Configuration (hidden_size: 1024)\n",
    "        enc_config = BertConfig(\n",
    "            vocab_size=300,\n",
    "            hidden_size=1024,\n",
    "            num_hidden_layers=12,\n",
    "            num_attention_heads=16,\n",
    "            intermediate_size=3072,\n",
    "            hidden_act=\"gelu\",\n",
    "            hidden_dropout_prob=0.1,\n",
    "            attention_probs_dropout_prob=0.1,\n",
    "            max_position_embeddings=512,\n",
    "            type_vocab_size=2,\n",
    "            pad_token_id=0\n",
    "        )\n",
    "        self.text_encoder2 = BertModel(enc_config)\n",
    "\n",
    "        # 2. Decoder Configuration (hidden_size: 768)\n",
    "        dec_config = BertConfig(\n",
    "            vocab_size=300,\n",
    "            hidden_size=768,\n",
    "            num_hidden_layers=12,\n",
    "            num_attention_heads=12,\n",
    "            intermediate_size=3072,\n",
    "            hidden_act=\"gelu\",\n",
    "            hidden_dropout_prob=0.1,\n",
    "            attention_probs_dropout_prob=0.1,\n",
    "            max_position_embeddings=512,\n",
    "            type_vocab_size=2,\n",
    "            pad_token_id=0,\n",
    "            is_decoder=True,\n",
    "            add_cross_attention=True\n",
    "        )\n",
    "        self.text_encoder = BertForMaskedLM(dec_config)\n",
    "        for layer in self.text_encoder.bert.encoder.layer:\n",
    "            layer.crossattention.self.key = nn.Linear(1024, 768)\n",
    "            layer.crossattention.self.value = nn.Linear(1024, 768)\n",
    "\n",
    "        # 4. Bottleneck Layers\n",
    "        if self.use_linear:\n",
    "            self.encode_prefix = nn.Linear(1024, 64)\n",
    "            self.decode_prefix = nn.Linear(64, 1024)\n",
    "\n",
    "        # 5. Load Checkpoint\n",
    "        if checkpoint_path:\n",
    "            self.load_checkpoint(checkpoint_path)\n",
    "\n",
    "    def load_checkpoint(self, path):\n",
    "        print(f\"Loading checkpoint from {path}...\")\n",
    "        checkpoint = torch.load(path, map_location='cpu')\n",
    "        \n",
    "        # Handle different checkpoint structures (Lightning vs Raw PyTorch)\n",
    "        state_dict = checkpoint.get('state_dict', checkpoint.get('model', checkpoint))\n",
    "        \n",
    "        # Standardize keys: remove \"model.\" prefix if it exists\n",
    "        new_state_dict = {k.replace('model.', ''): v for k, v in state_dict.items()}\n",
    "        \n",
    "        # Load the model - strict=False ignores the position_ids buffer mismatch\n",
    "        msg = self.load_state_dict(new_state_dict, strict=False)\n",
    "        print(f\"Checkpoint loaded. Missing: {len(msg.missing_keys)}, Unexpected: {len(msg.unexpected_keys)}\")\n",
    "\n",
    "    def forward(self, text_input_ids, text_attention_mask, text_input_ids2, text_attention_mask2):\n",
    "        # 1. Encode with frozen text_encoder2\n",
    "        with torch.no_grad():\n",
    "            enc_out = self.text_encoder2(\n",
    "                input_ids=text_input_ids2,\n",
    "                attention_mask=text_attention_mask2,\n",
    "                return_dict=True\n",
    "            ).last_hidden_state # Shape: [batch, seq, 1024]\n",
    "\n",
    "        # 2. Bottleneck Projection\n",
    "        if self.use_linear:\n",
    "            # Compress to 64 then expand back to 1024\n",
    "            latent = self.encode_prefix(enc_out)\n",
    "            enc_out = self.decode_prefix(latent)\n",
    "\n",
    "        # 3. Decode with Cross-Attention\n",
    "        # Labels are used for training reconstruction (shifted internally if using loss)\n",
    "        dec_out = self.text_encoder(\n",
    "            input_ids=text_input_ids,\n",
    "            attention_mask=text_attention_mask,\n",
    "            encoder_hidden_states=enc_out, # This is the 1024-dim context\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        return dec_out.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23d6b351",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T06:32:47.968589Z",
     "iopub.status.busy": "2026-02-09T06:32:47.967160Z",
     "iopub.status.idle": "2026-02-09T06:32:53.482920Z",
     "shell.execute_reply": "2026-02-09T06:32:53.481551Z"
    },
    "papermill": {
     "duration": 5.528509,
     "end_time": "2026-02-09T06:32:53.485102",
     "exception": false,
     "start_time": "2026-02-09T06:32:47.956593",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for bi-directional self-attention.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING PRETRAINED MODEL: /kaggle/input/checkpoint/checkpoint_autoencoder.ckpt\n",
      "Loading Status: _IncompatibleKeys(missing_keys=[], unexpected_keys=['text_encoder2.embeddings.position_ids', 'text_encoder.bert.embeddings.position_ids'])\n",
      "AE total parameters: 119,491,180\n",
      "AE trainable parameters: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "ckpt_path = '/kaggle/input/checkpoint/checkpoint_autoencoder.ckpt'\n",
    "\n",
    "# 1. Initialize the model with the manual surgery already inside\n",
    "# Note: We pass use_linear=True to match your encode/decode_prefix layers\n",
    "ae_model = LDMolAutoencoder(use_linear=True, tokenizer=tokenizer, device = device)\n",
    "\n",
    "if os.path.exists(ckpt_path):\n",
    "    print(f'LOADING PRETRAINED MODEL: {ckpt_path}')\n",
    "    \n",
    "    # Use weights_only=False if the ckpt contains custom classes\n",
    "    checkpoint = torch.load(ckpt_path, map_location='cpu', weights_only=False)\n",
    "    \n",
    "    # Extract the state dict (handling both 'model' and 'state_dict' keys)\n",
    "    state_dict = checkpoint.get('model', checkpoint.get('state_dict', checkpoint))\n",
    "    clean_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        new_key = k.replace('model.', '') \n",
    "        clean_state_dict[new_key] = v\n",
    "\n",
    "    # Load with strict=False to ignore the 'position_ids' buffer mismatches\n",
    "    msg = ae_model.load_state_dict(clean_state_dict, strict=False)\n",
    "    print(f'Loading Status: {msg}')\n",
    "else:\n",
    "    print(f'WARNING: Checkpoint not found at {ckpt_path}')\n",
    "\n",
    "# 2. Freeze all parameters\n",
    "for param in ae_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 3. Memory Optimization: Remove the heavy 1024-dim Encoder\n",
    "# You mentioned deleting text_encoder2; ensure your code doesn't \n",
    "# call it in the forward pass later if you do this!\n",
    "if hasattr(ae_model, 'text_encoder2'):\n",
    "    del ae_model.text_encoder2\n",
    "    # Optional: Clear CUDA cache if you've already moved to GPU\n",
    "    # torch.cuda.empty_cache() \n",
    "\n",
    "# 4. Move to device and set to evaluation mode\n",
    "ae_model = ae_model.to(device)\n",
    "ae_model.eval()\n",
    "\n",
    "# 5. Verify parameter counts\n",
    "total_params = sum(p.numel() for p in ae_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in ae_model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'AE total parameters: {total_params:,}')\n",
    "print(f'AE trainable parameters: {trainable_params:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b0ddd0",
   "metadata": {
    "papermill": {
     "duration": 0.008747,
     "end_time": "2026-02-09T06:32:53.503168",
     "exception": false,
     "start_time": "2026-02-09T06:32:53.494421",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Text/Instruction Encoder & Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c674e48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T06:32:53.522690Z",
     "iopub.status.busy": "2026-02-09T06:32:53.522041Z",
     "iopub.status.idle": "2026-02-09T06:32:53.606648Z",
     "shell.execute_reply": "2026-02-09T06:32:53.605773Z"
    },
    "papermill": {
     "duration": 0.097233,
     "end_time": "2026-02-09T06:32:53.609215",
     "exception": false,
     "start_time": "2026-02-09T06:32:53.511982",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import argparse\n",
    "from einops import repeat\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "import time\n",
    "from rdkit import Chem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36dad973",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T06:32:53.629202Z",
     "iopub.status.busy": "2026-02-09T06:32:53.628891Z",
     "iopub.status.idle": "2026-02-09T06:33:25.620382Z",
     "shell.execute_reply": "2026-02-09T06:33:25.592359Z"
    },
    "papermill": {
     "duration": 32.035342,
     "end_time": "2026-02-09T06:33:25.653555",
     "exception": false,
     "start_time": "2026-02-09T06:32:53.618213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e1c41b2dcc7435ab8cc81dd0d6876f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/700 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e671687b70674b128ad83b2cc236f971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c45d249aefd433d8f4e94717beae097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56f7717cbe4e4d3f8b30a56309e01cd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ee80929d33b404486a28a4fcba90df9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf516c69f2654260892e95347967460b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4389e1a1467a4411af71ea690a870388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text encoder #parameters: 374130176, #trainable: 0\n"
     ]
    }
   ],
   "source": [
    "text_encoder = T5ForConditionalGeneration.from_pretrained('laituan245/molt5-large-caption2smiles').to(device)\n",
    "text_tokenizer = T5Tokenizer.from_pretrained(\"laituan245/molt5-large-caption2smiles\", model_max_length=512)\n",
    "del text_encoder.decoder\n",
    "\n",
    "for param in text_encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "text_encoder.eval()\n",
    "print(f'text encoder #parameters: {sum(p.numel() for p in text_encoder.parameters())}, #trainable: {sum(p.numel() for p in text_encoder.parameters() if p.requires_grad)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "efdf25ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T06:33:26.143668Z",
     "iopub.status.busy": "2026-02-09T06:33:26.136461Z",
     "iopub.status.idle": "2026-02-09T06:33:26.272893Z",
     "shell.execute_reply": "2026-02-09T06:33:26.248435Z"
    },
    "papermill": {
     "duration": 0.417264,
     "end_time": "2026-02-09T06:33:26.311807",
     "exception": false,
     "start_time": "2026-02-09T06:33:25.894543",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def molT5_encoder(descriptions, molt5, molt5_tokenizer, description_length, device):\n",
    "    tokenized = molt5_tokenizer(descriptions, padding='max_length', truncation=True, max_length=description_length, return_tensors=\"pt\").to(device)\n",
    "    encoder_outputs = molt5.encoder(input_ids=tokenized.input_ids, attention_mask=tokenized.attention_mask, return_dict=True).last_hidden_state\n",
    "    return encoder_outputs, tokenized.attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21095ed0",
   "metadata": {
    "papermill": {
     "duration": 0.1061,
     "end_time": "2026-02-09T06:33:26.591222",
     "exception": false,
     "start_time": "2026-02-09T06:33:26.485122",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Modifying the Description/Instruction here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "369993fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T06:33:26.961379Z",
     "iopub.status.busy": "2026-02-09T06:33:26.960888Z",
     "iopub.status.idle": "2026-02-09T06:33:27.012561Z",
     "shell.execute_reply": "2026-02-09T06:33:26.994497Z"
    },
    "papermill": {
     "duration": 0.330234,
     "end_time": "2026-02-09T06:33:27.070552",
     "exception": false,
     "start_time": "2026-02-09T06:33:26.740318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"This molecule contains an benzoyl group.\"\n",
    "prompt_null = \"no dsecription.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1620e4c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T06:33:27.604730Z",
     "iopub.status.busy": "2026-02-09T06:33:27.604112Z",
     "iopub.status.idle": "2026-02-09T06:33:40.440407Z",
     "shell.execute_reply": "2026-02-09T06:33:40.439521Z"
    },
    "papermill": {
     "duration": 13.106238,
     "end_time": "2026-02-09T06:33:40.442469",
     "exception": false,
     "start_time": "2026-02-09T06:33:27.336231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "biot5_embed, pad_mask = molT5_encoder([prompt], text_encoder, text_tokenizer, 200, device)\n",
    "biot5_embed_null, pad_mask_null = molT5_encoder([prompt_null], text_encoder, text_tokenizer, 200, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e73870c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T06:33:40.463419Z",
     "iopub.status.busy": "2026-02-09T06:33:40.463024Z",
     "iopub.status.idle": "2026-02-09T06:33:40.473463Z",
     "shell.execute_reply": "2026-02-09T06:33:40.471837Z"
    },
    "papermill": {
     "duration": 0.024194,
     "end_time": "2026-02-09T06:33:40.476295",
     "exception": false,
     "start_time": "2026-02-09T06:33:40.452101",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def AE_SMILES_encoder(sm, ae_model):\n",
    "    if sm[0][:5] == \"[CLS]\":    sm = [s[5:] for s in sm]\n",
    "    text_input = ae_model.tokenizer(sm).to(ae_model.device)\n",
    "    text_input_ids = text_input\n",
    "    text_attention_mask = torch.where(text_input_ids == 0, 0, 1).to(text_input.device)\n",
    "    if hasattr(ae_model.text_encoder2, 'bert'):\n",
    "        output = ae_model.text_encoder2.bert(text_input_ids, attention_mask=text_attention_mask, return_dict=True, mode='text').last_hidden_state\n",
    "    else:\n",
    "        output = ae_model.text_encoder2(text_input_ids, attention_mask=text_attention_mask, return_dict=True).last_hidden_state\n",
    "\n",
    "    if hasattr(ae_model, 'encode_prefix'):\n",
    "        output = ae_model.encode_prefix(output)\n",
    "        if ae_model.output_dim*2 == output.size(-1):\n",
    "            mean, logvar = torch.chunk(output, 2, dim=-1)\n",
    "            logvar = torch.clamp(logvar, -30.0, 20.0)\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            output = mean + std * torch.randn_like(mean)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47396881",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T06:33:40.505759Z",
     "iopub.status.busy": "2026-02-09T06:33:40.505230Z",
     "iopub.status.idle": "2026-02-09T06:33:41.961151Z",
     "shell.execute_reply": "2026-02-09T06:33:41.960020Z"
    },
    "papermill": {
     "duration": 1.474646,
     "end_time": "2026-02-09T06:33:41.965324",
     "exception": false,
     "start_time": "2026-02-09T06:33:40.490678",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "biot5_embed = repeat(biot5_embed, '1 L D -> B L D', B=5)\n",
    "pad_mask = repeat(pad_mask, '1 L -> B L', B=5)\n",
    "y_cond = biot5_embed.to(device).type(torch.float32)\n",
    "pad_mask_cond = pad_mask.to(device).bool()\n",
    "\n",
    "biot5_embed_null = repeat(biot5_embed_null, '1 L D -> B L D', B=5)\n",
    "pad_mask_null = repeat(pad_mask_null, '1 L -> B L', B=5)\n",
    "y_null = biot5_embed_null.to(device).to(torch.float32)\n",
    "pad_mask_null = pad_mask_null.to(device).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c04e112",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T06:33:42.010830Z",
     "iopub.status.busy": "2026-02-09T06:33:42.010295Z",
     "iopub.status.idle": "2026-02-09T06:33:42.020180Z",
     "shell.execute_reply": "2026-02-09T06:33:42.018988Z"
    },
    "papermill": {
     "duration": 0.032383,
     "end_time": "2026-02-09T06:33:42.023110",
     "exception": false,
     "start_time": "2026-02-09T06:33:41.990727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_validity(smiles):\n",
    "    from rdkit import Chem\n",
    "    v = []\n",
    "    for l in smiles:\n",
    "        try:\n",
    "            if l == \"\":\n",
    "                continue\n",
    "            s = Chem.MolToSmiles(Chem.MolFromSmiles(l), isomericSmiles=False)\n",
    "            v.append(s)\n",
    "        except:\n",
    "            continue\n",
    "    u = list(set(v))\n",
    "    if len(u) == 0:\n",
    "        return 0., 0.\n",
    "    return len(v) / len(smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ffbf83e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T06:33:42.074658Z",
     "iopub.status.busy": "2026-02-09T06:33:42.073887Z",
     "iopub.status.idle": "2026-02-09T06:33:42.118562Z",
     "shell.execute_reply": "2026-02-09T06:33:42.116648Z"
    },
    "papermill": {
     "duration": 0.073538,
     "end_time": "2026-02-09T06:33:42.120921",
     "exception": false,
     "start_time": "2026-02-09T06:33:42.047383",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.distributions.categorical import Categorical\n",
    "@torch.no_grad()\n",
    "def generate(model, image_embeds, text, stochastic=True, prop_att_mask=None, k=None):\n",
    "    text_atts = torch.where(text == 0, 0, 1)\n",
    "    if prop_att_mask is None:   prop_att_mask = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(image_embeds.device)\n",
    "    token_output = model.text_encoder(text,\n",
    "                                      attention_mask=text_atts,\n",
    "                                      encoder_hidden_states=image_embeds,\n",
    "                                      encoder_attention_mask=prop_att_mask,\n",
    "                                      return_dict=True)\n",
    "                                        # batch*300\n",
    "    token_output = token_output.logits[:, -1, :]\n",
    "    if k:\n",
    "        p = torch.softmax(token_output, dim=-1)\n",
    "        if stochastic:\n",
    "            output = torch.multinomial(p, num_samples=k, replacement=False)\n",
    "            return torch.log(torch.stack([p[i][output[i]] for i in range(output.size(0))])), output\n",
    "        else:\n",
    "            output = torch.topk(p, k=k, dim=-1)  # batch*k\n",
    "            return torch.log(output.values), output.indices\n",
    "    if stochastic:\n",
    "        p = torch.softmax(token_output, dim=-1)\n",
    "        m = Categorical(p)\n",
    "        token_output = m.sample()\n",
    "    else:\n",
    "        token_output = torch.argmax(token_output, dim=-1)\n",
    "    return token_output.unsqueeze(1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def AE_SMILES_decoder(pv, model, stochastic=False, k=2, max_length=150):\n",
    "    if hasattr(model, 'decode_prefix'):\n",
    "        pv = model.decode_prefix(pv)\n",
    "\n",
    "    tokenizer = model.tokenizer\n",
    "    if tokenizer is None:\n",
    "        raise ValueError('Tokenizer is not defined')\n",
    "    # test\n",
    "    model.eval()\n",
    "    candidate = []\n",
    "    if k == 1:\n",
    "        text_input = torch.tensor([tokenizer.cls_token_id]).expand(pv.size(0), 1).to(model.device)  # batch*1\n",
    "        for _ in range(max_length):\n",
    "            output = generate(model, pv, text_input, stochastic=False)\n",
    "            if output.sum() == 0:\n",
    "                break\n",
    "\n",
    "            text_input = torch.cat([text_input, output], dim=-1)\n",
    "        for i in range(text_input.size(0)):\n",
    "            sentence = text_input[i]\n",
    "            cdd = tokenizer.decode(sentence)[0]#newtkn\n",
    "            candidate.append(cdd)\n",
    "    else:\n",
    "        for prop_embeds in pv:\n",
    "            prop_embeds = prop_embeds.unsqueeze(0)\n",
    "            product_input = torch.tensor([tokenizer.cls_token_id]).expand(1, 1).to(model.device)\n",
    "            values, indices = generate(model, prop_embeds, product_input, stochastic=stochastic, k=k)\n",
    "            product_input = torch.cat([torch.tensor([tokenizer.cls_token_id]).expand(k, 1).to(model.device), indices.squeeze(0).unsqueeze(-1)], dim=-1)\n",
    "            current_p = values.squeeze(0)\n",
    "            final_output = []\n",
    "            for _ in range(max_length):\n",
    "                values, indices = generate(model, prop_embeds, product_input, stochastic=stochastic, k=k)\n",
    "                k2_p = current_p[:, None] + values\n",
    "                product_input_k2 = torch.cat([product_input.unsqueeze(1).repeat(1, k, 1), indices.unsqueeze(-1)], dim=-1)\n",
    "                if tokenizer.sep_token_id in indices:\n",
    "                    ends = (indices == tokenizer.sep_token_id).nonzero(as_tuple=False)\n",
    "                    for e in ends:\n",
    "                        p = k2_p[e[0], e[1]].cpu().item()\n",
    "                        final_output.append((p, product_input_k2[e[0], e[1]]))\n",
    "                        k2_p[e[0], e[1]] = -1e5\n",
    "                    if len(final_output) >= k ** 1:\n",
    "                        break\n",
    "                current_p, i = torch.topk(k2_p.flatten(), k)\n",
    "                next_indices = torch.from_numpy(np.array(np.unravel_index(i.cpu().numpy(), k2_p.shape))).T\n",
    "                product_input = torch.stack([product_input_k2[i[0], i[1]] for i in next_indices], dim=0)\n",
    "\n",
    "            candidate_k = []\n",
    "            final_output = sorted(final_output, key=lambda x: x[0], reverse=True)[:k]\n",
    "            for p, sentence in final_output:\n",
    "                cdd = tokenizer.decode(sentence[:-1])[0]#newtkn\n",
    "                candidate_k.append(cdd)\n",
    "            if candidate_k == []:\n",
    "                candidate.append(\"\")\n",
    "            else:\n",
    "                candidate.append(candidate_k[0])\n",
    "            # candidate.append(random.choice(candidate_k))\n",
    "    return candidate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f2812b",
   "metadata": {
    "papermill": {
     "duration": 0.016767,
     "end_time": "2026-02-09T06:33:42.150688",
     "exception": false,
     "start_time": "2026-02-09T06:33:42.133921",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Generated Molecule**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d11ee16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T06:33:42.182122Z",
     "iopub.status.busy": "2026-02-09T06:33:42.181614Z",
     "iopub.status.idle": "2026-02-09T06:41:17.563785Z",
     "shell.execute_reply": "2026-02-09T06:41:17.562337Z"
    },
    "papermill": {
     "duration": 455.417284,
     "end_time": "2026-02-09T06:41:17.577687",
     "exception": false,
     "start_time": "2026-02-09T06:33:42.160403",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: This molecule contains an benzoyl group.\n",
      "==================================================\n",
      "Generated 5 SMILES strings:\n",
      "1: Cc1cccc(-c2ccc([C@@H](C(=O)O)c3cc(C(=O)N(C)CN)cc4c3CCCCC4)cc2I)c1I\n",
      "2: CCn1c([S@@](O)C(=O)C(COC(=O)C[S@@](CO)c2ccccc2O)=C(c2ccccc2)C2[C@]1(O)C(=O)C[C@H](C(C)(C)C)[C@@]1(c3ccccc3)[C@H]2CC[C@H]2N1)nc2ccccc21CCN(C(=O)[C@@]12CCC(C=C(c3ccccc3)C(=O)O)=C2O)CC1\n",
      "3: CNCCc1ccccc1C(=O)Nn1ncc2c(/Cc3ccccc3)cn([C@H]2C)c1=CO[C@@H](C(=O)O)C[C@H]2O\n",
      "4: CCOC(=O)c1cc([C@](CC)(CC)[A(CC)CC)c(O)c2cc(=O)c3cc([(CC)CC)c([(CC)[(CC)[(CC)CC)4c(=O)c(CC)c(CC)c(CC)[3[(CC)[[(CC)[[(CC)[(CC)[(CC)[[[(CC)[[1(CC)[[1[CCOC(=O)[CCOC(=O)[CCOC(=O)[CCOC(=O)[CCOC(=O)CCOC(=O)[CCOC(=O)CCOC(=O)[CCOC(=O)CCOC(=O)[CCOC(=O)CCOC(=O)[[CCOC(=O)[[CCOC(=O)[[[CCOC(=O)[[[[CCOC(=O)[CCOC(=O)[CCOC(=O)[CCOC(=O)[[[CCOC(=O)[CCOC(=O)[CCOC(=O)[[CCOC(=O)CCOC(=O)[CCOC(=O)[CCOC(=O)[[CCOC(=O)CCOC(=O)CCOC(=O)[[CCOC(=O)[CCOC(=O)[CCOC(=O)[CCOC(=O)[CCOC(=O)[CCOC(=O)[CCOC(=O)[CCOC(=O)[CCOC(=O)[[[[[[[\n",
      "5: CC(C)n1Cc2c(sc3c2Cc2ccccc2C3(C)C)C2c(s1)[C@]1(OC(=O)N1C(=O)N13C[C@]45C[C@H]6C(C)(C)C(=O)[C@]67C[C@]5(C(=O)[N+]1(C)C)C(=O)c1c3c([nH]c3c1O)C41OC(=O)c3ccccc31)C(C)(C)[N+]2C(C)C\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "z = torch.randn(5, model.in_channels, latent_size, 1, device=device)\n",
    "using_cfg = 2.5\n",
    "if using_cfg:\n",
    "    z = torch.cat([z, z], 0)\n",
    "    y = torch.cat([y_cond, y_null], 0)\n",
    "    pad_mask = torch.cat([pad_mask_cond, pad_mask_null], 0)\n",
    "    model_kwargs = dict(y=y, pad_mask=pad_mask, cfg_scale=2.5)\n",
    "    sample_fn = model.forward_with_cfg\n",
    "else:\n",
    "    model_kwargs = dict(y=y_cond, pad_mask=pad_mask)\n",
    "    sample_fn = model.forward\n",
    "samples = diffusion.p_sample_loop(sample_fn, z.shape, noise=z, clip_denoised=False, model_kwargs=model_kwargs, progress=False, device=device)\n",
    "if using_cfg:\n",
    "    samples, _ = samples.chunk(2, dim=0)\n",
    "\n",
    "ae_model.text_encoder.config.is_decoder = True\n",
    "ae_model.text_encoder.config.add_cross_attention = True\n",
    "samples = samples.squeeze(-1).permute((0, 2, 1))\n",
    "smiles_list = AE_SMILES_decoder(samples, ae_model,stochastic=False, k=1)\n",
    "\n",
    "print(f\"\\nPrompt: {prompt}\")\n",
    "print(\"=\"*50)\n",
    "print(\"Generated 5 SMILES strings:\")\n",
    "for i, s in enumerate(smiles_list, 1):\n",
    "    print(f\"{i}: {s}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c9d813f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T06:41:17.598841Z",
     "iopub.status.busy": "2026-02-09T06:41:17.598278Z",
     "iopub.status.idle": "2026-02-09T06:41:17.605809Z",
     "shell.execute_reply": "2026-02-09T06:41:17.604989Z"
    },
    "papermill": {
     "duration": 0.020117,
     "end_time": "2026-02-09T06:41:17.607584",
     "exception": false,
     "start_time": "2026-02-09T06:41:17.587467",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_validity(smiles):\n",
    "    from rdkit import Chem\n",
    "    valid_smiles = []\n",
    "    for s_str in smiles:\n",
    "        try:\n",
    "            if s_str == \"\" or s_str is None:\n",
    "                continue\n",
    "            # Attempt to create a molecule and then back to SMILES to canonicalize\n",
    "            mol = Chem.MolFromSmiles(s_str)\n",
    "            if mol is not None:\n",
    "                canonical_s = Chem.MolToSmiles(mol, isomericSmiles=False)\n",
    "                valid_smiles.append(canonical_s)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    unique_valid = list(set(valid_smiles))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    validity_ratio = len(valid_smiles) / len(smiles) if len(smiles) > 0 else 0.0\n",
    "    uniqueness_ratio = len(unique_valid) / len(valid_smiles) if len(valid_smiles) > 0 else 0.0\n",
    "    \n",
    "    return validity_ratio, uniqueness_ratio, unique_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "79d34310",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T06:41:17.628691Z",
     "iopub.status.busy": "2026-02-09T06:41:17.628210Z",
     "iopub.status.idle": "2026-02-09T06:41:17.807603Z",
     "shell.execute_reply": "2026-02-09T06:41:17.806385Z"
    },
    "papermill": {
     "duration": 0.192612,
     "end_time": "2026-02-09T06:41:17.810020",
     "exception": false,
     "start_time": "2026-02-09T06:41:17.617408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Generated: 5\n",
      "Validity: 20.00%\n",
      "Uniqueness: 100.00%\n",
      "First 5 valid SMILES: ['Cc1cccc(-c2ccc(C(C(=O)O)c3cc(C(=O)N(C)CN)cc4c3CCCCC4)cc2I)c1I']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[06:41:17] SMILES Parse Error: unclosed ring for input: 'CCn1c([S@@](O)C(=O)C(COC(=O)C[S@@](CO)c2ccccc2O)=C(c2ccccc2)C2[C@]1(O)C(=O)C[C@H](C(C)(C)C)[C@@]1(c3ccccc3)[C@H]2CC[C@H]2N1)nc2ccccc21CCN(C(=O)[C@@]12CCC(C=C(c3ccccc3)C(=O)O)=C2O)CC1'\n",
      "[06:41:17] SMILES Parse Error: unclosed ring for input: 'CNCCc1ccccc1C(=O)Nn1ncc2c(/Cc3ccccc3)cn([C@H]2C)c1=CO[C@@H](C(=O)O)C[C@H]2O'\n",
      "[06:41:17] SMILES Parse Error: syntax error while parsing: CCOC(=O)c1cc([C@](CC)(CC)[A(CC)CC)c(O)c2cc(=O)c3cc([(CC)CC)c([(CC)[(CC)[(CC)CC)4c(=O)c(CC)c(CC)c(CC)[3[(CC)[[(CC)[[(CC)[(CC)[(CC)[[[(CC)[[1(CC)[[1[CCOC(=O)[CCOC(=O)[CCOC(=O)[CCOC(=O)[CCOC(=O)CCOC(=O)[CCOC(=O)CCOC(=O)[CCOC(=O)CCOC(=O)[CCOC(=O)CCOC(=O)[[CCOC(=O)[[CCOC(=O)[[[CCOC(=O)[[[[CCOC(=O)[CCOC(=O)[CCOC(=O)[CCOC(=O)[[[CCOC(=O)[CCOC(=O)[CCOC(=O)[[CCOC(=O)CCOC(=O)[CCOC(=O)[CCOC(=O)[[CCOC(=O)CCOC(=O)CCOC(=O)[[CCOC(=O)[CCOC(=O)[CCOC(=O)[CCOC(=O)[CCOC(=O)[CCOC(=O)[CCOC(=O)[CCOC(=O)[CCOC(=O)[[[[[[[\n",
      "[06:41:17] SMILES Parse Error: check for mistakes around position 27:\n",
      "[06:41:17] O)c1cc([C@](CC)(CC)[A(CC)CC)c(O)c2cc(=O)c\n",
      "[06:41:17] ~~~~~~~~~~~~~~~~~~~~^\n",
      "[06:41:17] SMILES Parse Error: extra open parentheses while parsing: CCOC(=O)c1cc([C@](CC)(CC)[A(CC)CC)c(O)c2cc(=O)c3cc([(CC)CC)c([(CC)[(CC)[(CC)CC)4c(=O)c(CC)c(CC)c(CC)[3[(CC)[[(CC)[[(CC)[(CC)[(CC)[[[(CC)[[1(CC)[[1[CCOC(=O)[CCOC(=O)[CCOC(=O)[CCOC(=O)[CCOC(=O)CCOC(=O)[CCOC(=O)CCOC(=O)[CCOC(=O)CCOC(=O)[CCOC(=O)CCOC(=O)[[CCOC(=O)[[CCOC(=O)[[[CCOC(=O)[[[[CCOC(=O)[CCOC(=O)[CCOC(=O)[CCOC(=O)[[[CCOC(=O)[CCOC(=O)[CCOC(=O)[[CCOC(=O)CCOC(=O)[CCOC(=O)[CCOC(=O)[[CCOC(=O)CCOC(=O)CCOC(=O)[[CCOC(=O)[CCOC(=O)[CCOC(=O)[CCOC(=O)[CCOC(=O)[CCOC(=O)[CCOC(=O)[CCOC(=O)[CCOC(=O)[[[[[[[\n",
      "[06:41:17] SMILES Parse Error: check for mistakes around position 13:\n",
      "[06:41:17] CCOC(=O)c1cc([C@](CC)(CC)[A(CC)CC)c(O)c2c\n",
      "[06:41:17] ~~~~~~~~~~~~^\n",
      "[06:41:17] SMILES Parse Error: Failed parsing SMILES 'CCOC(=O)c1cc([C@](CC)(CC)[A(CC)CC)c(O)c2cc(=O)c3cc([(CC)CC)c([(CC)[(CC)[(CC)CC)4c(=O)c(CC)c(CC)c(CC)[3[(CC)[[(CC)[[(CC)[(CC)[(CC)[[[(CC)[[1(CC)[[1[CCOC(=O)[CCOC(=O)[CCOC(=O)[CCOC(=O)[CCOC(=O)CCOC(=O)[CCOC(=O)CCOC(=O)[CCOC(=O)CCOC(=O)[CCOC(=O)CCOC(=O)[[CCOC(=O)[[CCOC(=O)[[[CCOC(=O)[[[[CCOC(=O)[CCOC(=O)[CCOC(=O)[CCOC(=O)[[[CCOC(=O)[CCOC(=O)[CCOC(=O)[[CCOC(=O)CCOC(=O)[CCOC(=O)[CCOC(=O)[[CCOC(=O)CCOC(=O)CCOC(=O)[[CCOC(=O)[CCOC(=O)[CCOC(=O)[CCOC(=O)[CCOC(=O)[CCOC(=O)[CCOC(=O)[CCOC(=O)[CCOC(=O)[[[[[[[' for input: 'CCOC(=O)c1cc([C@](CC)(CC)[A(CC)CC)c(O)c2cc(=O)c3cc([(CC)CC)c([(CC)[(CC)[(CC)CC)4c(=O)c(CC)c(CC)c(CC)[3[(CC)[[(CC)[[(CC)[(CC)[(CC)[[[(CC)[[1(CC)[[1[CCOC(=O)[CCOC(=O)[CCOC(=O)[CCOC(=O)[CCOC(=O)CCOC(=O)[CCOC(=O)CCOC(=O)[CCOC(=O)CCOC(=O)[CCOC(=O)CCOC(=O)[[CCOC(=O)[[CCOC(=O)[[[CCOC(=O)[[[[CCOC(=O)[CCOC(=O)[CCOC(=O)[CCOC(=O)[[[CCOC(=O)[CCOC(=O)[CCOC(=O)[[CCOC(=O)CCOC(=O)[CCOC(=O)[CCOC(=O)[[CCOC(=O)CCOC(=O)CCOC(=O)[[CCOC(=O)[CCOC(=O)[C... truncated[06:41:17] SMILES Parse Error: unclosed ring for input: 'CC(C)n1Cc2c(sc3c2Cc2ccccc2C3(C)C)C2c(s1)[C@]1(OC(=O)N1C(=O)N13C[C@]45C[C@H]6C(C)(C)C(=O)[C@]67C[C@]5(C(=O)[N+]1(C)C)C(=O)c1c3c([nH]c3c1O)C41OC(=O)c3ccccc31)C(C)(C)[N+]2C(C)C'\n"
     ]
    }
   ],
   "source": [
    "validity, uniqueness, valid_list = get_validity(smiles_list)\n",
    "\n",
    "print(f\"Total Generated: {len(smiles_list)}\")\n",
    "print(f\"Validity: {validity:.2%}\")\n",
    "print(f\"Uniqueness: {uniqueness:.2%}\")\n",
    "print(f\"First 5 valid SMILES: {valid_list[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78248b8e",
   "metadata": {
    "papermill": {
     "duration": 0.009399,
     "end_time": "2026-02-09T06:41:17.829883",
     "exception": false,
     "start_time": "2026-02-09T06:41:17.820484",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "082140f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T06:41:17.850715Z",
     "iopub.status.busy": "2026-02-09T06:41:17.850056Z",
     "iopub.status.idle": "2026-02-09T06:41:17.854966Z",
     "shell.execute_reply": "2026-02-09T06:41:17.854075Z"
    },
    "papermill": {
     "duration": 0.017397,
     "end_time": "2026-02-09T06:41:17.856692",
     "exception": false,
     "start_time": "2026-02-09T06:41:17.839295",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "74bd78ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T06:41:17.877445Z",
     "iopub.status.busy": "2026-02-09T06:41:17.876955Z",
     "iopub.status.idle": "2026-02-09T06:41:18.023959Z",
     "shell.execute_reply": "2026-02-09T06:41:18.022687Z"
    },
    "papermill": {
     "duration": 0.159883,
     "end_time": "2026-02-09T06:41:18.026098",
     "exception": false,
     "start_time": "2026-02-09T06:41:17.866215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mols = [Chem.MolFromSmiles(smi) for smi in valid_list]\n",
    "\n",
    "# 2. Filter out any None values just in case\n",
    "mols = [m for m in mols if m is not None]\n",
    "\n",
    "# 3. Create a grid image of all valid molecules\n",
    "# 'molsPerRow' sets how many molecules are in each row\n",
    "img = Draw.MolsToGridImage(\n",
    "    mols, \n",
    "    molsPerRow=4, \n",
    "    subImgSize=(300, 300), \n",
    "    legends=[f\"Mol {i+1}\" for i in range(len(mols))]\n",
    ")\n",
    "\n",
    "# 4. Display the grid in your notebook\n",
    "display(img)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8953893,
     "sourceId": 14067141,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9431507,
     "sourceId": 14756174,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9442123,
     "sourceId": 14771704,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 603.138334,
   "end_time": "2026-02-09T06:41:20.860848",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-09T06:31:17.722514",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "05134263cca34845a578601b5392821f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "07e435e7262742789541f368212d293e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8d423e721dab48c2921b8de470c1215c",
       "placeholder": "​",
       "style": "IPY_MODEL_fe83954fe07e4959b9239e8b709e8da2",
       "tabbable": null,
       "tooltip": null,
       "value": " 792k/792k [00:01&lt;00:00, 412kB/s]"
      }
     },
     "0c5000d439774444bbec589d397dea25": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "11e7a02868694caa88779dee2893fabf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "13da53a238c449b78d97e4a62c859b00": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8559c2c4fab74d7b8da8e6a933af685b",
       "placeholder": "​",
       "style": "IPY_MODEL_e69bc8a3b4534602a354b9dabc030707",
       "tabbable": null,
       "tooltip": null,
       "value": " 3.13G/3.13G [00:14&lt;00:00, 239MB/s]"
      }
     },
     "15ee9d1095fe41a39e23c258fee251c9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8d0727a9b1864bb6b50101fa518955ab",
       "placeholder": "​",
       "style": "IPY_MODEL_722d03d0dd804314901e3ca1c2e7f1a8",
       "tabbable": null,
       "tooltip": null,
       "value": " 2.42M/? [00:00&lt;00:00, 1.81MB/s]"
      }
     },
     "169f3693636c4ad196394e32b53ec4cc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "193f4443816e44bf8302b5111ddda0cb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "1b60ab4148994c61a7866a6186181019": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "264061305dae44c0b382ff669d8b14a1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2b68d9eba0ff4288be8ae21ac7570b47": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_51aa716067554363ab271700c4825729",
       "placeholder": "​",
       "style": "IPY_MODEL_65b62f3f77144f65be98ae66a4880f6d",
       "tabbable": null,
       "tooltip": null,
       "value": "tokenizer.json: "
      }
     },
     "339b1135b0564e4cbab442fbff6985c5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "373a2872635544efb780a04782fdb185": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "37f8255800df4f378a7a2f7de40f328c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_52eee1567e514be89cbec98362c4b1c2",
       "max": 3132781861.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b040b099e256487d98e7b2473cbf377d",
       "tabbable": null,
       "tooltip": null,
       "value": 3132781861.0
      }
     },
     "3b281befa3cc41ed9dde53730ca93779": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3ee80929d33b404486a28a4fcba90df9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_867cffb611dd410589a5dae85b5f1e0e",
        "IPY_MODEL_7e9a1a06352a441d8e85a4cf3d31616a",
        "IPY_MODEL_07e435e7262742789541f368212d293e"
       ],
       "layout": "IPY_MODEL_3b281befa3cc41ed9dde53730ca93779",
       "tabbable": null,
       "tooltip": null
      }
     },
     "4389e1a1467a4411af71ea690a870388": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_2b68d9eba0ff4288be8ae21ac7570b47",
        "IPY_MODEL_96b895ee4f1e400baf8cdee85e450325",
        "IPY_MODEL_15ee9d1095fe41a39e23c258fee251c9"
       ],
       "layout": "IPY_MODEL_8080e4a9845742e2b74d4fbb9ee4314c",
       "tabbable": null,
       "tooltip": null
      }
     },
     "45f8d765f56c4cd5bfcf0e3cc1032960": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "48ee132efadd4bbca3442037fb89c2b1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9628154fd546404f912916d53f18b4b1",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e7d60498b6ce46b0aaa9091a0753b4ed",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "49e346811c504a89a05355de264e1ff3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8e95baf03c844336a1fbe1ba63baefe6",
       "placeholder": "​",
       "style": "IPY_MODEL_cbd611b0299246fcb1214c4dba244b43",
       "tabbable": null,
       "tooltip": null,
       "value": "model.safetensors: 100%"
      }
     },
     "4e1c41b2dcc7435ab8cc81dd0d6876f4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_4e3afd5e744e4f0ab4cb2384d674dd86",
        "IPY_MODEL_70b3e0f0630f4e18845b2b72a29504f5",
        "IPY_MODEL_a78f2abdf8734359a70f65b281c00de9"
       ],
       "layout": "IPY_MODEL_45f8d765f56c4cd5bfcf0e3cc1032960",
       "tabbable": null,
       "tooltip": null
      }
     },
     "4e3afd5e744e4f0ab4cb2384d674dd86": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1b60ab4148994c61a7866a6186181019",
       "placeholder": "​",
       "style": "IPY_MODEL_9f9647fa9d114173a26e6240e422601c",
       "tabbable": null,
       "tooltip": null,
       "value": "config.json: 100%"
      }
     },
     "51aa716067554363ab271700c4825729": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "52eee1567e514be89cbec98362c4b1c2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "556f6aacc01741cd9993b5418761f98a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "56f7717cbe4e4d3f8b30a56309e01cd5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d7f4d9bc28ef41688171695abb87e0ef",
        "IPY_MODEL_48ee132efadd4bbca3442037fb89c2b1",
        "IPY_MODEL_fd0a66b175fd4f4199c6d394c2430567"
       ],
       "layout": "IPY_MODEL_804485666b7845e4a19c83e838139ca6",
       "tabbable": null,
       "tooltip": null
      }
     },
     "5840d7c8ba004c78a9883a0520229914": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5fd183fb1f2743e7b77b443bb9cb05e4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "65b62f3f77144f65be98ae66a4880f6d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6c45d249aefd433d8f4e94717beae097": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_49e346811c504a89a05355de264e1ff3",
        "IPY_MODEL_d2160357e77b4ec192b9f38d66c752b6",
        "IPY_MODEL_adfa1b663f934ffc87e21c53f046b94d"
       ],
       "layout": "IPY_MODEL_edce178207d0432c951e1257d7d2b42c",
       "tabbable": null,
       "tooltip": null
      }
     },
     "70b3e0f0630f4e18845b2b72a29504f5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_754ef0ba15be41deb19db514592fb95d",
       "max": 700.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_193f4443816e44bf8302b5111ddda0cb",
       "tabbable": null,
       "tooltip": null,
       "value": 700.0
      }
     },
     "722d03d0dd804314901e3ca1c2e7f1a8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "7456dc0edbc64d6aaafbd50201a65f1b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "754ef0ba15be41deb19db514592fb95d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7e9a1a06352a441d8e85a4cf3d31616a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_264061305dae44c0b382ff669d8b14a1",
       "max": 791656.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_cb9b381834584451b9e55de3542dfdfb",
       "tabbable": null,
       "tooltip": null,
       "value": 791656.0
      }
     },
     "804485666b7845e4a19c83e838139ca6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8080e4a9845742e2b74d4fbb9ee4314c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8156d468d1a846448f166f4cb6b942ca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8559c2c4fab74d7b8da8e6a933af685b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "867cffb611dd410589a5dae85b5f1e0e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ee7553c9589f424baa495c46f2b8f994",
       "placeholder": "​",
       "style": "IPY_MODEL_339b1135b0564e4cbab442fbff6985c5",
       "tabbable": null,
       "tooltip": null,
       "value": "spiece.model: 100%"
      }
     },
     "8d0727a9b1864bb6b50101fa518955ab": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8d423e721dab48c2921b8de470c1215c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8e95baf03c844336a1fbe1ba63baefe6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8ef7eb7af96f4f05bcd51df6e17c12f6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9628154fd546404f912916d53f18b4b1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "96b895ee4f1e400baf8cdee85e450325": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_05134263cca34845a578601b5392821f",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_dcead98499fb4542a18a9e427e14d235",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "9c7e8a26fcc44999804853fbf9086680": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_373a2872635544efb780a04782fdb185",
       "placeholder": "​",
       "style": "IPY_MODEL_8ef7eb7af96f4f05bcd51df6e17c12f6",
       "tabbable": null,
       "tooltip": null,
       "value": "pytorch_model.bin: 100%"
      }
     },
     "9d497ad1195745058efd8725d1dddd13": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9f3d9a7ef4a646f390dc7ec2286df5cc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c85a0dc51d6c494a9f74cfe5571ba634",
       "placeholder": "​",
       "style": "IPY_MODEL_5fd183fb1f2743e7b77b443bb9cb05e4",
       "tabbable": null,
       "tooltip": null,
       "value": "special_tokens_map.json: "
      }
     },
     "9f9647fa9d114173a26e6240e422601c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a3a4879395f34edba75bf595d49bd294": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "a50538b6d3874b68b0895e10df11e0b7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a762c1d084394b409abe5d127f983e4e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a50538b6d3874b68b0895e10df11e0b7",
       "placeholder": "​",
       "style": "IPY_MODEL_11e7a02868694caa88779dee2893fabf",
       "tabbable": null,
       "tooltip": null,
       "value": " 1.79k/? [00:00&lt;00:00, 12.1kB/s]"
      }
     },
     "a78f2abdf8734359a70f65b281c00de9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c172272bda0c4815babbfda0b8ca8463",
       "placeholder": "​",
       "style": "IPY_MODEL_8156d468d1a846448f166f4cb6b942ca",
       "tabbable": null,
       "tooltip": null,
       "value": " 700/700 [00:00&lt;00:00, 57.7kB/s]"
      }
     },
     "aa09cf1c4ec849a0b773924b79645f6e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ff604caf9e6747f19580395d38bd2d70",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_a3a4879395f34edba75bf595d49bd294",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "adfa1b663f934ffc87e21c53f046b94d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d444271919dd4edf85ec382bdef84b2c",
       "placeholder": "​",
       "style": "IPY_MODEL_556f6aacc01741cd9993b5418761f98a",
       "tabbable": null,
       "tooltip": null,
       "value": " 3.13G/3.13G [00:24&lt;00:00, 453MB/s]"
      }
     },
     "b040b099e256487d98e7b2473cbf377d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "b28ead25e4b6495086e9c15b1dbb1189": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bf516c69f2654260892e95347967460b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_9f3d9a7ef4a646f390dc7ec2286df5cc",
        "IPY_MODEL_aa09cf1c4ec849a0b773924b79645f6e",
        "IPY_MODEL_a762c1d084394b409abe5d127f983e4e"
       ],
       "layout": "IPY_MODEL_b28ead25e4b6495086e9c15b1dbb1189",
       "tabbable": null,
       "tooltip": null
      }
     },
     "c172272bda0c4815babbfda0b8ca8463": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c40277b6b43f49a98fc8cb03aa0a35ab": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c85a0dc51d6c494a9f74cfe5571ba634": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cb9b381834584451b9e55de3542dfdfb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "cbd611b0299246fcb1214c4dba244b43": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d2160357e77b4ec192b9f38d66c752b6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_efd0a564cea3494784859bb95a665523",
       "max": 3132668808.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_169f3693636c4ad196394e32b53ec4cc",
       "tabbable": null,
       "tooltip": null,
       "value": 3132668808.0
      }
     },
     "d444271919dd4edf85ec382bdef84b2c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d7f4d9bc28ef41688171695abb87e0ef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0c5000d439774444bbec589d397dea25",
       "placeholder": "​",
       "style": "IPY_MODEL_5840d7c8ba004c78a9883a0520229914",
       "tabbable": null,
       "tooltip": null,
       "value": "tokenizer_config.json: "
      }
     },
     "dcead98499fb4542a18a9e427e14d235": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "e671687b70674b128ad83b2cc236f971": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_9c7e8a26fcc44999804853fbf9086680",
        "IPY_MODEL_37f8255800df4f378a7a2f7de40f328c",
        "IPY_MODEL_13da53a238c449b78d97e4a62c859b00"
       ],
       "layout": "IPY_MODEL_7456dc0edbc64d6aaafbd50201a65f1b",
       "tabbable": null,
       "tooltip": null
      }
     },
     "e69bc8a3b4534602a354b9dabc030707": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e7d60498b6ce46b0aaa9091a0753b4ed": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "edce178207d0432c951e1257d7d2b42c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ee7553c9589f424baa495c46f2b8f994": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "efd0a564cea3494784859bb95a665523": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fd0a66b175fd4f4199c6d394c2430567": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9d497ad1195745058efd8725d1dddd13",
       "placeholder": "​",
       "style": "IPY_MODEL_c40277b6b43f49a98fc8cb03aa0a35ab",
       "tabbable": null,
       "tooltip": null,
       "value": " 2.13k/? [00:00&lt;00:00, 22.5kB/s]"
      }
     },
     "fe83954fe07e4959b9239e8b709e8da2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ff604caf9e6747f19580395d38bd2d70": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
